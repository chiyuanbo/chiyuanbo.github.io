[{"title":"Pipeline","url":"/2022/12/28/cyb-mds/devops/jenkins/使用Jenkinsfile/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n流水线支持 两种语法：声明式（在 Pipeline 2.5 引入）和脚本式流水线。 两种语法都支持构建持续交付流水线。\n\n### 创建 Jenkinsfile\n\nJenkinsfile 是一个文本文件，它包含了 Jenkins 流水线的定义并被检入源代码控制仓库。下面的流水线实现了基本的三阶段持续交付流水线。\n\n```groovy\npipeline {\n    agent any\n\tstages {\n        stage('Build') {\n            steps {\n                echo 'Building..'\n            }\n        }\n        stage('Test') {\n            steps {\n                echo 'Testing..'\n            }\n        }\n        stage('Deploy') {\n            steps {\n                echo 'Deploying....'\n            }\n        }\n    }\n}\n```\n上面的声明式流水线示例包含了实现持续交付流水线的最小必要结构。agent指令是必需的，它指示 Jenkins 为流水线分配一个执行器和工作区。一个合法的声明式流水线还需要 stages 指令和 steps 指令，因为它们指示 Jenkins 要执行什么，在哪个阶段执行。","tags":["DevOps","Jenkins"]},{"title":"Pipeline入门","url":"/2022/12/28/cyb-mds/devops/jenkins/Pipeline入门/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n###  jenkins-Pipeline基础语法\n\n####   1) jenkins-Pipeline总体介绍\n\n* Pipeline，简而言之，就是一套运行与jenkins上的工作流框架，将原本独立运行于单个或多个节点的任务连接起来，实现单个任务难以完成的复杂流程编排与可视化。\n* Pipeline是jenkins2.x最核心的特性，帮助jenkins实现从CI到CD与devops的转。\n*  https://jenkins.io/2.0/\n\n####   2) 什么是jenkins Pipeline\n\n* jenkins Pipeline是一组插件，让jenkins可以实现持续交付管道的落地和实施。\n*  持续交付管道（CD Pipeline）是将软件从版本控制阶段到交付给用户或客户的完整过程的自动化表现\n*  软件的每一次更改（提交到源代码管理系统）都要经过一个复杂的过程才能被发布\n* Pipeline提供了一组可扩展的工具 ，通过Pipeline Domain Specific Language（DSL) syntax可以达到Pipeline as Code的目的\n*  Pipeline as code： jenkinsfile存储在项目的源代码库\n\n####   3) 为什么要用Pipeline\n\n- 代码：Pipeline以代码的形式实现，通常被检入源代码控制，使团队能够编辑，审查和迭代其CD流程\n- 可持续性：jenkins重启或者中断后都不会影响Pipeline job\n- 停顿：Pipeline可以选择停止并等待人工输入或批准，然后在继续Pipeline运行。\n- 多功能：Pipeline支持现实世界的复杂CD要求，包括fork/join子进程，循环和并行执行工作的能力。\n- 可扩展：Pipeline插件支持其DSL的自定义扩展以及与其他插件集成的多个选项\n\n####   4) pipeline 基础语法\n\n```groovy\n• Stage\n　　• 阶段，一个pipeline可以划分为若干个Stage，每个Stage代表一组操作，列如：“Build”、“Test”、“Deploy”。\n　　• 注意，Stage是一个逻辑分组的慨念，可以跨多个node\n• Node\n　　• 节点，一个node就是一jenkins节点，或者是Master，或者是Agent，是执行Step的具体运行环境。\n• Step\n　　• 步骤，Step是最基本的操作单元，小到创建一个目录，大到构建一个docker镜像，由各类jenkins Plugin提供，例如：sh   'make'\n```\n\n####   5) jenkins Pipeline入门\n\n```groovy\n• pipeline脚本是由Groovy语言实现\n  　　• 无需专门学习Groovy\n• Pipeline支持两种语法\n  　　• Declarative声明式（在Pipeline plugin 2.5中引入）\n  　　• Scripted Pipeline脚本式\n• 如何创建基本的Pipeline\n  　　• 直接在jenkins Web UI网页界面中输入脚本\n  　　• 通过创建一个jenkinsfile可以检入项目的源代码管理库\n• 最佳实践\n  　　• 通常推荐在jenkins中直接从源代码控制（SCM）中载入jenkinsfile Pipeline\n```\n\n####   6) jenkins编写简单pipeline脚本\n\n```groovy\nnode {\n    stage('拉取代码'){\n        echo \"拉取代码\"\n    }\n    stage('代码编译'){\n        echo \"代码编译\"\n    }\n    stage('部署发布测试'){\n        echo \"部署发布测试\"\n    }\n}\n```\n\n(1) 通过经典UI创建一个新任务\n\n![](https://raw.githubusercontent.com/chiyuanbo/pic/master/20221228123618.png)\n\n(2) 选择pipeline,输入任务名\n\n![](https://raw.githubusercontent.com/chiyuanbo/pic/master/20221228123700.png)\n\n(3) 编写pipeline,保存\n\n![](https://raw.githubusercontent.com/chiyuanbo/pic/master/20221228124421.png)\n\n(4) 构建,查看视图\n\n![](https://raw.githubusercontent.com/chiyuanbo/pic/master/20221228124737.png)","tags":["DevOps","Jenkins"]},{"title":"THBD学习","url":"/2022/12/27/cyb-mds/java/code/THBD学习/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n##### 文件解释\n\n| 文件名          | 功能            | 补充                                            |\n| --------------- | --------------- | ----------------------------------------------- |\n| settings.gradle | 引入源码的rack  | 修改后需要刷新gradle                            |\n| dao.ioc.xml     | 各种连接ip      |                                                 |\n| csa.jar         | 前端包          | 和其他jar包一样都在.gradle文件夹内,直接替换即可 |\n| csa.gradle      | csa的gradle配置 | 修改后需要刷新gradle,有时需要initDB             |\n|                 |                 |                                                 |\n|                 |                 |                                                 |\n\n","tags":["Code","Java","THBD","Gradle"]},{"title":"实现Split函数","url":"/2022/12/02/cyb-mds/database/sql/mysql/实现Split函数/","content":"\n==作者：YB-Chi==\n\n```sql\nSELECT substring_index(substring_index(a.event_samples, '&TAB&', b.help_topic_id + 1), '&TAB&', - 1) AS event_sample\nFROM parser_rule a\nINNER JOIN \nmysql.help_topic b\nON b.help_topic_id < ((length(a.event_samples) - length(REPLACE(a.event_samples, '&TAB&', '')))/5 + 1)\ngroup by event_sample\n```\n\n**PS:\t/5是因为分隔符为5位**","tags":["sql","mysql"]},{"title":"集群表","url":"/2022/10/28/cyb-mds/database/ClickHouse/集群表/","content":"\n==作者：YB-Chi==\n\n\n\n1. 通过cluster在每个节点创建本地表\n\n```sql\nCREATE TABLE st_order_mt ON CLUSTER cyb_cluster\n(\n    `id` UInt32,\n    `sku_id` String,\n    `total_amount` Decimal(16, 2),\n    `create_time` Datetime\n)\nENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/st_order_mt', '{replica}')\nPARTITION BY toYYYYMMDD(create_time)\nPRIMARY KEY id\nORDER BY (id, sku_id)\n```\n\n2. 创建Distribute分布式表\n\n```sql\nCREATE TABLE st_order_mt_ALL ON CLUSTER cyb_cluster\n(\n    `id` UInt32,\n    `sku_id` String,\n    `total_amount` Decimal(16, 2),\n    `create_time` Datetime\n)\nENGINE = Distributed(cyb_cluster, cyb, st_order_mt, hiveHash(sku_id))\n```\n\n","tags":["Clickhouse","sql"]},{"title":"ClickHouse集群版部署","url":"/2022/10/28/cyb-mds/database/ClickHouse/ClickHouse集群3S2R方案/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n#### 部署结构\n\n3节点3实例3分片2副本![](https://raw.githubusercontent.com/chiyuanbo/pic/master/20221028114416.png)\n\n#### 配置文件\n\n`/etc/clickhouse-server/metrika.xml`\n\n```xml\n<?xml version=\"1.0\"?>\n<clickhouse>\n\t<clickhouse_remote_servers>\n\t\t<cyb_cluster>\n\t\t\t<shard>\n\t\t\t\t<internal_replication>true</internal_replication>\n\t\t\t\t<replica>\n\t\t\t\t\t<host>node1</host>\n\t\t\t\t\t<port>9000</port>\n\t\t\t\t</replica>\n\t\t\t\t<replica>\n\t\t\t\t\t<host>node2</host>\n\t\t\t\t\t<port>9002</port>\n\t\t\t\t</replica>\n\t\t\t</shard>\n\t\t\t<shard>\n\t\t\t\t<internal_replication>true</internal_replication>\n\t\t\t\t<replica>\n\t\t\t\t\t<host>node2</host>\n\t\t\t\t\t<port>9000</port>\n\t\t\t\t</replica>\n\t\t\t\t<replica>\n\t\t\t\t\t<host>node3</host>\n\t\t\t\t\t<port>9002</port>\n\t\t\t\t</replica>\n\t\t\t</shard>\n\t\t\t<shard>\n\t\t\t\t<internal_replication>true</internal_replication>\n\t\t\t\t<replica>\n\t\t\t\t\t<host>node3</host>\n\t\t\t\t\t<port>9000</port>\n\t\t\t\t</replica>\n\t\t\t\t<replica>\n\t\t\t\t\t<host>node1</host>\n\t\t\t\t\t<port>9002</port>\n\t\t\t\t</replica>\n\t\t\t</shard>\n\t\t</cyb_cluster>\n\t</clickhouse_remote_servers>\n     <macros>\n        <layer>ckcluster_3shard_2replica</layer>\n        <!--根据前面shard的配置，例子中总共3个shard-->\n        <shard>shard01</shard>\n        <!--每个节点配置本地主机名即可，或者唯一的数字id-->\n        <replica>node1</replica>\n    </macros>\n    <networks>\n       <ip>::/0</ip>\n    </networks>\n    <!-- 数据压缩算法  -->\n    <clickhouse_compression>\n        <case>\n            <min_part_size>10000000000</min_part_size>\n            <min_part_size_ratio>0.01</min_part_size_ratio>\n            <method>lz4</method>\n        </case>\n    </clickhouse_compression>\n</clickhouse>\n```\n\n其他节点差异化的参数就俩` <shard>shard01</shard>`和` <replica>node1</replica>`\n\n`config.xml`\n\n```xml\n<remote_servers incl=\"clickhouse_remote_servers\" />\n<include_from>/etc/clickhouse-server/metrika.xml</include_from>\n<macros incl=\"macros\" optional=\"true\"/>\n<zookeeper>\n    <node>\n        <host>node1</host>\n        <port>2181</port>\n    </node>\n    <node>\n        <host>node2</host>\n        <port>2181</port>\n    </node>\n    <node>\n        <host>node3</host>\n        <port>2181</port>\n    </node>\n</zookeeper>\n```\n\n#### 后续\n\n```shell\nchown clickhouse:clickhouse /etc/clickhouse-server/metrika.xml\n#3节点需要每个节点起俩实例,需要修改端口,这里不做演示了,可以参考`12实例15node多实例集群部署方案`\n#启动\nsystemctl start clickhouse-server\n#开机自启\nsystemctl enable clickhouse-server\n```\n\n","tags":["Linux","Clickhouse"]},{"title":"Redis-8-集合统计","url":"/2022/10/27/cyb-mds/module/Redis/Redis-8-集合统计/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n要想选择合适的集合，我们就得了解常用的集合统计模式。集合类型常见的四种统计模式，包括聚合统计、排序统计、二值状态统计和基数统计。\n\n(实际场景这里不做记录,王争老师的举例没有摸透)\n\n\n\n### 聚合统计\n\n所谓的聚合统计，就是指统计多个集合元素的聚合结果，包括：统计多个集合的共有元素（交集统计）；把两个集合相比，统计其中一个集合独有的元素（差集统计）；统计多个集合的所有元素（并集统计）。\n\n当你需要对多个集合进行聚合计算时，Set类型会是一个非常不错的选择。不过，我要提醒你一下，这里有一个潜在的风险。\n\nSet的差集、并集和交集的计算复杂度较高，在数据量较大的情况下，如果直接执行这些计算，会导致Redis实例阻塞。所以，我给你分享一个小建议：**你可以从主从集群中选择一个从库，让它专门负责聚合计算，或者是把数据读取到客户端，在客户端来完成聚合统计**，这样就可以规避阻塞主库实例和其他从库实例的风险了。\n\n\n\n### 排序统计\n\n集合中的元素可以按序排列，这种对元素保序的集合类型叫作有序集合。在Redis常用的4个集合类型中（List、Hash、Set、Sorted Set），List和Sorted Set就属于有序集合。\n\n**List是按照元素进入List的顺序进行排序的，而Sorted Set可以根据元素的权重来排序**，我们可以自己来决定每个元素的权重值。比如说，我们可以根据元素插入Sorted Set的时间确定权重值，先插入的元素权重小，后插入的元素权重大。\n\n在面对需要展示最新列表、排行榜等场景时，如果数据更新频繁或者需要分页显示，建议你优先考虑使用Sorted Set。\n\n\n\n### 二值状态统计\n\n如果只需要统计数据的二值状态，例如商品有没有、用户在不在等，就可以使用Bitmap，因为它只用一个bit位就能表示0或1。在记录海量数据时，Bitmap能够有效地节省内存空间。\n\n\n\n### 基数统计\n\n如果page1非常火爆，UV达到了千万，这个时候，一个Set就要记录千万个用户ID。对于一个搞大促的电商网站而言，这样的页面可能有成千上万个，如果每个页面都用这样的一个Set，就会消耗很大的内存空间。\n\nHyperLogLog是一种用于统计基数的数据集合类型，它的最大优势就在于，当集合元素数量非常多时，它计算基数所需的空间总是固定的，而且还很小。在Redis中，每个 HyperLogLog只需要花费 12 KB 内存，就可以计算接近 2^64 个元素的基数。你看，和元素越多就越耗费内存的Set和Hash类型相比，HyperLogLog就非常节省空间。\n\n不过，有一点需要你注意一下，HyperLogLog的统计规则是基于概率完成的，所以它给出的统计结果是有一定误差的，标准误算率是0.81%。这也就意味着，你使用HyperLogLog统计的UV是100万，但实际的UV可能是101万。虽然误差率不算大，但是，如果你需要精确统计结果的话，最好还是继续用Set或Hash类型。\n\n\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4z465oplwj227m1cbnpd.jpg\" alt=\"image\" style=\"zoom:33%;\" />\n\n\n\n**摘选自:极客时间-Redis核心技术与实战**","tags":["redis"]},{"title":"Redis-7-String&ziplist","url":"/2022/10/27/cyb-mds/module/Redis/Redis-7-String&ziplist/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n### String类型的内存空间消耗\n\n一个图片存储系统，因为图片数量巨大，所以我们就用10位数来表示图片ID和图片存储对象ID，例如，图片ID为1101000051，它在存储系统中对应的ID号是3301000051。\n\n```\nphoto_id: 1101000051\nphoto_obj_id: 3301000051\n```\n\n图片ID和图片存储对象ID正好一一对应，是典型的键-单值模式,即键值对中的值就是一个值，而不是一个集合，这和String类型提供的“一个键对应一个值的数据”的保存形式刚好契合，故此尝试使用String类型。\n\n刚开始，保存了1亿张图片，大约用了6.4GB的内存。但是，随着图片数据量的不断增加，Redis内存使用量也在增加，结果就遇到了大内存Redis实例因为生成RDB而响应变慢的问题。一组图片ID及其存储对象ID的记录，实际只需要16字节就可以了。但是实际一个图片ID和图片存储对象ID的记录平均用了64字节。\n\n其实，除了记录实际数据，String类型还需要额外的内存空间记录数据长度、空间使用等信息，这些信息也叫作元数据。当实际保存的数据较小时，元数据的空间开销就显得比较大了，有点“喧宾夺主”的意思。\n\nString类型具体是怎么保存数据的呢？\n\n当你保存64位有符号整数时，String类型会把它保存为一个8字节的Long类型整数，这种保存方式通常也叫作int编码方式。\n\n但是，当你保存的数据中包含字符时，String类型就会用简单动态字符串（Simple Dynamic String，SDS）结构体来保存，如下图所示：\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4utqgt0m2j21hi12wq7p.jpg\" alt=\"image\" style=\"zoom:20%;\" />\n\n- **buf**：字节数组，保存实际数据。为了表示字节数组的结束，Redis会自动在数组最后加一个“\\0”，这就会额外占用1个字节的开销。\n- **len**：占4个字节，表示buf的已用长度。\n- **alloc**：也占个4字节，表示buf的实际分配长度，一般大于len。\n\n在SDS中，buf保存实际数据，而len和alloc本身其实是SDS结构体的额外开销，此外还有一个来自于RedisObject结构体的开销。\n\n因为Redis的数据类型有很多，而且，不同数据类型都有些相同的元数据要记录（比如最后一次访问的时间、被引用的次数等），所以，Redis会用一个RedisObject结构体来统一记录这些元数据，同时指向实际数据。\n\n一个RedisObject包含了8字节的元数据和一个8字节指针，这个指针再进一步指向具体数据类型的实际数据所在，例如指向String类型的SDS结构所在的内存地址。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4utt6ztwkj21pi1a07ag.jpg\" alt=\"image\" style=\"zoom:20%;\" />\n\n为了节省内存空间，Redis还对Long类型整数和SDS的内存布局做了专门的设计。\n\n* 当保存的是Long类型整数时，RedisObject中的指针就直接赋值为整数数据了，这样就不用额外的指针再指向整数了，节省了指针的空间开销。\n* 当保存的是字符串数据，并且字符串小于等于44字节时，RedisObject中的元数据、指针和SDS是一块连续的内存区域，这样就可以避免内存碎片。这种布局方式也被称为embstr编码方式。\n* 当字符串大于44字节时，SDS的数据量就开始变多了，Redis就不再把SDS和RedisObject布局在一起了，而是会给SDS分配独立的空间，并用指针指向SDS结构。这种布局方式被称为raw编码模式。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4utuioonvj22dc1hu7t3.jpg\" alt=\"image\" style=\"zoom:20%;\" />\n\n开始计算上述内存，因为10位数的图片ID和图片存储对象ID是Long类型整数，所以可以直接用int编码的RedisObject保存。每个int编码的RedisObject元数据部分占8字节，指针部分被直接赋值为8字节的整数了。此时，每个ID会使用16字节，加起来一共是32字节。但是，另外的32字节去哪儿了呢？\n\nRedis会使用一个全局哈希表保存所有键值对，哈希表的每一项是一个dictEntry的结构体，用来指向一个键值对。dictEntry结构中有三个8字节的指针，分别指向key、value以及下一个dictEntry，三个指针共24字节，如下图所示：\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4utyglbw7j21tk12rdqu.jpg\" alt=\"image\" style=\"zoom:20%;\" />\n\n但是，这三个指针只有24字节，为什么会占用了32字节呢？这就要提到Redis使用的内存分配库jemalloc了。\n\njemalloc在分配内存时，会根据我们申请的字节数N，找一个比N大，但是最接近N的2的幂次数作为分配的空间，这样可以减少频繁分配的次数。\n\n举个例子。如果你申请6字节空间，jemalloc实际会分配8字节空间；如果你申请24字节空间，jemalloc则会分配32字节。所以，在我们刚刚说的场景里，dictEntry结构就占用了32字节。\n\n我们来换算下，如果要保存的图片有1亿张，那么1亿条的图片ID记录就需要6.4GB内存空间，其中有4.8GB的内存空间都用来保存元数据了，额外的内存空间开销很大。那么，有没有更加节省内存的方法呢？\n\n\n\n### 节省内存开销的ziplist\n\nRedis有一种底层数据结构，叫压缩列表（ziplist），这是一种非常节省内存的结构。\n\n压缩列表的构成。表头有三个字段zlbytes、zltail和zllen，分别表示列表长度、列表尾的偏移量，以及列表中的entry个数。压缩列表尾还有一个zlend，表示列表结束。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4uu19zdsvj22o10r0gud.jpg\" alt=\"image\" style=\"zoom:20%;\" />\n\n压缩列表之所以能节省内存，就在于它是用一系列连续的entry保存数据。每个entry的元数据包括下面几部分。\n\n- **prev_len**，表示前一个entry的长度。prev_len有两种取值情况：1字节或5字节。取值1字节时，表示上一个entry的长度小于254字节。虽然1字节的值能表示的数值范围是0到255，但是压缩列表中zlend的取值默认是255，因此，就默认用255表示整个压缩列表的结束，其他表示长度的地方就不能再用255这个值了。所以，当上一个entry长度小于254字节时，prev_len取值为1字节，否则，就取值为5字节。\n- **len**：表示自身长度，4字节；\n- **encoding**：表示编码方式，1字节；\n- **content**：保存实际数据。\n\n这些entry会挨个儿放置在内存中，不需要再用额外的指针进行连接，这样就可以节省指针所占用的空间。\n\n我们以保存图片存储对象ID为例，来分析一下压缩列表是如何节省内存空间的。\n\n每个**entry**保存一个图片存储对象ID（8字节），此时，每个entry的prev_len只需要1个字节就行，因为每个entry的前一个entry长度都只有8字节，小于254字节。这样一来，一个图片的存储对象ID所占用的内存大小是14字节（1+4+1+8=14），实际分配16字节。\n\nRedis基于压缩列表实现了List、Hash和Sorted Set这样的集合类型，这样做的最大好处就是节省了dictEntry的开销。当你用String类型时，一个键值对就有一个dictEntry，要用32字节空间。但采用集合类型时，一个key就对应一个集合的数据，能保存的数据多了很多，但也只用了一个dictEntry，这样就节省了内存。\n\n这个方案听起来很好，但还存在一个问题：在用集合类型保存键值对时，一个键对应了一个集合的数据，但是在我们的场景中，一个图片ID只对应一个图片的存储对象ID，我们该怎么用集合类型呢？换句话说，在一个键对应一个值（也就是单值键值对）的情况下，我们该怎么用集合类型来保存这种单值键值对呢？\n\n\n\n### 如何用集合类型保存单值的键值对\n\n在保存单值的键值对时，可以采用基于Hash类型的二级编码方法。这里说的二级编码，就是把一个单值的数据拆分成两部分，前一部分作为Hash集合的key，后一部分作为Hash集合的value，这样一来，我们就可以把单值数据保存到Hash集合中了。\n\n以图片ID 1101000060和图片存储对象ID 3302000080为例，我们可以把图片ID的前7位（1101000）作为Hash类型的键，把图片ID的最后3位（060）和图片存储对象ID分别作为Hash类型值中的key和value。\n\n按照这种设计方法，我在Redis中插入了一组图片ID及其存储对象ID的记录，并且用info命令查看了内存开销，我发现，增加一条记录后，内存占用只增加了16字节，如下所示：\n\n```\n127.0.0.1:6379> info memory\n# Memory\nused_memory:1039120\n127.0.0.1:6379> hset 1101000 060 3302000080\n(integer) 1\n127.0.0.1:6379> info memory\n# Memory\nused_memory:1039136\n```\n\n在使用String类型时，每个记录需要消耗64字节，这种方式却只用了16字节，所使用的内存空间是原来的1/4，满足了我们节省内存空间的需求。\n\nRedis Hash类型的两种底层实现结构，分别是压缩列表和哈希表。Hash类型设置了用压缩列表保存数据时的两个阈值，一旦超过了阈值，Hash类型就会用哈希表来保存数据了。\n\n- hash-max-ziplist-entries：表示用压缩列表保存时哈希集合中的最大元素个数。\n- hash-max-ziplist-value：表示用压缩列表保存时哈希集合中单个元素的最大长度。\n\n一旦从压缩列表转为了哈希表，Hash类型就会一直用哈希表进行保存，而不会再转回压缩列表了。在节省内存空间方面，哈希表就没有压缩列表那么高效了。\n\n**为了能充分使用压缩列表的精简内存布局，我们一般要控制保存在Hash集合中的元素个数**。所以，在刚才的二级编码中，我们只用图片ID最后3位作为Hash集合的key，也就保证了Hash集合的元素个数不超过1000，同时，我们把hash-max-ziplist-entries设置为1000，这样一来，Hash集合就可以一直使用压缩列表来节省内存空间了。\n\n**摘选自:极客时间-Redis核心技术与实战**","tags":["redis"]},{"title":"Redis-6-切片集群","url":"/2022/10/27/cyb-mds/module/Redis/Redis-6-切片集群/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n#### 数据切片和实例的对应分布关系\n\n切片集群是一种保存大量数据的通用机制,从3.0开始，官方提供了一个名为Redis Cluster的方案，用于实现切片集群。Redis Cluster方案中就规定了数据和实例的对应规则。\n\n具体来说，Redis Cluster方案采用哈希槽（Hash Slot），来处理数据和实例之间的映射关系。在Redis Cluster方案中，一个切片集群共有16384个哈希槽，这些哈希槽类似于数据分区，每个键值对都会根据它的key，被映射到一个哈希槽中。\n\n具体的映射过程分为两大步：首先根据键值对的key，按照CRC16算法计算一个16 bit的值；然后，再用这个16bit值对16384取模，得到0~16383范围内的模数，每个模数代表一个相应编号的哈希槽。\n\n我们在部署Redis Cluster方案时，可以使用cluster create命令创建集群，此时，Redis会自动把这些槽平均分布在集群实例上。例如，如果集群中有N个实例，那么，每个实例上的槽个数为16384/N个。当然， 我们也可以使用cluster meet命令手动建立实例间的连接，形成集群，再使用cluster addslots命令，指定每个实例上的哈希槽个数。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4rbw7297ej21x20z6n93.jpg\" alt=\"image\" style=\"zoom:25%;\" />\n\n**在手动分配哈希槽时，需要把16384个槽都分配完，否则Redis集群无法正常工作**\n\n\n\n#### 客户端如何定位数据\n\nRedis实例会把自己的哈希槽信息发给和它相连接的其它实例，来完成哈希槽分配信息的扩散。当实例之间相互连接后，每个实例就有所有哈希槽的映射关系了。客户端和集群实例建立连接后，实例就会把哈希槽的分配信息发给客户端，客户端把哈希槽信息缓存在本地。当客户端请求键值对时，会先计算键所对应的哈希槽，然后就可以给相应的实例发送请求了。\n\n但是，在集群中，实例和哈希槽的对应关系并不是一成不变的，最常见的变化有两个：\n\n- 在集群中，实例有新增或删除，Redis需要重新分配哈希槽；\n- 为了负载均衡，Redis需要把哈希槽在所有实例上重新分布一遍。\n\nRedis Cluster方案提供了一种**重定向机制，**所谓的“重定向”，就是指，客户端给一个实例发送数据读写操作时，这个实例上并没有相应的数据，那么，这个实例就会给客户端返回包含新实例地址的MOVED命令响应结果，然后客户端再给新实例发送操作命令，同时还会更新本地缓存。\n\n```shell\nGET hello:key\n(error) MOVED 13320 172.16.19.5:6379\n```\n\n其中，MOVED命令表示，客户端请求的键值对所在的哈希槽13320，实际是在172.16.19.5这个实例上。通过返回的MOVED命令，就相当于把哈希槽所在的新实例的信息告诉给客户端了。这样一来，客户端就可以直接和172.16.19.5连接，并发送操作请求了。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594faly1h4s7n8m609j226a1qi4o4.jpg\" alt=\"image\" style=\"zoom:25%;\" />\n\n需要注意的是，在上图中，当客户端给实例2发送命令时，Slot 2中的数据已经全部迁移到了实例3。在实际应用时，如果Slot 2中的数据比较多，就可能会出现一种情况：客户端向实例2发送请求，但此时，Slot 2中的数据只有一部分迁移到了实例3，还有部分数据没有迁移。在这种迁移部分完成的情况下，客户端就会收到一条ASK报错信息，如下所示：\n\n```shell\nGET hello:key\n(error) ASK 13320 172.16.19.5:6379\n```\n\n这个结果中的ASK命令就表示，客户端请求的键值对所在的哈希槽13320，在172.16.19.5这个实例上，但是这个哈希槽正在迁移。此时，客户端需要先给172.16.19.5这个实例发送一个ASKING命令。这个命令的意思是，让这个实例允许执行客户端接下来发送的命令。然后，客户端再向这个实例发送GET命令，以读取数据。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4s7qq3egsj22bc1qi7wh.jpg\" alt=\"image\" style=\"zoom:25%;\" />\n\n**摘选自:极客时间-Redis核心技术与实战**","tags":["redis"]},{"title":"Redis-3-RDB","url":"/2022/10/27/cyb-mds/module/Redis/Redis-4-RDB/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n\n\n#### 什么是RDB\n\nRDB(Redis DataBase)就是把某一时刻的状态以文件的形式写到磁盘上，也就是**内存快照**。\n\n#### 给哪些内存数据做快照？\n\nRedis的数据都在内存中，为了提供所有数据的可靠性保证，它执行的是**全量快照**\n\n#### RDB方式\n\n对于Redis而言，它的单线程模型就决定了，我们要尽量避免所有会阻塞主线程的操作，所以，针对任何操作，我们都会提一个灵魂之问：“它会阻塞主线程吗?”RDB文件的生成是否会阻塞主线程，这就关系到是否会降低Redis的性能。\n\nRedis提供了两个命令来生成RDB文件，分别是save和bgsave。\n\n- save：在主线程中执行，会导致阻塞；\n- bgsave：创建一个子进程，专门用于写入RDB文件，避免了主线程的阻塞，这也是Redis RDB文件生成的默认配置。\n\n#### 快照时数据能修改吗\n\n简单来说，bgsave子进程是由主线程fork生成的，可以共享主线程的所有内存数据。bgsave子进程运行后，开始读取主线程的内存数据，并把它们写入RDB文件。\n\n此时，如果主线程对这些数据也都是读操作（例如图中的键值对A），那么，主线程和bgsave子进程相互不影响。但是，如果主线程要修改一块数据（例如图中的键值对C），那么，这块数据就会被复制一份，生成该数据的副本。然后，bgsave子进程会把这个副本数据写入RDB文件，而在这个过程中，主线程仍然可以直接修改原来的数据。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4j7oufa25j23341qiwzj.jpg\" alt=\"image\" style=\"zoom:25%;\" />\n\n这既保证了快照的完整性，也允许主线程同时对数据进行修改，避免了对正常业务的影响。\n\n\n\n#### 可以每秒做一次快照吗？\n\n虽然bgsave执行时不阻塞主线程，但是，**如果频繁地执行全量快照，也会带来两方面的开销**。\n\n一方面，频繁将全量数据写入磁盘，会给磁盘带来很大压力，多个快照竞争有限的磁盘带宽，前一个快照还没有做完，后一个又开始做了，容易造成恶性循环。\n\n另一方面，bgsave子进程需要通过fork操作从主线程创建出来。虽然，子进程在创建后不会再阻塞主线程，但是，fork这个创建过程本身会阻塞主线程，而且主线程的内存越大，阻塞时间越长。如果频繁fork出bgsave子进程，这就会频繁阻塞主线程了。那么，有什么其他好方法吗？\n\n\n\n#### 如何平衡\n\nRedis 4.0中提出了一个**混合使用AOF日志和内存快照**的方法。简单来说，内存快照以一定的频率执行，在两次快照之间，使用AOF日志记录这期间的所有命令操作。\n\n如下图所示，T1和T2时刻的修改，用AOF日志记录，等到第二次做全量快照时，就可以清空AOF日志，因为此时的修改都已经记录到快照中了，恢复时就不再用日志了。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/e4c5846616c19fe03dbf528437beb320.jpg\" alt=\"img\" style=\"zoom:25%;\" />\n\n最后，关于AOF和RDB的选择问题，我想再给你提三点建议：\n\n- 数据不能丢失时，内存快照和AOF的混合使用是一个很好的选择；\n- 如果允许分钟级别的数据丢失，可以只使用RDB；\n- 如果只用AOF，优先使用everysec的配置选项，因为它在可靠性和性能之间取了一个平衡。\n\n**摘选自:极客时间-Redis核心技术与实战**\n","tags":["redis"]},{"title":"Redis-3-AOF","url":"/2022/10/27/cyb-mds/module/Redis/Redis-3-AOF/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n#### AOF日志是如何实现的\n\n说到日志，我们比较熟悉的是数据库的写前日志（Write Ahead Log, WAL），也就是说，在实际写数据前，先把修改的数据记到日志文件中，以便故障时进行恢复。不过，AOF日志正好相反，它是写后日志，“写后”的意思是Redis是先执行命令，把数据写入内存，然后才记录日志，如下图所示：\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4iwncalmvj22he1dpjxs.jpg\" alt=\"image\" style=\"zoom: 16%;\" />\n\n\n\n#### AOF为什么要先执行命令再记日志呢\n\nAOF里记录的是Redis收到的每一条命令，这些命令是以文本形式保存的。同时为了避免额外的检查开销，Redis在向AOF里面记录日志的时候，并不会先去对这些命令进行语法检查。所以，如果先记日志再执行命令的话，日志中就有可能记录了错误的命令，Redis在使用日志恢复数据时，就可能会出错。而写后日志这种方式，就是先让系统执行命令，只有命令能执行成功，才会被记录到日志中，否则，系统就会直接向客户端报错。所以，Redis使用写后日志这一方式的一大好处是，可以避免出现记录错误命令的情况。\n\n除此之外，AOF还有一个好处：它是在命令执行后才记录日志，所以**不会阻塞当前的写操作**。\n\n\n\n不过，AOF也有两个潜在的风险。\n\n首先，如果刚执行完一个命令，还没有来得及记日志就宕机了，那么这个命令和相应的数据就有丢失的风险。\n\n其次，AOF虽然避免了对当前命令的阻塞，但可能会给下一个操作带来阻塞风险。这是因为，AOF日志也是在主线程中执行的，如果在把日志文件写入磁盘时，磁盘写压力大，就会导致写盘很慢，进而导致后续的操作也无法执行了。\n\n\n\n#### 三种写回策略\n\n对于这个问题，AOF机制给我们提供了三个选择，也就是AOF配置项appendfsync的三个可选值。\n\n- **Always**，同步写回：每个写命令执行完，立马同步地将日志写回磁盘；\n- **Everysec**，每秒写回：每个写命令执行完，只是先把日志写到AOF文件的内存缓冲区，每隔一秒把缓冲区中的内容写入磁盘；\n- **No**，操作系统控制的写回：每个写命令执行完，只是先把日志写到AOF文件的内存缓冲区，由操作系统决定何时将缓冲区内容写回磁盘。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4ix59iqbpj21rg0iy7jm.jpg\" alt=\"image\" style=\"zoom:30%;\" />\n\n总结一下就是：想要获得高性能，就选择No策略；如果想要得到高可靠性保证，就选择Always策略；如果允许数据有一点丢失，又希望性能别受太大影响的话，那么就选择Everysec策略。\n\n随着接收的写命令越来越多，AOF文件会越来越大。一是，文件系统本身对文件大小有限制，无法保存过大的文件；二是，如果文件太大，之后再往里面追加命令记录的话，效率也会变低；三是，如果发生宕机，AOF中记录的命令要一个个被重新执行，用于故障恢复，如果日志文件太大，整个恢复过程就会非常缓慢，这就会影响到Redis的正常使用。所以，我们就要采取一定的控制手段，这个时候，**AOF重写机制**就登场了。\n\n\n\n#### AOF重写\n\nAOF重写机制就是在重写时，Redis根据数据库的现状创建一个新的AOF文件，也就是说，读取数据库中的所有键值对，然后对每一个键值对用一条命令记录它的写入。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4ixjuul32j23340u8wtz.jpg\" alt=\"image\" style=\"zoom: 25%;\" />\n\n当我们对一个列表先后做了6次修改操作后，列表的最后状态是[“D”, “C”, “N”]，此时，只用LPUSH u:list “N”, “C”, \"D\"这一条命令就能实现该数据的恢复，这就节省了五条命令的空间。对于被修改过成百上千次的键值对来说，重写能节省的空间当然就更大了。\n\n不过，虽然AOF重写后，日志文件会缩小，但是，要把整个数据库的最新数据的操作日志都写回磁盘，仍然是一个非常耗时的过程。这时，我们就要继续关注另一个问题了：重写会不会阻塞主线程？\n\n\n\n#### AOF重写会阻塞吗\n\n我把重写的过程总结为“**一个拷贝，两处日志**”。\n\n“一个拷贝”就是指，每次执行重写时，主线程fork出后台的bgrewriteaof子进程。此时，fork会把主线程的内存拷贝一份给bgrewriteaof子进程，这里面就包含了数据库的最新数据。然后，bgrewriteaof子进程就可以在不影响主线程的情况下，逐一把拷贝的数据写成操作，记入重写日志。\n\n“两处日志”又是什么呢？\n\n因为主线程未阻塞，仍然可以处理新来的操作。此时，如果有写操作，第一处日志就是指正在使用的AOF日志，Redis会把这个操作写到它的缓冲区。这样一来，即使宕机了，这个AOF日志的操作仍然是齐全的，可以用于恢复。\n\n而第二处日志，就是指新的AOF重写日志。这个操作也会被写到重写日志的缓冲区。这样，重写日志也不会丢失最新的操作。等到拷贝数据的所有操作记录重写完成后，重写日志记录的这些最新操作也会写入新的AOF文件，以保证数据库最新状态的记录。此时，我们就可以用新的AOF文件替代旧文件了。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4j5msehajj23341g0kee.jpg\" alt=\"image\" style=\"zoom: 25%;\" />\n\n总结来说，每次AOF重写时，Redis会先执行一个内存拷贝，用于重写；然后，使用两个日志保证在重写过程中，新写入的数据不会丢失。而且，因为Redis采用额外的线程进行数据重写，所以，这个过程并不会阻塞主线程。\n\n\n\n**摘选自:极客时间-Redis核心技术与实战**","tags":["redis"]},{"title":"Redis-2-单线程","url":"/2022/10/27/cyb-mds/module/Redis/Redis-2-单线程/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n\n\n#### 简介\n\n我们通常说，Redis 是单线程，**主要是指 Redis 的网络 IO和键值对读写是由一个线程来完成的**，这也是 Redis 对外提供键值存储服务的主要流程。但 Redis 的其他功能，比如持久化、异步删除、集群数据同步等，其实是由额外的线程执行的。\n所以，严格来说，Redis 并不是单线程，但是我们一般把 Redis 称为单线程高性能，这样显得“酷”些。接下来，我也会把 Redis 称为单线程模式。\n\n#### 为什么使用单线程\n\n通常情况下，在我们采用多线程后，如果没有良好的系统设计，实际得到的结果，其实是右图所展示的那样。我们刚开始增加线程数时，系统吞吐率会增加，但是，再进一步增加线程时，系统吞吐率就增长迟缓了，有时甚至还会出现下降的情况。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4fsxx892cj21rg0nuq74.jpg\" alt=\"image\" style=\"zoom: 33%;\" />\n\n为什么会出现这种情况呢？一个关键的瓶颈在于，系统中通常会存在被多线程同时访问的共享资源，比如一个共享的数据结构。当有多个线程要修改这个共享资源时，为了保证共享资源的正确性，就需要有额外的机制进行保证，而这个额外的机制，就会带来额外的开销。\n\n并发访问控制一直是多线程开发中的一个难点问题，如果没有精细的设计，比如说，只是简单地采用一个粗粒度互斥锁，就会出现不理想的结果：即使增加了线程，大部分线程也在等待获取访问共享资源的互斥锁，并行变串行，系统吞吐率并没有随着线程的增加而增加。\n而且，采用多线程开发一般会引入同步原语来保护共享资源的并发访问，这也会降低系统代码的易调试性和可维护性。为了避免这些问题，Redis 直接采用了单线程模式。\n\n#### 单线程 Redis 为什么那么快\n\n一方面，Redis 的大部分操作在内存上完成，再加上它采用了高效的数据结构，例如哈希表和跳表，这是它实现高性能的一个重要原因。另一方面，就是 Redis 采用了多路复用机制，使其在网络 IO 操作中能并发处理大量的客户端请求，实现高吞吐率。\n\n\n\n\n\n**摘选自:极客时间-Redis核心技术与实战**\n\n","tags":["redis"]},{"title":"Redis-17-分布式锁","url":"/2022/10/27/cyb-mds/module/Redis/Redis-17-分布式锁/","content":"\n==作者：YB-Chi==\n\n[toc]\n\nRedis属于分布式系统，当有多个客户端需要争抢锁时，我们必须要保证，**这把锁不能是某个客户端本地的锁**。否则的话，其它客户端是无法访问这把锁的，当然也就不能获取这把锁了。所以，在分布式系统中，当有多个客户端需要获取锁时，我们需要分布式锁。\n\n### 单机上的锁和分布式锁的联系与区别\n\n对于在单机上运行的多线程程序来说，锁本身可以用一个变量表示。\n\n- 变量值为0时，表示没有线程获取锁；\n- 变量值为1时，表示已经有线程获取到锁了。\n\n我们通常说的线程调用加锁和释放锁的操作，到底是啥意思呢？我来解释一下。实际上，一个线程调用加锁操作，其实就是检查锁变量值是否为0。如果是0，就把锁的变量值设置为1，表示获取到锁，如果不是0，就返回错误信息，表示加锁失败，已经有别的线程获取到锁了。而一个线程调用释放锁操作，其实就是将锁变量的值置为0，以便其它线程可以来获取锁。\n\n```java\nacquire_lock(){\n  if lock == 0\n     lock = 1\n     return 1\n  else\n     return 0\n} \n\nrelease_lock(){\n  lock = 0\n  return 1\n}\n```\n\n和单机上的锁类似，分布式锁同样可以**用一个变量来实现**。客户端加锁和释放锁的操作逻辑，也和单机上的加锁和释放锁操作逻辑一致：**加锁时同样需要判断锁变量的值，根据锁变量值来判断能否加锁成功；释放锁时需要把锁变量值设置为0，表明客户端不再持有锁**。\n\n但是，和线程在单机上操作锁不同的是，在分布式场景下，**锁变量需要由一个共享存储系统来维护**，只有这样，多个客户端才可以通过访问共享存储系统来访问锁变量。相应的，**加锁和释放锁的操作就变成了读取、判断和设置共享存储系统中的锁变量值**。\n\n这样一来，我们就可以得出实现分布式锁的两个要求。\n\n- 要求一：分布式锁的加锁和释放锁的过程，涉及多个操作。所以，在实现分布式锁时，我们需要保证这些锁操作的原子性；\n- 要求二：共享存储系统保存了锁变量，如果共享存储系统发生故障或宕机，那么客户端也就无法进行锁操作了。在实现分布式锁时，我们需要考虑保证共享存储系统的可靠性，进而保证锁的可靠性。\n\n其实，**我们既可以基于单个Redis节点来实现，也可以使用多个Redis节点实现**。在这两种情况下，锁的可靠性是不一样的。我们先来看基于单个Redis节点的实现方法。\n\n### 基于单个Redis节点实现分布式锁\n\n作为分布式锁实现过程中的共享存储系统，Redis可以使用键值对来保存锁变量，再接收和处理不同客户端发送的加锁和释放锁的操作请求。那么，键值对的键和值具体是怎么定的呢？\n\n我们要赋予锁变量一个变量名，把这个变量名作为键值对的键，而锁变量的值，则是键值对的值，这样一来，Redis就能保存锁变量了，客户端也就可以通过Redis的命令操作来实现锁操作。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h5c6bfdl51j226c1qi4kc.jpg\" alt=\"image\" style=\"zoom:32%;\" />\n\n我们再来分析下加锁操作。\n\n在图中，客户端A和C同时请求加锁。因为Redis使用单线程处理请求，所以，即使客户端A和C同时把加锁请求发给了Redis，Redis也会串行处理它们的请求。\n\n我们假设Redis先处理客户端A的请求，读取lock_key的值，发现lock_key为0，所以，Redis就把lock_key的value置为1，表示已经加锁了。紧接着，Redis处理客户端C的请求，此时，Redis会发现lock_key的值已经为1了，所以就返回加锁失败的信息。\n\n刚刚说的是加锁的操作，那释放锁该怎么操作呢？其实，释放锁就是直接把锁变量值设置为0。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h5c6ex7pmzj22bc1qian9.jpg)\n\n这张图片展示了客户端A请求释放锁的过程。当客户端A持有锁时，锁变量lock_key的值为1。客户端A执行释放锁操作后，Redis将lock_key的值置为0，表明已经没有客户端持有锁了。\n\n因为加锁包含了三个操作（读取锁变量、判断锁变量值以及把锁变量值设置为1），而这三个操作在执行时需要保证原子性。那怎么保证原子性呢？要想保证操作的原子性，有两种通用的方法，分别是使用Redis的单命令操作和使用Lua脚本。那么，在分布式加锁场景下，该怎么应用这两个方法呢？\n\n我们先来看下，Redis可以用哪些单命令操作实现加锁操作。\n\n首先是SETNX命令，它用于设置键值对的值。具体来说，就是这个命令在执行时会判断键值对是否存在，如果不存在，就设置键值对的值，如果存在，就不做任何设置。\n\n举个例子，如果执行下面的命令时，key不存在，那么key会被创建，并且值会被设置为value；如果key已经存在，SETNX不做任何赋值操作。\n\n```shell\nSETNX key value\n```\n\n对于释放锁操作来说，我们可以在执行完业务逻辑后，使用DEL命令删除锁变量。不过，你不用担心锁变量被删除后，其他客户端无法请求加锁了。因为SETNX命令在执行时，如果要设置的键值对（也就是锁变量）不存在，SETNX命令会先创建键值对，然后设置它的值。所以，释放锁之后，再有客户端请求加锁时，SETNX命令会创建保存锁变量的键值对，并设置锁变量的值，完成加锁。\n\n总结来说，我们就可以用SETNX和DEL命令组合来实现加锁和释放锁操作。下面的伪代码示例显示了锁操作的过程，你可以看下。\n\n```shell\n// 加锁\nSETNX lock_key 1\n// 业务逻辑\nDO THINGS\n// 释放锁\nDEL lock_key\n```\n\n不过，使用SETNX和DEL命令组合实现分布锁，存在两个潜在的风险。\n\n第一个风险是，假如某个客户端在执行了SETNX命令、加锁之后，紧接着却在操作共享数据时发生了异常，结果一直没有执行最后的DEL命令释放锁。因此，锁就一直被这个客户端持有，其它客户端无法拿到锁，也无法访问共享数据和执行后续操作，这会给业务应用带来影响。\n\n针对这个问题，一个有效的解决方法是，**给锁变量设置一个过期时间**。这样一来，即使持有锁的客户端发生了异常，无法主动地释放锁，Redis也会根据锁变量的过期时间，在锁变量过期后，把它删除。其它客户端在锁变量过期后，就可以重新请求加锁，这就不会出现无法加锁的问题了。\n\n我们再来看第二个风险。如果客户端A执行了SETNX命令加锁后，假设客户端B执行了DEL命令释放锁，此时，客户端A的锁就被误释放了。如果客户端C正好也在申请加锁，就可以成功获得锁，进而开始操作共享数据。这样一来，客户端A和C同时在对共享数据进行操作，数据就会被修改错误，这也是业务层不能接受的。\n\n为了应对这个问题，我们需要**能区分来自不同客户端的锁操作**，具体咋做呢？其实，我们可以在锁变量的值上想想办法。\n\n在使用SETNX命令进行加锁的方法中，我们通过把锁变量值设置为1或0，表示是否加锁成功。1和0只有两种状态，无法表示究竟是哪个客户端进行的锁操作。所以，我们在加锁操作时，可以让每个客户端给锁变量设置一个唯一值，这里的唯一值就可以用来标识当前操作的客户端。在释放锁操作时，客户端需要判断，当前锁变量的值是否和自己的唯一标识相等，只有在相等的情况下，才能释放锁。这样一来，就不会出现误释放锁的问题了。\n\n知道了解决方案，那么，在Redis中，具体是怎么实现的呢？我们再来了解下。\n\n在查看具体的代码前，我要先带你学习下Redis的SET命令。\n\n我们刚刚在说SETNX命令的时候提到，对于不存在的键值对，它会先创建再设置值（也就是“不存在即设置”），为了能达到和SETNX命令一样的效果，Redis给SET命令提供了类似的选项NX，用来实现“不存在即设置”。如果使用了NX选项，SET命令只有在键值对不存在时，才会进行设置，否则不做赋值操作。此外，SET命令在执行时还可以带上EX或PX选项，用来设置键值对的过期时间。\n\n举个例子，执行下面的命令时，只有key不存在时，SET才会创建key，并对key进行赋值。另外，**key的存活时间由seconds或者milliseconds选项值来决定**。\n\n```\nSET key value [EX seconds | PX milliseconds]  [NX]\n```\n\n有了SET命令的NX和EX/PX选项后，我们就可以用下面的命令来实现加锁操作了。\n\n```\n// 加锁, unique_value作为客户端唯一性的标识\nSET lock_key unique_value NX PX 10000\n```\n\n其中，unique_value是客户端的唯一标识，可以用一个随机生成的字符串来表示，PX 10000则表示lock_key会在10s后过期，以免客户端在这期间发生异常而无法释放锁。\n\n因为在加锁操作中，每个客户端都使用了一个唯一标识，所以在释放锁操作时，我们需要判断锁变量的值，是否等于执行释放锁操作的客户端的唯一标识，如下所示：\n\n```\n//释放锁 比较unique_value是否相等，避免误释放\nif redis.call(\"get\",KEYS[1]) == ARGV[1] then\n    return redis.call(\"del\",KEYS[1])\nelse\n    return 0\nend\n```\n\n这是使用Lua脚本（unlock.script）实现的释放锁操作的伪代码，其中，KEYS[1]表示lock_key，ARGV[1]是当前客户端的唯一标识，这两个值都是我们在执行Lua脚本时作为参数传入的。\n\n最后，我们执行下面的命令，就可以完成锁释放操作了。\n\n```\nredis-cli  --eval  unlock.script lock_key , unique_value \n```\n\n你可能也注意到了，在释放锁操作中，我们使用了Lua脚本，这是因为，释放锁操作的逻辑也包含了读取锁变量、判断值、删除锁变量的多个操作，而Redis在执行Lua脚本时，可以以原子性的方式执行，从而保证了锁释放操作的原子性。\n\n好了，到这里，你了解了如何使用SET命令和Lua脚本在Redis单节点上实现分布式锁。但是，我们现在只用了一个Redis实例来保存锁变量，如果这个Redis实例发生故障宕机了，那么锁变量就没有了。此时，客户端也无法进行锁操作了，这就会影响到业务的正常执行。所以，我们在实现分布式锁时，还需要保证锁的可靠性。那怎么提高呢？这就要提到基于多个Redis节点实现分布式锁的方式了。\n\n### 基于多个Redis节点实现高可靠的分布式锁\n\n当我们要实现高可靠的分布式锁时，就不能只依赖单个的命令操作了，我们需要按照一定的步骤和规则进行加解锁操作，否则，就可能会出现锁无法工作的情况。“一定的步骤和规则”是指啥呢？其实就是分布式锁的算法。\n\n为了避免Redis实例故障而导致的锁无法工作的问题，Redis的开发者Antirez提出了分布式锁算法Redlock。\n\nRedlock算法的基本思路，是让客户端和多个独立的Redis实例依次请求加锁，如果客户端能够和半数以上的实例成功地完成加锁操作，那么我们就认为，客户端成功地获得分布式锁了，否则加锁失败。这样一来，即使有单个Redis实例发生故障，因为锁变量在其它实例上也有保存，所以，客户端仍然可以正常地进行锁操作，锁变量并不会丢失。\n\n我们来具体看下Redlock算法的执行步骤。Redlock算法的实现需要有N个独立的Redis实例。接下来，我们可以分成3步来完成加锁操作。\n\n**第一步是，客户端获取当前时间。**\n\n**第二步是，客户端按顺序依次向N个Redis实例执行加锁操作。**\n\n这里的加锁操作和在单实例上执行的加锁操作一样，使用SET命令，带上NX，EX/PX选项，以及带上客户端的唯一标识。当然，如果某个Redis实例发生故障了，为了保证在这种情况下，Redlock算法能够继续运行，我们需要给加锁操作设置一个超时时间。\n\n如果客户端在和一个Redis实例请求加锁时，一直到超时都没有成功，那么此时，客户端会和下一个Redis实例继续请求加锁。加锁操作的超时时间需要远远地小于锁的有效时间，一般也就是设置为几十毫秒。\n\n**第三步是，一旦客户端完成了和所有Redis实例的加锁操作，客户端就要计算整个加锁过程的总耗时。**\n\n客户端只有在满足下面的这两个条件时，才能认为是加锁成功。\n\n- 条件一：客户端从超过半数（大于等于 N/2+1）的Redis实例上成功获取到了锁；\n- 条件二：客户端获取锁的总耗时没有超过锁的有效时间。\n\n在满足了这两个条件后，我们需要重新计算这把锁的有效时间，计算的结果是锁的最初有效时间减去客户端为获取锁的总耗时。如果锁的有效时间已经来不及完成共享数据的操作了，我们可以释放锁，以免出现还没完成数据操作，锁就过期了的情况。\n\n当然，如果客户端在和所有实例执行完加锁操作后，没能同时满足这两个条件，那么，客户端向所有Redis节点发起释放锁的操作。\n\n在Redlock算法中，释放锁的操作和在单实例上释放锁的操作一样，只要执行释放锁的Lua脚本就可以了。这样一来，只要N个Redis实例中的半数以上实例能正常工作，就能保证分布式锁的正常工作了。\n\n所以，在实际的业务应用中，如果你想要提升分布式锁的可靠性，就可以通过Redlock算法来实现。\n\n### 小结\n\n分布式锁是由共享存储系统维护的变量，多个客户端可以向共享存储系统发送命令进行加锁或释放锁操作。Redis作为一个共享存储系统，可以用来实现分布式锁。\n\n在基于单个Redis实例实现分布式锁时，对于加锁操作，我们需要满足三个条件。\n\n1. 加锁包括了读取锁变量、检查锁变量值和设置锁变量值三个操作，但需要以原子操作的方式完成，所以，我们使用SET命令带上NX选项来实现加锁；\n2. 锁变量需要设置过期时间，以免客户端拿到锁后发生异常，导致锁一直无法释放，所以，我们在SET命令执行时加上EX/PX选项，设置其过期时间；\n3. 锁变量的值需要能区分来自不同客户端的加锁操作，以免在释放锁时，出现误释放操作，所以，我们使用SET命令设置锁变量值时，每个客户端设置的值是一个唯一值，用于标识客户端。\n\n和加锁类似，释放锁也包含了读取锁变量值、判断锁变量值和删除锁变量三个操作，不过，我们无法使用单个命令来实现，所以，我们可以采用Lua脚本执行释放锁操作，通过Redis原子性地执行Lua脚本，来保证释放锁操作的原子性。\n\n不过，基于单个Redis实例实现分布式锁时，会面临实例异常或崩溃的情况，这会导致实例无法提供锁操作，正因为此，Redis也提供了Redlock算法，用来实现基于多个实例的分布式锁。这样一来，锁变量由多个实例维护，即使有实例发生了故障，锁变量仍然是存在的，客户端还是可以完成锁操作。Redlock算法是实现高可靠分布式锁的一种有效解决方案，你可以在实际应用中把它用起来。","tags":["redis"]},{"title":"Redis-16-无锁原子操作,如何应对并发","url":"/2022/10/27/cyb-mds/module/Redis/Redis-16-无锁原子操作,如何应对并发/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n### 简介\n\n一旦有了并发写操作，数据就会被修改，如果我们没有对并发写请求做好控制，就可能导致数据被改错，影响到业务的正常使用（例如库存数据错误，导致下单异常）。\n\n为了保证并发访问的正确性，Redis提供了两种方法，分别是**加锁**和**原子操作**。\n\n1. **加锁**是一种常用的方法，在读取数据前，客户端需要先获得锁，否则就无法进行操作。当一个客户端获得锁后，就会一直持有这把锁，直到客户端完成数据更新，才释放这把锁。\n\n   看上去好像是一种很好的方案，但是，其实这里会有两个问题：一个是，如果加锁操作多，会降低系统的并发访问性能；第二个是，Redis客户端要加锁时，需要用到分布式锁，而分布式锁实现复杂，需要用额外的存储系统来提供加解锁操作。\n\n2. **原子操作是另一种提供并发访问控制的方法**。原子操作是指执行过程保持原子性的操作，而且原子操作执行时并不需要再加锁，实现了无锁操作。这样一来，既能保证并发控制，还能减少对系统并发性能的影响。原子操作的目标是实现并发访问控制，那么当有并发访问请求时，我们具体需要控制什么呢？\n\n### 并发访问中需要对什么进行控制？\n\n并发访问控制对应的操作主要是数据修改操作。当客户端需要修改数据时，基本流程分成两步：\n\n1. 客户端先把数据读取到本地，在本地进行修改；\n2. 客户端修改完数据后，再写回Redis。\n\n我们把这个流程叫做“读取-修改-写回”操作（Read-Modify-Write，简称为RMW操作）。当有多个客户端对同一份数据执行RMW操作的话，我们就需要让RMW操作涉及的代码以原子性方式执行。访问同一份数据的RMW操作代码，就叫做临界区代码。\n\n不过，当有多个客户端并发执行临界区代码时，就会存在一些潜在问题。我们先看下临界区代码。假设客户端要对商品库存执行扣减1的操作，伪代码如下所示：\n\n```java\ncurrent = GET(id)\ncurrent--\nSET(id, current)\n```\n\n可以看到，客户端首先会根据商品id，从Redis中读取商品当前的库存值current（对应Read)，然后，客户端对库存值减1（对应Modify），再把库存值写回Redis（对应Write）。当有多个客户端执行这段代码时，这就是一份临界区代码。\n\n如果我们对临界区代码的执行没有控制机制，就会出现数据更新错误。在刚才的例子中，假设现在有两个客户端A和B，同时执行刚才的临界区代码，就会出现错误。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h5avwvcvgkj22991e1wpy.jpg)\n\n如果按正确的逻辑处理，客户端A和B对库存值各做了一次扣减，库存值应该为8。所以，这里的库存值明显更新错了。\n\n为了保证数据并发修改的正确性，我们可以用锁把并行操作变成串行操作，串行操作就具有互斥性。一个客户端持有锁后，其他客户端只能等到锁释放，才能拿锁再进行修改。\n\n下面的伪代码显示了使用锁来控制临界区代码的执行情况。\n\n```java\nLOCK()\ncurrent = GET(id)\ncurrent--\nSET(id, current)\nUNLOCK()\n```\n\n虽然加锁保证了互斥性，但是**加锁也会导致系统并发性能降低**。\n\n![img](https://raw.githubusercontent.com/chiyuanbo/pic/master/845b4694700264482d64a3dbb7a36525.jpg)\n\n\n\n### Redis的两种原子操作方法\n\n为了实现并发控制要求的临界区代码互斥执行，Redis的原子操作采用了两种方法：\n\n1. 把多个操作在Redis中实现成一个操作，也就是单命令操作；\n2. 把多个操作写到一个Lua脚本中，以原子性方式执行单个Lua脚本。\n\n**单命令操作**\n\n你可能也注意到了，虽然Redis的单个命令操作可以原子性地执行，但是在实际应用中，数据修改时可能包含多个操作，至少包括读数据、数据增减、写回数据三个操作，这显然就不是单个命令操作了，那该怎么办呢？\n\n别担心，Redis提供了INCR/DECR命令，把这三个操作转变为一个原子操作了。INCR/DECR命令可以对数据进行**增值/减值**操作，而且它们本身就是单个命令操作，Redis在执行它们时，本身就具有互斥性。\n\n```\nDECR id \n```\n\n所以，如果我们执行的RMW操作是对数据进行增减值的话，Redis提供的原子操作INCR和DECR可以直接帮助我们进行并发控制。\n\n但是，如果我们要执行的操作不是简单地增减数据，而是有更加复杂的判断逻辑或者是其他操作，那么，Redis的单命令操作已经无法保证多个操作的互斥执行了。所以，这个时候，我们需要使用第二个方法，也就是Lua脚本。\n\n**Lua脚本**\n\nRedis会把整个Lua脚本作为一个整体执行，在执行的过程中不会被其他命令打断，从而保证了Lua脚本中操作的原子性。如果我们有多个操作要执行，但是又无法用INCR/DECR这种命令操作来实现，就可以把这些要执行的操作编写到一个Lua脚本中。然后，我们可以使用Redis的EVAL命令来执行脚本。这样一来，这些操作在执行时就具有了互斥性。\n\n当一个业务应用的访问用户增加时，我们有时需要限制某个客户端在一定时间范围内的访问次数，比如爆款商品的购买限流、社交网络中的每分钟点赞次数限制等。\n\n那该怎么限制呢？我们可以把客户端IP作为key，把客户端的访问次数作为value，保存到Redis中。客户端每访问一次后，我们就用INCR增加访问次数。\n\n不过，在这种场景下，客户端限流其实同时包含了对访问次数和时间范围的限制，例如每分钟的访问次数不能超过20。所以，我们可以在客户端第一次访问时，给对应键值对设置过期时间，例如设置为60s后过期。同时，在客户端每次访问时，我们读取客户端当前的访问次数，如果次数超过阈值，就报错，限制客户端再次访问。你可以看下下面的这段代码，它实现了对客户端每分钟访问次数不超过20次的限制。\n\n```c++\n//获取ip对应的访问次数\ncurrent = GET(ip)\n//如果超过访问次数超过20次，则报错\nIF current != NULL AND current > 20 THEN\n    ERROR \"exceed 20 accesses per second\"\nELSE\n    //如果访问次数不足20次，增加一次访问计数\n    value = INCR(ip)\n    //如果是第一次访问，将键值对的过期时间设置为60s后\n    IF value == 1 THEN\n        EXPIRE(ip,60)\n    END\n    //执行其他操作\n    DO THINGS\nEND\n```\n\n可以看到，在这个例子中，我们已经使用了INCR来原子性地增加计数。但是，客户端限流的逻辑不只有计数，还包括**访问次数判断和过期时间设置**。\n\n对于这些操作，我们同样需要保证它们的原子性。否则，如果客户端使用多线程访问，访问次数初始值为0，第一个线程执行了INCR(ip)操作后，第二个线程紧接着也执行了INCR(ip)，此时，ip对应的访问次数就被增加到了2，我们就无法再对这个ip设置过期时间了。这样就会导致，这个ip对应的客户端访问次数达到20次之后，就无法再进行访问了。即使过了60s，也不能再继续访问，显然不符合业务要求。\n\n所以，这个例子中的操作无法用Redis单个命令来实现，此时，我们就可以使用Lua脚本来保证并发控制。我们可以把访问次数加1、判断访问次数是否为1，以及设置过期时间这三个操作写入一个Lua脚本，如下所示：\n\n```lua\nlocal current\ncurrent = redis.call(\"incr\",KEYS[1])\nif tonumber(current) == 1 then\n    redis.call(\"expire\",KEYS[1],60)\nend\n```\n\n假设我们编写的脚本名称为lua.script，我们接着就可以使用Redis客户端，带上eval选项，来执行该脚本。脚本所需的参数将通过以下命令中的keys和args进行传递。\n\n```shell\nredis-cli  --eval lua.script  keys , args\n```\n\n这样一来，访问次数加1、判断访问次数是否为1，以及设置过期时间这三个操作就可以原子性地执行了。即使客户端有多个线程同时执行这个脚本，Redis也会依次串行执行脚本代码，避免了并发操作带来的数据错误。\n\n### 小结\n\n在并发访问时，并发的RMW操作会导致数据错误，所以需要进行并发控制。所谓并发控制，就是要保证临界区代码的互斥执行。\n\nRedis提供了两种原子操作的方法来实现并发控制，分别是单命令操作和Lua脚本。因为原子操作本身不会对太多的资源限制访问，可以维持较高的系统并发性能。\n\n但是，单命令原子操作的适用范围较小，并不是所有的RMW操作都能转变成单命令的原子操作（例如INCR/DECR命令只能在读取数据后做原子增减），当我们需要对读取的数据做更多判断，或者是我们对数据的修改不是简单的增减时，单命令操作就不适用了。\n\n而Redis的Lua脚本可以包含多个操作，这些操作都会以原子性的方式执行，绕开了单命令操作的限制。不过，如果把很多操作都放在Lua脚本中原子执行，会导致Redis执行脚本的时间增加，同样也会降低Redis的并发性能。所以，我给你一个小建议：**在编写Lua脚本时，你要避免把不\\**\\*\\*需要\\*\\**\\*做并发控制的操作写入脚本中**。\n\n当然，加锁也能实现临界区代码的互斥执行，只是如果有多个客户端加锁时，就需要分布式锁的支持了。\n\n**摘选自:极客时间-Redis核心技术与实战**","tags":["redis"]},{"title":"Redis-13-数据一致性","url":"/2022/10/27/cyb-mds/module/Redis/Redis-13-数据一致性/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n### 缓存和数据库的数据不一致是如何发生的？\n\n首先，我们得清楚“数据的一致性”具体是啥意思。其实，这里的“一致性”包含了两种情况：\n\n- 缓存中有数据，那么，缓存的数据值需要和数据库中的值相同；\n- 缓存中本身没有数据，那么，数据库中的值必须是最新值。\n\n不符合这两种情况的，就属于缓存和数据库的数据不一致问题了。不过，当缓存的读写模式不同时，缓存数据不一致的发生情况不一样，我们的应对方法也会有所不同。\n\n**读写缓存**：如果要对数据进行增删改，就需要在缓存中进行，同时还要根据采取的写回策略，决定是否同步写回到数据库中。\n\n- 同步直写策略：写缓存时，也同步写数据库，缓存和数据库中的数据一致；\n- 异步写回策略：写缓存时不同步写数据库，等到数据从缓存中淘汰时，再写回数据库。使用这种策略时，如果数据还没有写回数据库，缓存就发生了故障，那么，此时，数据库就没有最新的数据了。\n\n所以，对于读写缓存来说，要想保证缓存和数据库中的数据一致，就要采用同步直写策略。不过，需要注意的是，如果采用这种策略，就需要同时更新缓存和数据库。所以，我们要在业务应用中使用事务机制，来保证缓存和数据库的更新具有原子性，也就是说，两者要不一起更新，要不都不更新，返回错误信息，进行重试。否则，我们就无法实现同步直写。\n\n当然，在有些场景下，我们对数据一致性的要求可能不是那么高，比如说缓存的是电商商品的非关键属性或者短视频的创建或修改时间等，那么，我们可以使用异步写回策略。\n\n\n\n**只读缓存**：如果有数据新增，会直接写入数据库；而有数据删改时，就需要把只读缓存中的数据标记为无效。这样一来，应用后续再访问这些增删改的数据时，因为缓存中没有相应的数据，就会发生缓存缺失。此时，应用再从数据库中把数据读入缓存，这样后续再访问数据时，就能够直接从缓存中读取了。如下图\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h52nvtavojj22bc115qef.jpg\" alt=\"image\" style=\"zoom: 25%;\" />\n\n那么，这个过程中会不会出现数据不一致的情况呢？考虑到新增数据和删改数据的情况不一样，所以我们分开来看。\n\n**1.新增数据**\n\n如果是新增数据，数据会直接写到数据库中，不用对缓存做任何操作，此时，缓存中本身就没有新增数据，而数据库中是最新值，这种情况符合我们刚刚所说的一致性的第2种情况，所以，此时，缓存和数据库的数据是一致的。\n\n**2.删改数据**\n\n如果发生删改操作，应用既要更新数据库，也要在缓存中删除数据。这两个操作如果无法保证原子性，也就是说，要不都完成，要不都没完成，此时，就会出现数据不一致问题了。这个问题比较复杂，我们来分析一下。\n\n1. 我们假设应用先删除缓存，再更新数据库，如果缓存删除成功，但是数据库更新失败，那么，应用再访问数据时，缓存中没有数据，就会发生缓存缺失。然后，应用再访问数据库，但是数据库中的值为旧值，应用就访问到旧值了。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h52o077lofj22bc16cnd8.jpg)\n\n2. 如果应用先完成了数据库的更新，但是，在删除缓存时失败了，那么，数据库中的值是新值，而缓存中的是旧值，这肯定是不一致的。这个时候，如果有其他的并发请求来访问数据，按照正常的缓存访问流程，就会先在缓存中查询，但此时，就会读到旧值了。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h52o28o66nj22bc16y7kq.jpg)\n\n总结一下\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h52rwz1mruj223j0ot1kx.jpg)\n\n### 如何解决数据不一致问题？\n\n**重试机制**\n\n具体来说，可以把要删除的缓存值或者是要更新的数据库值暂存到消息队列中（例如使用Kafka消息队列）。当应用没有能够成功地删除缓存值或者是更新数据库值时，可以从消息队列中重新读取这些值，然后再次进行删除或更新。\n\n如果能够成功地删除或更新，我们就要把这些值从消息队列中去除，以免重复操作，此时，我们也可以保证数据库和缓存的数据一致了。否则的话，我们还需要再次进行重试。如果重试超过的一定次数，还是没有成功，我们就需要向业务层发送报错信息了。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h52rzkh4pbj22bc1f4x0b.jpg)\n\n刚刚说的是在更新数据库和删除缓存值的过程中，其中一个操作失败的情况，实际上，即使这两个操作第一次执行时都没有失败，当有**大量并发请求**时，应用还是有可能读到不一致的数据。\n\n同样，我们按照不同的删除和更新顺序，分成两种情况来看。在这两种情况下，我们的解决方法也有所不同。\n\n**情况一：先删除缓存，再更新数据库。**\n\n假设线程A删除缓存值后，还没有来得及更新数据库（比如说有网络延迟），线程B就开始读取数据了，那么这个时候，线程B会发现缓存缺失，就只能去数据库读取。这会带来两个问题：\n\n1. 线程B读取到了旧值；\n2. 线程B是在缓存缺失的情况下读取的数据库，所以，它还会把旧值写入缓存，这可能会导致其他线程从缓存中读到旧值。\n\n等到线程B从数据库读取完数据、更新了缓存后，线程A才开始更新数据库，此时，缓存中的数据是旧值，而数据库中的是最新值，两者就不一致了。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h52tch06o9j229l11g7wh.jpg\" alt=\"image\" style=\"zoom:30%;\" />\n\n这该怎么办呢？我来给你提供一种解决方案。\n\n**在线程A更新完数据库值以后，我们可以让它先sleep一小段时间，再进行一次缓存删除操作。**\n\n之所以要加上sleep的这段时间，就是为了让线程B能够先从数据库读取数据，再把缺失的数据写入缓存，然后，线程A再进行删除。所以，线程A sleep的时间，就需要大于线程B读取数据再写入缓存的时间。这个时间怎么确定呢？建议你在业务程序运行的时候，统计下线程读数据和写缓存的操作时间，以此为基础来进行估算。\n\n这样一来，其它线程读取数据时，会发现缓存缺失，所以会从数据库中读取最新值。因为这个方案会在第一次删除缓存值后，延迟一段时间再次进行删除，所以我们也把它叫做“延迟双删”。\n\n```java\nredis.delKey(X)\ndb.update(X)\nThread.sleep(N)\nredis.delKey(X)\n```\n\n**情况二：先更新数据库值，再删除缓存值。**\n\n如果线程A删除了数据库中的值，但还没来得及删除缓存值，线程B就开始读取数据了，那么此时，线程B查询缓存时，发现缓存命中，就会直接从缓存中读取旧值。不过，在这种情况下，如果其他线程并发读缓存的请求不多，那么，就不会有很多请求读取到旧值。而且，线程A一般也会很快删除缓存值，这样一来，其他线程再次读取时，就会发生缓存缺失，进而从数据库中读取最新值。所以，这种情况对业务的影响较小。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h52wl0gvsqj228r0qmh7f.jpg)\n\n\n\n好了，到这里，我们了解到了，缓存和数据库的数据不一致一般是由两个原因导致的，我给你提供了相应的解决方案。\n\n- 删除缓存值或更新数据库失败而导致数据不一致，你可以使用重试机制确保删除或更新操作成功。\n- 在删除缓存值、更新数据库的这两步操作中，有其他线程的并发读操作，导致其他线程读取到旧值，应对方案是延迟双删。\n\n### 小结\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h52wn061t9j228911nqv5.jpg)\n\n在大多数业务场景下，我们会把Redis作为只读缓存使用。针对只读缓存来说，我们既可以先删除缓存值再更新数据库，也可以先更新数据库再删除缓存。我的建议是，优先使用先更新数据库再删除缓存的方法，原因主要有两个：\n\n1. 先删除缓存值再更新数据库，有可能导致请求因缓存缺失而访问数据库，给数据库带来压力；\n2. 如果业务应用中读取数据库和写缓存的时间不好估算，那么，延迟双删中的等待时间就不好设置。\n\n不过，当使用先更新数据库再删除缓存时，也有个地方需要注意，如果业务层要求必须读取一致的数据，那么，我们就需要在更新数据库时，先在Redis缓存客户端暂存并发读请求，等数据库更新完、缓存值删除后，再读取数据，从而保证数据一致性。\n\n\n\n**摘选自:极客时间-Redis核心技术与实战**","tags":["redis"]},{"title":"Redis-15-缓存污染","url":"/2022/10/27/cyb-mds/module/Redis/Redis-15-缓存污染/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n### 缓存污染介绍\n\n那什么是缓存污染呢？在一些场景下，有些数据被访问的次数非常少，甚至只会被访问一次。当这些数据服务完访问请求后，如果还继续留存在缓存中的话，就只会白白占用缓存空间。这种情况，就是缓存污染。\n\n当缓存污染不严重时，只有少量数据占据缓存空间，此时，对缓存系统的影响不大。但是，缓存污染一旦变得严重后，就会有大量不再访问的数据滞留在缓存中。如果这时数据占满了缓存空间，我们再往缓存中写入新数据时，就需要先把这些数据逐步淘汰出缓存，这就会引入额外的操作时间开销，进而会影响应用的性能。\n\n### 解决方案\n\n#### LRU缓存策略\n\nLRU策略的核心思想：如果一个数据刚刚被访问，那么这个数据肯定是热数据，还会被再次访问。\n\n按照这个核心思想，Redis中的LRU策略，会在每个数据对应的RedisObject结构体中设置一个lru字段，用来记录数据的访问时间戳。在进行数据淘汰时，LRU策略会在候选数据集中淘汰掉lru字段值最小的数据（也就是访问时间最久的数据）。\n\n所以，在数据被频繁访问的业务场景中，LRU策略的确能有效留存访问时间最近的数据。而且，因为留存的这些数据还会被再次访问，所以又可以提升业务应用的访问速度。\n\n但是，也正是**因为只看数据的访问时间，使用LRU策略在处理扫描式单次查询操作时，无法解决缓存污染**。所谓的扫描式单次查询操作，就是指应用对大量的数据进行一次全体读取，每个数据都会被读取，而且只会被读取一次。此时，因为这些被查询的数据刚刚被访问过，所以lru字段值都很大。\n\n在使用LRU策略淘汰数据时，这些数据会留存在缓存中很长一段时间，造成缓存污染。如果查询的数据量很大，这些数据占满了缓存空间，却又不会服务新的缓存请求，此时，再有新数据要写入缓存的话，还是需要先把这些旧数据替换出缓存才行，这会影响缓存的性能。\n\n如下图所示，数据6被访问后，被写入Redis缓存。但是，在此之后，数据6一直没有被再次访问，这就导致数据6滞留在缓存中，造成了污染。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h57a27bm1ij21p112zwpm.jpg)\n\n所以，对于采用了LRU策略的Redis缓存来说，扫描式单次查询会造成缓存污染。为了应对这类缓存污染问题，Redis从4.0版本开始增加了LFU淘汰策略。\n\n#### LFU缓存策略\n\n与LRU策略相比，LFU策略中会从两个维度来筛选并淘汰数据：一是，数据访问的时效性（访问时间离当前时间的远近）；二是，数据的被访问次数。\n\nLFU缓存策略是在LRU策略基础上，为每个数据增加了一个计数器，来统计这个数据的访问次数。当使用LFU策略筛选淘汰数据时，首先会根据数据的访问次数进行筛选，把访问次数最低的数据淘汰出缓存。如果两个数据的访问次数相同，LFU策略再比较这两个数据的访问时效性，把距离上一次访问时间更久的数据淘汰出缓存。\n\n和那些被频繁访问的数据相比，扫描式单次查询的数据因为不会被再次访问，所以它们的访问次数不会再增加。因此，LFU策略会优先把这些访问次数低的数据淘汰出缓存。这样一来，LFU策略就可以避免这些数据对缓存造成污染了。\n\n那么，LFU策略具体又是如何实现的呢？既然LFU策略是在LRU策略上做的优化，那它们的实现必定有些关系。为了避免操作链表的开销，Redis在实现LRU策略时使用了两个近似方法：\n\n- Redis是用RedisObject结构来保存数据的，RedisObject结构中设置了一个lru字段，用来记录数据的访问时间戳；\n- Redis并没有为所有的数据维护一个全局的链表，而是通过随机采样方式，选取一定数量（例如10个）的数据放入候选集合，后续在候选集合中根据lru字段值的大小进行筛选。\n\n在此基础上，**Redis在实现LFU策略的时候，只是把原来24bit大小的lru字段，又进一步拆分成了两部分**。\n\n1. ldt值：lru字段的前16bit，表示数据的访问时间戳；\n2. counter值：lru字段的后8bit，表示数据的访问次数。\n\n总结一下：当LFU策略筛选数据时，Redis会在候选集合中，根据数据lru字段的后8bit选择访问次数最少的数据进行淘汰。当访问次数相同时，再根据lru字段的前16bit值大小，选择访问时间最久远的数据进行淘汰。\n\n到这里，还没结束，**Redis只使用了8bit记录数据的访问次数，而8bit记录的最大值是255**，这样可以吗？\n\n在实际应用中，一个数据可能会被访问成千上万次。如果每被访问一次，counter值就加1的话，那么，只要访问次数超过了255，数据的counter值就一样了。在进行数据淘汰时，LFU策略就无法很好地区分并筛选这些数据，反而还可能会把不怎么访问的数据留存在了缓存中。\n\n因此，**在实现LFU策略时，Redis并没有采用数据每被访问一次，就给对应的counter值加1的计数规则，而是采用了一个更优化的计数规则**。\n\n简单来说，LFU策略实现的计数规则是：每当数据被访问一次时，首先，用计数器当前的值乘以配置项lfu_log_factor再加1，再取其倒数，得到一个p值；然后，把这个p值和一个取值范围在（0，1）间的随机数r值比大小，只有p值大于r值时，计数器才加1。\n\n下面这段Redis的部分源码，显示了LFU策略增加计数器值的计算逻辑。其中，baseval是计数器当前的值。计数器的初始值默认是5（由代码中的LFU_INIT_VAL常量设置），而不是0，这样可以避免数据刚被写入缓存，就因为访问次数少而被立即淘汰。\n\n```\ndouble r = (double)rand()/RAND_MAX;\n...\ndouble p = 1.0/(baseval*server.lfu_log_factor+1);\nif (r < p) counter++;   \n```\n\n使用了这种计算规则后，我们可以通过设置不同的lfu_log_factor配置项，来控制计数器值增加的速度，避免counter值很快就到255了。\n\n为了更进一步说明LFU策略计数器递增的效果，你可以看下下面这张表。这是Redis[官网](https://redis.io/topics/lru-cache)上提供的一张表，它记录了当lfu_log_factor取不同值时，在不同的实际访问次数情况下，计数器的值是如何变化的。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h57gfb6u83j21zg0hiame.jpg)\n\n应用负载的情况是很复杂的。在一些场景下，有些数据在短时间内被大量访问后就不会再被访问了。那么再按照访问次数来筛选的话，这些数据会被留存在缓存中，但不会提升缓存命中率。为此，Redis在实现LFU策略时，还设计了一个counter值的衰减机制。\n\n简单来说，LFU策略使用衰减因子配置项lfu_decay_time来控制访问次数的衰减。LFU策略会计算当前时间和数据最近一次访问时间的差值，并把这个差值换算成以分钟为单位。然后，LFU策略再把这个差值除以lfu_decay_time值，所得的结果就是数据counter要衰减的值。\n\n简单举个例子，假设lfu_decay_time取值为1，如果数据在N分钟内没有被访问，那么它的访问次数就要减N。如果lfu_decay_time取值更大，那么相应的衰减值会变小，衰减效果也会减弱。所以，如果业务应用中有短时高频访问的数据的话，建议把lfu_decay_time值设置为1，这样一来，LFU策略在它们不再被访问后，会较快地衰减它们的访问次数，尽早把它们从缓存中淘汰出去，避免缓存污染。\n\n### 总结\n\n缓存污染问题指的是留存在缓存中的数据，实际不会被再次访问了，但是又占据了缓存空间。如果这样的数据体量很大，甚至占满了缓存，每次有新数据写入缓存时，还需要把这些数据逐步淘汰出缓存，就会增加缓存操作的时间开销。\n\n因此，要解决缓存污染问题，最关键的技术点就是能识别出这些只访问一次或是访问次数很少的数据，在淘汰数据时，优先把它们筛选出来并淘汰掉。因为noviction策略不涉及数据淘汰，所以这节课，我们就从能否有效解决缓存污染这个维度，分析了Redis的其他7种数据淘汰策略。\n\nvolatile-random和allkeys-random是随机选择数据进行淘汰，无法把不再访问的数据筛选出来，可能会造成缓存污染。如果业务层明确知道数据的访问时长，可以给数据设置合理的过期时间，再设置Redis缓存使用volatile-ttl策略。当缓存写满时，剩余存活时间最短的数据就会被淘汰出缓存，避免滞留在缓存中，造成污染。\n\n当我们使用LRU策略时，由于LRU策略只考虑数据的访问时效，对于只访问一次的数据来说，LRU策略无法很快将其筛选出来。而LFU策略在LRU策略基础上进行了优化，在筛选数据时，首先会筛选并淘汰访问次数少的数据，然后针对访问次数相同的数据，再筛选并淘汰访问时间最久远的数据。\n\n在实际业务应用中，LRU和LFU两个策略都有应用。LRU和LFU两个策略关注的数据访问特征各有侧重，LRU策略更加关注数据的时效性，而LFU策略更加关注数据的访问频次。通常情况下，实际应用的负载具有较好的时间局部性，所以LRU策略的应用会更加广泛。但是，在扫描式查询的应用场景中，LFU策略就可以很好地应对缓存污染问题了，建议你优先使用。\n\n此外，如果业务应用中有短时高频访问的数据，除了LFU策略本身会对数据的访问次数进行自动衰减以外，我再给你个小建议：你可以优先使用volatile-lfu策略，并根据这些数据的访问时限设置它们的过期时间，以免它们留存在缓存中造成污染。\n\n**摘选自:极客时间-Redis核心技术与实战**","tags":["redis"]},{"title":"Redis-14-缓存雪崩、击穿、穿透","url":"/2022/10/27/cyb-mds/module/Redis/Redis-14-缓存雪崩、击穿、穿透/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n### 缓存雪崩\n\n缓存雪崩是指大量的应用请求无法在Redis缓存中进行处理，紧接着，应用将大量请求发送到数据库层，导致数据库层的压力激增。\n\n缓存雪崩一般是由两个原因导致的，应对方案也有所不同，我们一个个来看。\n\n**第一个原因是：缓存中有大量数据同时过期，导致大量请求无法得到处理。**\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h52xheojgwj22bc1qik73.jpg\" alt=\"image\" style=\"zoom: 33%;\" />\n\n针对大量数据同时失效带来的缓存雪崩问题，我给你提供两种解决方案。\n\n1. 微调过期时间：\n\n   我们可以避免给大量的数据设置相同的过期时间。如果业务层的确要求有些数据同时失效，你可以在用EXPIRE命令给每个数据设置过期时间时，给这些数据的过期时间增加一个较小的随机数（例如，随机增加1~3分钟），这样一来，不同数据的过期时间有所差别，但差别又不会太大，既避免了大量数据同时过期，同时也保证了这些数据基本在相近的时间失效，仍然能满足业务需求。\n\n2. 服务降级：\n\n   所谓的服务降级，是指发生缓存雪崩时，针对不同的数据采取不同的处理方式。\n\n   - 当业务应用访问的是非核心数据（例如电商商品属性）时，暂时停止从缓存中查询这些数据，而是直接返回预定义信息、空值或是错误信息；\n   - 当业务应用访问的是核心数据（例如电商商品库存）时，仍然允许查询缓存，如果缓存缺失，也可以继续通过数据库读取。\n\n   这样一来，只有部分过期数据的请求会发送到数据库，数据库的压力就没有那么大了。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h52xkrp67sj21h21atk0l.jpg\" alt=\"image\" style=\"zoom:25%;\" />\n\n\n\n**第二个原因是：Redis缓存实例发生故障宕机了，无法处理请求，这就会导致大量请求一下子积压到数据库层，从而发生缓存雪崩。**\n\n一般来说，一个Redis实例可以支持数万级别的请求处理吞吐量，而单个数据库可能只能支持数千级别的请求处理吞吐量，它们两个的处理能力可能相差了近十倍。由于缓存雪崩，Redis缓存失效，所以，数据库就可能要承受近十倍的请求压力，从而因为压力过大而崩溃。\n\n因为Redis实例发生了宕机，我们需要通过其他方法来应对缓存雪崩了。我给你提供两个建议。\n\n1. 是在业务系统中实现**服务熔断**或**请求限流**机制。\n\n   * **服务熔断**，是指在发生缓存雪崩时，为了防止引发连锁的数据库雪崩，甚至是整个系统的崩溃，我们暂停业务应用对缓存系统的接口访问。再具体点说，就是业务应用调用缓存接口时，缓存客户端并不把请求发给Redis缓存实例，而是直接返回，等到Redis缓存实例重新恢复服务后，再允许应用请求发送到缓存系统。这样一来，我们就避免了大量请求因缓存缺失，而积压到数据库系统，保证了数据库系统的正常运行。服务熔断虽然可以保证数据库的正常运行，但是暂停了整个缓存系统的访问，对业务应用的影响范围大。为了尽可能减少这种影响，我们也可以进行请求限流。\n\n   <img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h52xvkskv2j21av1b0gti.jpg\" alt=\"image\" style=\"zoom:25%;\" />\n\n   * **请求限流**，就是指，我们在业务系统的请求入口前端控制每秒进入系统的请求数，避免过多的请求被发送到数据库。假设业务系统正常运行时，请求入口前端允许每秒进入系统的请求是1万个，其中，9000个请求都能在缓存系统中进行处理，只有1000个请求会被应用发送到数据库进行处理。一旦发生了缓存雪崩，数据库的每秒请求数突然增加到每秒1万个，此时，我们就可以启动请求限流机制，在请求入口前端只允许每秒进入系统的请求数为1000个，再多的请求就会在入口前端被直接拒绝服务。所以，使用了请求限流，就可以避免大量并发请求压力传递到数据库层。\n\n   <img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h52xyocd09j22bc1qiqi0.jpg\" alt=\"image\" style=\"zoom:22%;\" />\n\n   \n\n2. 事前预防\n\n   通过主从节点的方式构建Redis缓存高可靠集群。如果Redis缓存的主节点故障宕机了，从节点还可以切换成为主节点，继续提供缓存服务，避免了由于缓存实例宕机而导致的缓存雪崩问题。\n\n### 缓存击穿\n\n缓存击穿是指，针对某个访问非常频繁的热点数据的请求，无法在缓存中进行处理，紧接着，访问该数据的大量请求，一下子都发送到了后端数据库，导致了数据库压力激增，会影响数据库处理其他请求。缓存击穿的情况，经常发生在热点数据过期失效时。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h52z06pfkuj22bc1qih0w.jpg)\n\n为了避免缓存击穿给数据库带来的激增压力，我们的解决方法也比较直接，对于访问特别频繁的热点数据，我们就不设置过期时间了。这样一来，对热点数据的访问请求，都可以在缓存中进行处理，而Redis数万级别的高吞吐量可以很好地应对大量的并发请求访问。\n\n### 缓存穿透\n\n缓存穿透是指要访问的数据既不在Redis缓存中，也不在数据库中，导致请求在访问缓存时，发生缓存缺失，再去访问数据库时，发现数据库中也没有要访问的数据。此时，应用也无法从数据库中读取数据再写入缓存，来服务后续请求，这样一来，缓存也就成了“摆设”，如果应用持续有大量请求访问数据，就会同时给缓存和数据库带来巨大压力\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h53s3ac2tvj22bc1qidvr.jpg\" alt=\"image\" style=\"zoom:29%;\" />\n\n那么，缓存穿透会发生在什么时候呢？一般来说，有两种情况。\n\n- 业务层误操作：缓存中的数据和数据库中的数据被误删除了，所以缓存和数据库中都没有数据；\n- 恶意攻击：专门访问数据库中没有的数据。\n\n为了避免缓存穿透的影响，我来给你提供三种应对方案。\n\n1. **缓存空值或缺省值。**\n\n   一旦发生缓存穿透，我们就可以针对查询的数据，在Redis中缓存一个空值或是和业务层协商确定的缺省值（例如，库存的缺省值可以设为0）。紧接着，应用发送的后续请求再进行查询时，就可以直接从Redis中读取空值或缺省值，返回给业务应用了，避免了把大量请求发送给数据库处理，保持了数据库的正常运行。\n\n2. **前端进行请求检测**\n\n   缓存穿透的一个原因是有大量的恶意请求访问不存在的数据，所以，一个有效的应对方案是在请求入口前端，对业务系统接收到的请求进行合法性检测，把恶意的请求（例如请求参数不合理、请求参数是非法值、请求字段不存在）直接过滤掉，不让它们访问后端缓存和数据库。这样一来，也就不会出现缓存穿透问题了。\n\n3. **使用布隆过滤器快速判断数据是否存在，避免从数据库中查询数据是否存在，减轻数据库压力。**\n\n   我们先来看下，布隆过滤器是如何工作的。\n\n   布隆过滤器由一个初值都为0的bit数组和N个哈希函数组成，可以用来快速判断某个数据是否存在。当我们想标记某个数据存在时（例如，数据已被写入数据库），布隆过滤器会通过三个操作完成标记：\n\n   - 首先，使用N个哈希函数，分别计算这个数据的哈希值，得到N个哈希值。\n   - 然后，我们把这N个哈希值对bit数组的长度取模，得到每个哈希值在数组中的对应位置。\n   - 最后，我们把对应位置的bit位设置为1，这就完成了在布隆过滤器中标记数据的操作。\n\n   如果数据不存在（例如，数据库里没有写入数据），我们也就没有用布隆过滤器标记过数据，那么，bit数组对应bit位的值仍然为0。\n   \n   当需要查询某个数据时，我们就执行刚刚说的计算过程，先得到这个数据在bit数组中对应的N个位置。紧接着，我们查看bit数组中这N个位置上的bit值。只要这N个bit值有一个不为1，这就表明布隆过滤器没有对该数据做过标记，所以，查询的数据一定没有在数据库中保存。\n   \n   ![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h579n4ti18j22a111aak2.jpg)\n   \n   图中布隆过滤器是一个包含10个bit位的数组，使用了3个哈希函数，当在布隆过滤器中标记数据X时，X会被计算3次哈希值，并对10取模，取模结果分别是1、3、7。所以，bit数组的第1、3、7位被设置为1。当应用想要查询X时，只要查看数组的第1、3、7位是否为1，只要有一个为0，那么，X就肯定不在数据库中。\n   \n   正是基于布隆过滤器的快速检测特性，我们可以在把数据写入数据库时，使用布隆过滤器做个标记。当缓存缺失后，应用查询数据库时，可以通过查询布隆过滤器快速判断数据是否存在。如果不存在，就不用再去数据库中查询了。这样一来，即使发生缓存穿透了，大量请求只会查询Redis和布隆过滤器，而不会积压到数据库，也就不会影响数据库的正常运行。布隆过滤器可以使用Redis实现，本身就能承担较大的并发访问压力。\n\n### 总结\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h579pdvwnpj226z0yj7wh.jpg)\n\n**摘选自:极客时间-Redis核心技术与实战**\n\n","tags":["redis"]},{"title":"Redis-12-LRU&LFU","url":"/2022/10/27/cyb-mds/module/Redis/Redis-12-LRU&LFU/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n### LRU\n\nvolatile-lru和allkeys-lru策略都用到了LRU算法。LRU算法的全称是Least Recently Used，从名字上就可以看出，这是按照最近最少使用的原则来筛选数据，最不常用的数据会被筛选出来，而最近频繁使用的数据会留在缓存中。\n\n那具体是怎么筛选的呢？LRU会把所有的数据组织成一个链表，链表的头和尾分别表示MRU端和LRU端，分别代表最近最常使用的数据和最近最不常用的数据 。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4zggdeqxxj21ba1amdpu.jpg\" alt=\"image\" style=\"zoom: 33%;\" />\n\n如果有一个新数据15要被写入缓存，但此时已经没有缓存空间了，也就是链表没有空余位置了，那么，LRU算法做两件事：\n\n1. 数据15是刚被访问的，所以它会被放到MRU端；\n2. 算法把LRU端的数据5从缓存中删除，相应的链表中就没有数据5的记录了。\n\n其实，LRU算法背后的想法非常朴素：它认为刚刚被访问的数据，肯定还会被再次访问，所以就把它放在MRU端；长久不访问的数据，肯定就不会再被访问了，所以就让它逐渐后移到LRU端，在缓存满时，就优先删除它。\n\n不过，LRU算法在实际实现时，需要用链表管理所有的缓存数据，这会**带来额外的空间开销**。而且，当有数据被访问时，需要在链表上把该数据移动到MRU端，如果有大量数据被访问，就会带来很多链表移动操作，会很耗时，进而会降低Redis缓存性能。\n\n所以，在Redis中，LRU算法被做了简化，以减轻数据淘汰对缓存性能的影响。具体来说，Redis默认会记录每个数据的最近一次访问的时间戳（由键值对数据结构RedisObject中的lru字段记录）。然后，Redis在决定淘汰的数据时，第一次会随机选出N个数据，把它们作为一个候选集合。接下来，Redis会比较这N个数据的lru字段，把lru字段值最小的数据从缓存中淘汰出去。\n\nRedis提供了一个配置参数maxmemory-samples，这个参数就是Redis选出的数据个数N。例如，我们执行如下命令，可以让Redis选出100个数据作为候选数据集：\n\n```shell\nCONFIG SET maxmemory-samples 100\n```\n\n当需要再次淘汰数据时，Redis需要挑选数据进入第一次淘汰时创建的候选集合。这儿的挑选标准是：**能进入候选集合的数据的lru字段值必须小于候选集合中最小的lru值**。当有新数据进入候选数据集后，如果候选数据集中的数据个数达到了maxmemory-samples，Redis就把候选数据集中lru字段值最小的数据淘汰出去。\n\n\n\n### LFU\n\nLFU缓存策略是在LRU策略基础上，为每个数据增加了一个计数器，来统计这个数据的访问次数。当使用LFU策略筛选淘汰数据时，首先会根据数据的访问次数进行筛选，把访问次数最低的数据淘汰出缓存。如果两个数据的访问次数相同，LFU策略再比较这两个数据的访问时效性，把距离上一次访问时间更久的数据淘汰出缓存。\n\n重点放在缓存污染里介绍\n\n\n\n**摘选自:极客时间-Redis核心技术与实战**","tags":["redis"]},{"title":"Redis-10-旁路缓存Redis如何工作","url":"/2022/10/27/cyb-mds/module/Redis/Redis-10-旁路缓存Redis如何工作/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n### 缓存的特征\n\n一个系统中的不同层之间的访问速度不一样，所以我们才需要缓存，这样就可以把一些需要频繁访问的数据放在缓存中，以加快它们的访问速度。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4z5261umej21n40zkjw8.jpg\" alt=\"image\" style=\"zoom:25%;\" />\n\n计算机系统中，默认有两种缓存：\n\n- CPU里面的末级缓存，即LLC，用来缓存内存中的数据，避免每次从内存中存取数据；\n- 内存中的高速页缓存，即page cache，用来缓存磁盘中的数据，避免每次从磁盘中存取数据。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4z52kd6luj21nw0yf443.jpg\" alt=\"image\" style=\"zoom:25%;\" />\n\n**第一个特征**：在一个层次化的系统中，缓存一定是一个快速子系统，数据存在缓存中时，能避免每次从慢速子系统中存取数据。对应到互联网应用来说，Redis就是快速子系统，而数据库就是慢速子系统了。\n\n**第二个特征：缓存系统的容量大小总是小于后端慢速系统的，我们不可能把所有数据都放在缓存系统中**。\n\n\n\n### Redis缓存处理请求的两种情况\n\n把Redis用作缓存时，我们会把Redis部署在数据库的前端，业务应用在访问数据时，会先查询Redis中是否保存了相应的数据。此时，根据数据是否存在缓存中，会有两种情况。\n\n- **缓存命中**：Redis中有相应数据，就直接读取Redis，性能非常快。\n- **缓存缺失**：Redis中没有保存相应数据，就从后端数据库中读取数据，性能就会变慢。而且，一旦发生缓存缺失，为了让后续请求能从缓存中读取到数据，我们需要把缺失的数据写入Redis，这个过程叫作缓存更新。缓存更新操作会涉及到保证缓存和数据库之间的数据一致性问题。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4z5411745j224e19btki.jpg\" alt=\"image\" style=\"zoom:25%;\" />\n\n到这里，你可能已经发现了，使用Redis缓存时，我们基本有三个操作：\n\n- 应用读取数据时，需要先读取Redis；\n- 发生缓存缺失时，需要从数据库读取数据；\n- 发生缓存缺失时，还需要更新缓存。\n\n\n\n### Redis作为旁路缓存的使用操作\n\n使用Redis缓存时，具体来说，我们需要在应用程序中增加三方面的代码：\n\n- 当应用程序需要读取数据时，我们需要在代码中显式调用Redis的GET操作接口，进行查询；\n- 如果缓存缺失了，应用程序需要再和数据库连接，从数据库中读取数据；\n- 当缓存中的数据需要更新时，我们也需要在应用程序中显式地调用SET操作接口，把更新的数据写入缓存。\n\n```java\nString cacheKey = “productid_11010003”;\nString cacheValue = redisCache.get(cacheKey)；\n//缓存命中\nif ( cacheValue != NULL)\n   return cacheValue;\n//缓存缺失\nelse\n   cacheValue = getProductFromDB();\n   redisCache.put(cacheValue)  //缓存更新\n```\n\n\n\n### 缓存的类型\n\n#### 只读缓存\n\n当Redis用作只读缓存时，应用要读取数据的话，会先调用Redis GET接口，查询数据是否存在。而所有的数据写请求，会直接发往后端的数据库，在数据库中增删改。对于删改的数据来说，如果Redis已经缓存了相应的数据，应用需要把这些缓存的数据删除，Redis中就没有这些数据了。\n\n当应用再次读取这些数据时，会发生缓存缺失，应用会把这些数据从数据库中读出来，并写到缓存中。这样一来，这些数据后续再被读取时，就可以直接从缓存中获取了，能起到加速访问的效果。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4z56tl1z4j22bc19itte.jpg\" alt=\"image\" style=\"zoom: 25%;\" />\n\n只读缓存直接在数据库中更新数据的好处是，所有最新的数据都在数据库中，而数据库是提供数据可靠性保障的，这些数据不会有丢失的风险。当我们需要缓存图片、短视频这些用户只读的数据时，就可以使用只读缓存这个类型了。\n\n#### 读写缓存\n\n对于读写缓存来说，除了读请求会发送到缓存进行处理（直接在缓存中查询数据是否存在)，所有的写请求也会发送到缓存，在缓存中直接对数据进行增删改操作。此时，得益于Redis的高性能访问特性，数据的增删改操作可以在缓存中快速完成，处理结果也会快速返回给业务应用，这就可以提升业务应用的响应速度。\n\n但是，和只读缓存不一样的是，在使用读写缓存时，最新的数据是在Redis中，而Redis是内存数据库，一旦出现掉电或宕机，内存中的数据就会丢失。这也就是说，应用的最新数据可能会丢失，给应用业务带来风险。\n\n所以，根据业务应用对数据可靠性和缓存性能的不同要求，我们会有同步直写和异步写回两种策略。其中，同步直写策略优先保证数据可靠性，而异步写回策略优先提供快速响应。学习了解这两种策略，可以帮助我们根据业务需求，做出正确的设计选择。\n\n同步直写是指，写请求发给缓存的同时，也会发给后端数据库进行处理，等到缓存和数据库都写完数据，才给客户端返回。这样，即使缓存宕机或发生故障，最新的数据仍然保存在数据库中，这就提供了数据可靠性保证。\n\n不过，同步直写会降低缓存的访问性能。这是因为缓存中处理写请求的速度是很快的，而数据库处理写请求的速度较慢。即使缓存很快地处理了写请求，也需要等待数据库处理完所有的写请求，才能给应用返回结果，这就增加了缓存的响应延迟。\n\n而异步写回策略，则是优先考虑了响应延迟。此时，所有写请求都先在缓存中处理。等到这些增改的数据要被从缓存中淘汰出来时，缓存将它们写回后端数据库。这样一来，处理这些数据的操作是在缓存中进行的，很快就能完成。只不过，如果发生了掉电，而它们还没有被写回数据库，就会有丢失的风险了。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4z57yodghj227y17i7hb.jpg\" alt=\"image\" style=\"zoom:25%;\" />\n\n关于是选择只读缓存，还是读写缓存，主要看我们对写请求是否有加速的需求。\n\n- 如果需要对写请求进行加速，我们选择读写缓存；\n- 如果写请求很少，或者是只需要提升读请求的响应速度的话，我们选择只读缓存。\n\n\n\n**摘选自:极客时间-Redis核心技术与实战**","tags":["redis"]},{"title":"Redis-11-淘汰策略","url":"/2022/10/27/cyb-mds/module/Redis/Redis-11-淘汰策略/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n### 设置多大的缓存容量合适？\n\n实际应用中的数据访问是具有局部性的。下面有一张图，图里有红、蓝两条线，显示了不同比例数据贡献的访问量情况。蓝线代表了“八二原理”表示的数据局部性，而红线则表示在当前应用负载下，数据局部性的变化。\n\n我们先看看蓝线。它表示的就是“八二原理”，有20%的数据贡献了80%的访问了，而剩余的数据虽然体量很大，但只贡献了20%的访问量。这80%的数据在访问量上就形成了一条长长的尾巴，我们也称为“长尾效应”。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4z5fkru01j221b1lttjd.jpg\" alt=\"image\" style=\"zoom:25%;\" />\n\n如果按照“八二原理”来设置缓存空间容量，也就是把缓存空间容量设置为总数据量的20%的话，就有可能拦截到80%的访问。\n\n但是容量规划不能一概而论，是需要结合**应用数据实际访问特征**和**成本开销**来综合考虑的。系统的设计选择是一个权衡的过程：大容量缓存是能带来性能加速的收益，但是成本也会更高，而小容量缓存不一定就起不到加速访问的效果。一般来说，**我会建议把缓存容量设置为总数据量的15%到30%，兼顾访问性能和内存空间开销**。\n\n对于Redis来说，一旦确定了缓存最大容量，比如4GB，你就可以使用下面这个命令来设定缓存的大小了：\n\n```shell\nCONFIG SET maxmemory 4gb\n```\n\n**缓存被写满是不可避免的**。即使你精挑细选，确定了缓存容量，还是要面对缓存写满时的替换操作。缓存替换需要解决两个问题：决定淘汰哪些数据，如何处理那些被淘汰的数据。\n\n\n\n### Redis缓存有哪些淘汰策略？\n\nRedis 4.0之前一共实现了6种内存淘汰策略，在4.0之后，又增加了2种策略。我们可以按照是否会进行数据淘汰把它们分成两类：\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4z5iohrnzj21ct0l9dl8.jpg\" alt=\"image\" style=\"zoom:50%;\" />\n\n- volatile-ttl在筛选时，会针对设置了过期时间的键值对，根据过期时间的先后进行删除，越早过期的越先被删除。\n- volatile-random就像它的名称一样，在设置了过期时间的键值对中，进行随机删除。\n- volatile-lru会使用LRU算法筛选设置了过期时间的键值对。\n- volatile-lfu会使用LFU算法选择设置了过期时间的键值对。\n- allkeys-random策略，从所有键值对中随机选择并删除数据；\n- allkeys-lru策略，使用LRU算法在所有数据中进行筛选。\n- allkeys-lfu策略，使用LFU算法在所有数据中进行筛选。\n\n三个使用建议：\n\n- **优先使用allkeys-lru策略**。这样，可以充分利用LRU这一经典缓存算法的优势，把最近最常访问的数据留在缓存中，提升应用的访问性能。如果你的业务数据中有明显的冷热数据区分，我建议你使用allkeys-lru策略。\n- 如果业务应用中的数据访问频率相差不大，没有明显的冷热数据区分，建议使用allkeys-random策略，随机选择淘汰的数据就行。\n- **如果你的业务中有置顶的需求**，比如置顶新闻、置顶视频，那么，可以使用volatile-lru策略，同时不给这些置顶数据设置过期时间。这样一来，这些需要置顶的数据一直不会被删除，而其他数据会在过期时根据LRU规则进行筛选。\n\n一旦被淘汰的数据被选定后，Redis怎么处理这些数据呢？这就要说到缓存替换时的具体操作了。\n\n### 如何处理被淘汰的数据？\n\n一般来说，一旦被淘汰的数据选定后，如果这个数据是干净数据，那么我们就直接删除；如果这个数据是脏数据，我们需要把它写回数据库，如下图所示：\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4zgjefdahj21k80w2jx8.jpg\" alt=\"image\" style=\"zoom: 33%;\" />\n\n那怎么判断一个数据到底是干净的还是脏的呢？\n\n干净数据和脏数据的区别就在于，和最初从后端数据库里读取时的值相比，有没有被修改过。干净数据一直没有被修改，所以后端数据库里的数据也是最新值。在替换时，它可以被直接删除。\n\n对于Redis来说，它决定了被淘汰的数据后，会把它们删除。即使淘汰的数据是脏数据，Redis也不会把它们写回数据库。所以，我们在使用Redis缓存时，如果数据被修改了，需要在数据修改时就将它写回数据库。否则，这个脏数据被淘汰时，会被Redis删除，而数据库里也没有最新的数据了。\n\n\n\n\n\n**摘选自:极客时间-Redis核心技术与实战**\n\n","tags":["redis"]},{"title":"Redis-1-数据结构","url":"/2022/10/27/cyb-mds/module/Redis/Redis-1-数据结构/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n\n\n#### 问题画像\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4fjbh84hnj21kw16o7rg.jpg\" alt=\"image\" style=\"zoom:30%;\" />\n\n\n\n#### 数据结构\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4fn9zxb5xj23340x0dys.jpg\" alt=\"image\" style=\"zoom:20%;\" />\n\n\n\n#### 键和值的结构组织\n\n为了实现从键到值的快速访问，Redis 使用了一个哈希表来保存所有键值对。一个哈希表，其实就是一个数组，数组的每个元素称为一个哈希桶。所以，我们常说，一个哈希表是由多个哈希桶组成的，每个哈希桶中保存了键值对数据。哈希桶中的元素保存的并不是值本身，而是指向具体值的指针。哈希表的最大好\n处就是让我们可以用 O(1) 的时间复杂度来快速查找到键值对——我们只需要计算键的哈希值，就可以知道它所对应的哈希桶位置，然后就可以访问相应的 entry 元素。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4fole9e60j21d90obwlf.jpg\" alt=\"image\" style=\"zoom: 40%;\" />\n\n\n\n#### 哈希冲突\n\nRedis 解决哈希冲突的方式，和HashMap一样采用链式哈希。链式哈希，就是指同一个哈希桶中的多个元素用一个链表来保存，它们之间依次用指针连接。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4fp55lvqhj20y90udtgx.jpg\" alt=\"image\" style=\"zoom:50%;\" />\n\n但是，这里依然存在一个问题，哈希冲突链上的元素只能通过指针逐一查找再操作。如果哈希表里写入的数据越来越多，哈希冲突可能也会越来越多，这就会导致某些哈希冲突链过长，进而导致这个链上的元素查找耗时长，效率降低。对于追求“快”的 Redis 来说，这是不太能接受的。\n\n所以，Redis 会对哈希表做 rehash 操作。rehash 也就是增加现有的哈希桶数量，让逐渐增多的 entry 元素能在更多的桶之间分散保存，减少单个桶中的元素数量，从而减少单个桶中的冲突。\n\n为了使 rehash 操作更高效，Redis 默认使用了两个全局哈希表：哈希表 1 和哈希表 2。一开始，当你刚插入数据时，默认使用哈希表 1，此时的哈希表 2 并没有被分配空间。随着数据逐步增多，Redis 开始执行 rehash，这个过程分为三步：\n\n1. 给哈希表 2 分配更大的空间，例如是当前哈希表 1 大小的两倍；\n2. 把哈希表 1 中的数据重新映射并拷贝到哈希表 2 中；\n3. 释放哈希表 1 的空间。\n\n到此，我们就可以从哈希表 1 切换到哈希表 2，用增大的哈希表 2 保存更多数据，而原来的哈希表 1 留作下一次 rehash 扩容备用。这个过程看似简单，但是第二步涉及大量的数据拷贝，如果一次性把哈希表 1 中的数据都迁移完，会造成 Redis 线程阻塞，无法服务其他请求。此时，Redis 就无法快速访问数据了。\n\n为了避免这个问题，Redis 采用了**渐进式 rehash**。\n\n简单来说就是在第二步拷贝数据时，Redis 仍然正常处理客户端请求，每处理一个请求时，从哈希表 1 中的第一个索引位置开始，顺带着将这个索引位置上的所有entries 拷贝到哈希表 2 中；等处理下一个请求时，再顺带拷贝哈希表 1 中的下一个索引位置的entries。如下图所示\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4fq1b39y6j21d60tztjx.jpg\" alt=\"image\" style=\"zoom:50%;\" />\n\n\n\n\n\n**摘选自:极客时间-Redis核心技术与实战**","tags":["redis"]},{"title":"tomcat远程调试","url":"/2022/10/27/cyb-mds/java/tomcat/tomcat远程调试/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n \n\n**1、Linux中配置tomcat在catalina.sh中添加如下**\n`CATALINA_OPTS=\"-server-Xdebug -Xnoagent -Djava.compiler=NONE-Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=5888\"`\n\n**2、Window中修改 catalina,bat文件，添加：**\n\n`Set  “CATALINA_OPTS=-Xdebug -Xrunjdwp:transport=dt_socket,address=8000,server=y,suspend=n\"`\n\n如图：\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylf2sun31j20rw041dfz.jpg)\n\n远程调试参数说明：\n\n````powershell\n-Xdebug                             ： 启用调试模式\n\n-Xrunjdwp<sub-options>  : 加载JVM的JPDA参考实现库\n\ntransport=dt_socket          ：Socket连接，可选dt_shmem 通过共享内存的方式连接到调试服务器\n\naddress=8000                    ：调试服务器监听的端口\n\nserver=y                            ： 是否是服务器端，n为客户端\n\nsuspend=n                        ： 启动过程是否加载暂停，y为启动时暂停，方便调试启动过程\n````\n**3****、启动****tomcat****，看看****tomcat****是否启动成功，**\n\n如果启动成功，tomcat日志文件(catalina.out)中会有如下输出：\n\nListening for transport dt_socket at address: 8000\n\n**4****、使用****eclipse****调试：**\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylf3enpepj207y046glm.jpg)\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylf3vc0yxj20m80gq0u7.jpg)\n\n点击debug，就可进行调试了\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylf4e53hvj20hw05bgm1.jpg)\n\n剩下的就和普通调试一样了。断点直接在源代码中添加就行\n\n**5****、可能出现的连接问题：**\n\nFailed to connect to remote VM. Connection refused.\n\nConnection refused: connect。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylf5fu1jzj20ai05iaal.jpg)\n\n出现如图所示的情况可能是已经建立了一个连接了。\n\n解决方法：去debug透视图中![mark]![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylf62bbftj207p00mq2q.jpg)\n\n找到点击![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylf6xt1ulj200k00k08l.jpg)断开链接，就可以解决问题了。\n\n ","tags":["java","Tomcat"]},{"title":"JVM性能优化， Part 5  ―― Java的伸缩性","url":"/2022/10/27/cyb-mds/java/jvm/JVM性能优化， Part 5  ―― Java的伸缩性/","content":"\n==作者：Eva Andreasson,译者：吴杰==\n\n[toc]\n\n很多程序员在解决JVM性能问题的时候，花开了很多时间去调优应用程序级别的性能瓶颈，当你读完这本系列文章之后你会发现我可能更加系统地看待这类的问题。我说过JVM的自身技术限制了Java企业级应用的伸缩性。首先我们先列举一些主导因素。\n\n - 主流的硬件服务器提供了大量的内存\n\n - 分布式系统有大量内存的需求，而且该需求在持续增长\n\n - 一个普通Java应用程序所持有的对空间大概在1GB~4GB，这远远低于一个硬件服务器的内存管理能力以及一个分布式应用程序的内存需求量。这被称之为Java内存墙，如下图所示(图中表述Java应用服务器和常规Java应用的内存使用量的演变史)。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylf0zja0ij20s70ftq3c.jpg)\n\nJava内存墙(1980~2010)   (图片来源：Azul Systems)\n\n这给我们带来了如下JVM性能课题：\n\n1) 如果分配给应用程序的内存太小，将导致内存不足。JVM 不能及时释放内存空间给应用程序，最终将引发内存不足，或者JVM完全关闭。所以你必须提供更多的内存给应用程序。\n\n2) 如果给对响应时间敏感的应用增加内存，如果不重启你的系统或者优化你的应用，Java堆最终会碎片化。当碎片发生时，可能会导致应用中断100毫秒~100秒，这取决与你的Java应用，Java堆的大小以及其他的JVM调优参数。\n\n关于停顿的讨论大部分都集中在平均停顿或者目标停顿，很少涉及到堆压缩时的最坏停顿时间，在生产环境中堆中每千兆字节的有效数据的都将会发生大约1秒的停顿。\n\n2 ~ 4秒的停顿对大多数企业应用来说都是不能接受的，所以尽管实际的Java应用实例可能需要更多的内存空间，但实际只分配2~4GB的内存。在一些64位系统中带有很多关于伸缩性的JVM调优项，使得这些系统可以运行16GB乃至20GB的堆空间，并能满足典型响应时间的SLA。但是这些离现实较远，JVM目前的技术无法在进行堆压缩时避免停顿应用程序。Java应用开发人员苦于处理这两个为我们大多数人所抱怨的任务。\n\n - 架构/建模在大量的实例池之上，随之而来的是复杂的监控和管理操作。\n\n - 反复的JVM和应用程序调优以避免“stop the world“引起的停顿。大多数程序员希望停顿不要发生在系统峰值负载期间。我称之为不可能的目标。\n\n现在让我们深入一点Java的可伸缩性问题。\n\n#### **过度供给或过度实例化Java部署**\n\n为了充分利用内存资源，普通的做法是将Java应用部署在多个应用服务器实例上而不是一个或者少数应用服务器实例上。当一台Server上运行16个应用服务器实例可以充分利用所有的内存资源，但如此无法解决的是多实例的监控以及管理所带来的成本，尤其是当你的应用部署在多个Server上。\n\n另一个问题来了，峰值负载时的内存资源不是每天都需要的，这样就形成了巨大的浪费。有些情况下，一台物理机上可能只不是不超过3个“大应用服务器实例”，这样的部署更加不够经济也不够环保，尤其在非峰值负载期间。\n\n让我们来比较一下这两种部署架构，下图中左边是多而小的应用服务器实例部署模式，右边是少而大的应用服务器实例部署模式。两种模式处理同样的负载，究竟哪一种部署架构更具经济性。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylf1hzmaaj20k10dg0v7.jpg)\n\n大应用服务器部署场景 (图片来源：Azul Systems)\n\n如我之前说过的，并发压缩使得大应用服务器部署模式变得可行，而且可以突破JVM可伸缩性的限制。目前只有Azul的Zing JVM可以提供并发压缩的技术，另外Zing是Server侧的JVM，我们很乐意看到越来越多的开发者在JVM层面去挑战Java可伸缩性的问题。\n\n由于性能调优仍然是我们解决Java可伸缩性问题的主要手段，我们先来看有哪些主要的调优参数以及通过它们能达到什么样的效果。\n\n#### **调优参数：一些事例**\n\n最著名的调优参数莫过于”-Xmx”了，通过该参数可以指定Java的堆空间大小，实际上可能不同的JVM执行结果不太一样。\n\n有的JVM包含了内部结构(如编译器线程，垃圾回收器结构，代码缓存等等)所需要的内存在“-Xmx”的设定中，而有的则不包含。因此用户Java进程的大小不一定跟“-Xmx”的设定相吻合。\n\n如果你的应用程序分配对象的速率，对象的生命周期，或者对象的大小超过了JVM内存相关配置，一旦达到最大可使用内存的阈值将会发生内存溢出，用户进程则会停止。\n\n当你的应用程序纠结于内存的可用性时，最有效的方法就是通过”-Xmx”指定更大的内存去重启当前应用进程。为了避免频繁的重启，大多数企业生产环境都倾向于指定峰值负载时所需要的内存，造成过度配置优化。\n\n**提示：生产环境负载的调整**\n\nJava开发人员易犯的常见错误是在实验下的做的堆内存设置，在移植到生产环境是忘记重新调整。生产环境和实验室环境是不一样的，谨记根据生产环境的负载重新调整堆内存设置。\n\n##### **分代垃圾回收器调优**\n\n还有一些其他的优化选项”-Xns”和”-XX: NewSize”，用来调整年轻代的大小，用来指定堆中专门负责新对象分配的空间大小。\n\n大多数开发者都试图基于实验室环境调整年轻代的大小，这意味着在生产负载下存在失败的风险。一般新生代的大小设置为堆大小的三分之一至二分之一左右，但这不是一个准则，毕竟实际还要视应用程序逻辑而定。因此最好先调查清楚年轻代到年老代的蜕变率以及年老代对象的大小，在此基础上(确保年老代的大小，年老代过小会频繁促发GC导致内存溢出错误)尽可能地调大年轻代的空间。\n\n还有一个与年轻代相关的调优项”-XX:SurvivorRatio”，该选项用来指定年轻代中对象的生命周期，超过指定时长相关对象将被移至年老代。为了”正确”地设定该值，你需要知道年轻代空间回收的频率，能够估算到新对象在应用程序进程中被引用的时长，同时也取决于分配率。\n\n##### **并发垃圾回收调优**\n\n针对对停顿敏感的应用，建议使用并发垃圾回收，虽然并行的办法能够带来非常好的吞吐量基准测试分数，但是并行GC不利于缩短响应时间。并发 GC 是目前唯一有效的实现一致性和最少“stop the world”中断的方法。不同的JVM提供不同的并发GC的设定，Oracle JVM(hotspot)提供”-XX:+UseConcMarkSweepGC”,今后G1将成为Oracle JVM默认的并发垃圾回收器。\n\n **性能调优并不是真正的解决办法**\n\n或许你已经注意到上文中在讨论如何“正确“地设定调优此参数时，我刻意在”正确“二字上加了双引号。那是因为就我个人经验而言一旦涉及到性能参数调优，就没有严格意义上的正确设定。每一个设定值都是针对特定的场景。考虑到应用场景会发生变化，JVM 性能调整充其量是一个权宜之计。\n\n以堆的设置为例：如果2GB的堆可以应对20万并发用户，但是可能不能应付40万的并发用户。\n\n我们再以”-XX:SurvivorRatio”为例：当设定符合一个负载持续增长最高至每毫秒10000个交易的场景，当压力到达每毫秒50000个交易时又会发生什么呢？\n\n大多数企业级应用负载都是动态的，Java语言的动态内存管理以及动态编译等技术使得Java更加适合企业级应用。我们来看看一下两个配置清单。\n\n清单1. 应用程序(1)的启动选项\n\n````java\n>java -Xmx12g -XX:MaxPermSize=64M -XX:PermSize=32M -XX:MaxNewSize=2g \n-XX:NewSize=1g -XX:SurvivorRatio=16 -XX:+UseParNewGC \n-XX:+UseConcMarkSweepGC -XX:MaxTenuringThreshold=0 \n-XX:CMSInitiatingOccupancyFraction=60 -XX:+CMSParallelRemarkEnabled \n-XX:+UseCMSInitiatingOccupancyOnly -XX:ParallelGCThreads=12 \n-XX:LargePageSizeInBytes=256m …\n````\n\n清单2\\. 应用程序(2)的启动选项\n\n````java\n>java --Xms8g --Xmx8g --Xmn2g -XX:PermSize=64M -XX:MaxPermSize=256M \n-XX:-OmitStackTraceInFastThrow -XX:SurvivorRatio=2 -XX:-UseAdaptiveSizePolicy -XX:+UseConcMarkSweepGC \n-XX:+CMSConcurrentMTEnabled -XX:+CMSParallelRemarkEnabled -XX:+CMSParallelSurvivorRemarkEnabled \n-XX:CMSMaxAbortablePrecleanTime=10000 -XX:+UseCMSInitiatingOccupancyOnly \n-XX:CMSInitiatingOccupancyFraction=63 -XX:+UseParNewGC --Xnoclassgc …\n````\n\n两者的配置区别很大，因为他们是两个不同应用程序。感觉根据各自的应用特设都做了”正确“的配置与调优。在实验室环境下都运行良好，但在生产环境中最终会表现出疲态。清单1由于没有考虑到动态负载，到了生产环境即表现不良。清单2没有考虑到应用程序在生产环境中的特性变化。这两种情况应该归咎于开发团队，但是该归咎于何处呢？\n\n**变通办法可行吗？**\n\n有些企业通过精确测量交易对象的大小定义极致的对象回收空间并”精简“其架构来适配该空间。这也许是办法来削减碎片以应对一整天的交易(在不做堆压缩的情况下)。还有一个办法就是通过程序设计确保对象被引用的时间在一个比较短的时间内从而阻止其在SurvivorRatio时间之后不被迁往年老代而直接被回收，避免内存压缩的场景。这两种办法都可以，但是对应用开发人员和设计人员有一定的挑战。\n\n**谁保障应用程序的性能？**\n\n一个门户应用可能会在其活动负载峰值点出现故障；一个交易应用可能会在每次市场下跌和上升时无法正常运行；电子商务网站可能会无法应对节假日购物高峰期。这些都是真实世界的案例基本都是JVM性能参数调优导致的。当产生了经济损失，开发团队就会受到责备。也许某些场合下开发团队应该要受到责备，但是JVM的提供商又应该负起什么样儿的责任呢？\n\n首先JVM提供商应该要提供调优参数的优先顺序，至少这在短期内还是很有意义的。有一些新的调优选项是针对特定的、 新兴的企业应用程序场景。更多的调优选项是为了减轻JVM支持团队的工作负荷而将性能优化转嫁到应用开发者身上。但我个人认为这或将导致更加漫长的支持负荷，一些针对最糟糕场景的调优选项也将被延期，当然不是无限延期。\n\n毋庸置疑JVM的开发团队也在努力地进行着他们的工作，同时也只有应用实施者才会更加清楚他们应用的特定需求。但是应用的实施者或开发者是无法预测期动态的负载需求。在过去，JVM提供商也会去分析关于Java的性能与可扩展性问题，哪些是他们能够解决的。不是提供调优参数，而是直接去优化或创新垃圾回收的算法。更有趣是我们可以想象一下如果OpenJDK的社区聚集在一起重新考虑Java垃圾回收器将会发生什么！\n\n#### **JVM****性能的基准测试**\n\n调优参数有时被JVM提供商作为其竞争的工具，因为不同的调优可以改善他们的JVM在可预见的环境中的性能表现，本系列的最后一片文章中将调查这些基准测试来衡量JVM的性能。\n\n##### **JVM****开发者的挑战**\n\n真正的企业级可伸缩性需求是要求JVM能够适应动态灵活的应用负载。这是在特定吞吐量和响应时间内保证持续稳定性能的关键。这是JVM开发者才能完成历史使命，因此是时候号召我们Java开发者社区来迎接真正的Java可伸缩性的挑战。\n\n - 持续调优\n\n对于给定的应用，在一开始需要告知其需要多大的内存，之后的工作都应该有JVM来负责 ，JVM需要适配动态的应用负载和运行场景。\n\n- JVM实例数 vs. 实例的可扩展性\n\n现在的服务器都支持很大的内存，那么为什么JVM实例不能有效地利用它呢？将应用拆分部署许多小的应用服务器实例上，这从经济和环保角度都是一种浪费。现代的JVM需要跟上硬件和应用的发展潮流。\n\n - 真实世界的性能和可伸缩性\n\n企业不需要为其应用的性能需求去做极致的性能调优。JVM提供商和OpenJDK社区需要去解决Java可伸缩性的核心问题以及消除“stop the world“的操作。\n\n#### **结论**\n\n如果JVM做了这样的工作，并且提供了并发压缩的垃圾回收算法，JVM也不再成为Java可伸缩性的限制因素，Java应用开发者不需要花费痛苦的时间理解怎样配置JVM去获得最佳性能，从而将会有更多的有趣的Java应用层面的创新，而不是无休止的JVM调优。我要挑战JVM开发人员以及提供商所需要做的事情来相应甲骨文所提倡的“Make the Java Future“的活动。\n\n#### **JVM 性能优化系列**\n\n第一篇 《[JVM性能优化， Part 1 ―― JVM简介](https://chiyuanbo.github.io/2018/04/28/JVM%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%20Part%201%20%E2%80%95%E2%80%95%20JVM%E7%AE%80%E4%BB%8B/) 》\n\n第二篇《[JVM性能优化， Part 2 ―― 编译器](https://chiyuanbo.github.io/2018/04/28/JVM%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%20Part%202%20%E2%80%95%E2%80%95%20%E7%BC%96%E8%AF%91%E5%99%A8/)》\n\n第三篇[《JVM性能优化， Part 3 —— 垃圾回收》](https://chiyuanbo.github.io/2018/04/28/JVM%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%20Part%203%20%20%E2%80%95%E2%80%95%20%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/)\n\n第四篇[《JVM性能优化， Part 4 —— C4 垃圾回收》](https://chiyuanbo.github.io/2018/04/28/JVM%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%20Part%204%20%E2%80%95%E2%80%95%20C4%20%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/)\n\n第五篇[《JVM性能优化， Part 5 —— Java的伸缩性》](https://chiyuanbo.github.io/2018/04/28/JVM%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%20Part%205%20%20%E2%80%95%E2%80%95%20Java%E7%9A%84%E4%BC%B8%E7%BC%A9%E6%80%A7/)","tags":["Java","JVM"]},{"title":"JVM性能优化， Part 4  ―― C4 垃圾回收","url":"/2022/10/27/cyb-mds/java/jvm/JVM性能优化， Part 4 ―― C4 垃圾回收/","content":"\n==作者：Eva Andreasson,译者：曹旭东==\n\n[toc]\n\n本文是JVM性能优化 系列-第4篇。前3篇文章请参考文章结尾处的JVM优化系列文章。作为Eva Andreasson的JVM性能优化系列的第4篇，本文将对C4垃圾回收器进行介绍。使用C4垃圾回收器可以有效提升对低延迟有要求的企业级Java应用程序的伸缩性。\n\n到目前为止，本系列的文章将stop-the-world式的垃圾回收视为影响Java应用程序伸缩性的一大障碍，而伸缩性又是现代企业级Java应用程序开发的基础要求，因此这一问题亟待改善。幸运的是，针对此问题，JVM中已经出现了一些新特性，所使用的方式或是对stop-the-world式的垃圾回收做微调，或是消除冗长的暂停（这样更好些）。在一些多核系统中，内存不再是稀缺资源，因此，JVM的一些新特性就充分利用多核系统的潜在优势来增强Java应用程序的伸缩性。\n\n在本文中，我将着重介绍C4算法，该算法是Azul System公司中无暂停垃圾回收算法的新成果，目前只在Zing JVM上得到实现。此外，本文还将对Oracle公司的G1垃圾回收算法和IBM公司的Balanced Garbage Collection Policy算法做简单介绍。希望通过对这些垃圾回收算法的学习可以扩展你对Java内存管理模型和Java应用程序伸缩性的理解，并激发你对这方面内容的兴趣以便更深入的学习相关知识。至少，你可以学习到在选择JVM时有哪些需要关注的方面，以及在不同应用程序场景下要注意的事项。\n\n##### **C4算法中的并发性**\n\nAzul System公司的C4（Concurrent Continuously Compacting Collector，译者注，Azul官网给出的名字是Continuously Concurrent Compacting Collector）算法使用独一无二而又非常有趣的方法来实现低延迟的分代式垃圾回收。相比于大多数分代式垃圾回收器，C4的不同之处在于它认为垃圾回收并不是什么坏事（即应用程序产生垃圾很正常），而压缩是不可避免的。在设计之初，C4就是要牺牲各种动态内存管理的需求，以满足需要长时间运行的服务器端应用程序的需求。\n\nC4算法将释放内存的过程从应用程序行为和内存分配速率中分离出来，并加以区分。这样就实现了并发运行，即应用程序可以持续运行，而不必等待垃圾回收的完成。其中的并发性是关键所在，正是由于并发性的存在才可以使暂停时间不受垃圾回收周期内堆上活动数据数量和需要跟踪与更新的引用数量的影响，将暂停时间保持在较低的水平。正如我在本系列[第3篇](https://github.com/chiyuanbo/cyb-mds/blob/master/java/jvm/JVM%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%20Part%203%20%20%E2%80%95%E2%80%95%20%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6.md)中介绍的一样，大多数垃圾回收器在工作周期内都包含了stop-the-world式的压缩过程，这就是说应用程序的暂停时间会随活动数据总量和堆中对象间引用的复杂度的上升而增加。使用C4算法的垃圾回收器可以并发的执行压缩操作，即压缩与应用程序线程同时工作，从而解决了影响JVM伸缩性的最大难题。\n\n实际上，为了实现并发性，C4算法改变了现代Java企业级架构和部署模型的基本假设。想象一下拥有数百GB内存的JVM会是什么样的：\n\n*   部署Java应用程序时，对伸缩性的要求无需要多个JVM配合，在单一JVM实例中即可完成。这时的部署是什么样呢？\n*   有哪些以往因GC限制而无法在内存存储的对象？\n*   那些分布式集群（如缓存服务器、区域服务器，或其他类型的服务器节点）会有什么变化？当可以增加JVM内存而不会对应用程序响应时间造成负面影响时，传统的节点数量、节点死亡和缓存丢失的计算会有什么变化呢？\n\n##### **C4算法的3的阶段**\n\nC4算法的一个基本假设是“垃圾回收不是坏事”和“压缩不可避免”。C4算法的设计目标是实现垃圾回收的并发与协作，剔除stop-the-world式的垃圾回收。C4垃圾回收算法包含一下3个阶段：\n\n1.  _标记（Marking）_ — 找到活动对象\n2.  _重定位（Relocation）_ — 将存活对象移动到一起，以便可以释放较大的连续空间，这个阶段也可称为“压缩（compaction）”\n3.  _重映射（Remapping）_ — 更新被移动的对象的引用。\n\n下面的内容将对每个阶段做详细介绍。\n\n##### **C4算法中的标记阶段**\n\n在C4算法中，_标记阶段（marking phase）_使用了_并发标记（concurrent marking）_和引用跟踪_（reference-tracing）_的方法来标记活动对象，这方面内容已经在本系列的[第3篇](https://chiyuanbo.github.io/2018/04/28/JVM%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%20Part%203%20%20%E2%80%95%E2%80%95%20%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/)中介绍过。\n\n在标记阶段中，GC线程会从线程栈和寄存器中的活动对象开始，遍历所有的引用，标记找到的对象，这些GC线程会遍历堆上所有的可达（reachable）对象。在这个阶段，C4算法与其他并发标记器的工作方式非常相似。\n\nC4算法的标记器与其他并发标记器的区别也是始于并发标记阶段的。在并发标记阶段中，如果应用程序线程修改未标记的对象，那么该对象会被放到一个队列中，以备遍历。这就保证了该对象最终会被标记，也因为如此，C4垃圾回收器或另一个应用程序线程不会重复遍历该对象。这样就节省了标记时间，消除了递归重标记（recursive remark）的风险。（注意，长时间的递归重标记有可能会使应用程序因无法获得足够的内存而抛出OOM错误，这也是大部分垃圾回收场景中的普遍问题。）\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fyleyveux0j20jh0aoq3q.jpg)\n\nFigure 1\\. Application threads traverse the heap just once during marking\n\n如果C4算法的实现是基于脏卡表（dirty-card tables）或其他对已经遍历过的堆区域的读写操作进行记录的方法，那垃圾回收线程就需要重新访问这些区域做重标记。在极端条件下，垃圾回收线程会陷入到永无止境的重标记中 —— 至少这个过程可能会长到使应用程序因无法分配到新的内存而抛出OOM错误。但C4算法是基于_LVB（load value barrier）_实现的，LVB具有自愈能力，可以使应用程序线程迅速查明某个引用是否已经被标记过了。如果这个引用没有被标记过，那么应用程序会将其添加到GC队列中。一旦该引用被放入到队列中，它就不会再被重标记了。应用程序线程可以继续做它自己的事。\n\n_脏对象（dirty object）和卡表（card table）_\n由于某些原因（例如在一个并发垃圾回收周期中，对象被修改了），垃圾回收器需要重新访问某些对象，那么这些对象_脏对象（dirty object）_。这这些脏对象，或堆中脏区域的引用，通过会记录在一个专门的数据结构中，这就是卡表。\n\n在C4算法中，并没有重标记（re-marking）这个阶段，在第一次便利整个堆时就会将所有可达对象做标记。因为运行时不需要做重标记，也就不会陷入无限循环的重标记陷阱中，由此而降低了应用程序因无法分配到内存而抛出OOM错误的风险。\n\n##### **C4算法中的重定位 ——　应用程序线程与GC的协作**\n\nC4算法中，*重定位阶段（reloacation phase）*是由GC线程和应用程序线程以协作的方式，并发完成的。这是因为GC线程和应用程序线程会同时工作，而且无论哪个线程先访问将被移动的对象，都会以协作的方式帮助完成该对象的移动任务。因此，应用程序线程可以继续执行自己的任务，而不必等待整个垃圾回收周期的完成。\n\n正如Figure 2所示，碎片内存页中的活动对象会被重定位。在这个例子中，应用程序线程先访问了要被移动的对象，那么应用程序线程也会帮助完成移动该对象的工作的初始部分，这样，它就可以很快的继续做自己的任务。虚拟地址（指相关引用）可以指向新的正确位置，内存也可以快速回收。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylezgrpswj20jh0azaau.jpg)\n\nFigure 2\\. A page selected for relocation and the empty new page that it will be moved to\n\n如果是GC线程先访问到了将被移动的对象，那事情就简单多了，GC线程会执行移动操作的。如果在重映射阶段（re-mapping phase，后续会提到）也访问这个对象，那么它必须检查该对象是否是要被移动的。如果是，那么应用程序线程会重新定位这个对象的位置，以便可以继续完成自己任务。（对大对象的移动是通过将该对象打碎再移动完成的。如果你对这部分内容感兴趣的话，推荐你阅读一下相关资源中的这篇白皮书“C4: The Continuously Concurrent Compacting Collector”）\n\n当所有的活动对象都从某个内存也中移出后，剩下的就都是垃圾数据了，这个内存页也就可以被整体回收了。正如Figure 2中所示。\n\n_关于清理_\n在C4算法中并没有清理阶段（sweep phase），因此也就不需要这个在大多数垃圾回收算法中比较常用的操作。在指向被移动的对象的引用都更新为指向新的位置之前，from页中的虚拟地址空间必须被完整保留。所以C4算法的实现保证了，在所有指向这个页的引用处于稳定状态前，所有的虚拟地址空间都会被锁定。然后，算法会立即回收物理内存页。\n\n很明显，无需执行stop-the-world式的移动对象是有很大好处的。由于在重定位阶段，所有活动对象都是并发移动的，因此它们可以被更有效率的放入到相邻的地址中，并且可以充分的压缩。通过并发执行重定位操作，堆被压缩为连续空间，也无需挂起所有的应用程序线程。这种方式消除了Java应用程序访问内存的传统限制（更多关于Java应用程序内存模型的内容参见ImportNew编译整理的第一篇《[JVM性能优化， Part 1 ―― JVM简介](https://chiyuanbo.github.io/2018/04/28/JVM%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%20Part%201%20%E2%80%95%E2%80%95%20JVM%E7%AE%80%E4%BB%8B/)》）。\n\n经过上述的过程后，如何更新引用呢？如何实现一个非stop-the-world式的操作呢？\n\n##### **C4算法中的重映射**\n\n在重定位阶段，某些指向被移动的对象的引用会自动更新。但是，在重定位阶段，那些指向了被移动的对象的引用并没有更新，仍然指向原处，所以它们需要在后续完成更新操作。C4算法中的_重映射阶段（re-mapping phase）_负责完成对那些活动对象已经移出，但仍指向那些的引用进行更新。当然，重映射也是一个协作式的并发操作。\n\nFigure 3中，在重定位阶段，活动对象已经被移动到了一个新的内存页中。在重定位之后，GC线程立即开始更新那些仍然指向之前的虚拟地址空间的引用，将它们指向那些被移动的对象的新地址。垃圾回收器会一直执行此项任务，直到所有的引用都被更新，这样原先虚拟内存空间就可以被整体回收了。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylf05n3byj20jh0c8t9f.jpg)\n\nFigure 3\\. Whatever thread finds an invalid address enables an update to the correct new address\n\n但如果在GC完成对所有引用的更新之前，应用程序线程想要访问这些引用的话，会出现什么情况呢？在C4算法中，应用程序线程可以很方便的帮助完成对引用进行更新的工作。如果在重映射阶段，应用程序线程访问了处于非稳定状态的引用，它会找到该引用的正确指向。如果应用程序线程找到了正确的引用，它会更新该引用的指向。当完成更新后，应用程序线程会继续自己的工作。\n\n协作式的重映射保证了引用只会被更新一次，该引用下的子引用也都可以指向正确的新地址。此外，在大多数其他GC实现中，引用指向的地址不会被存储在该对象被移动之前的位置；相反，这些地址被存储在一个堆外结构（off-heap structure）中。这样，无需在对所有引用的更新完成之前，再花费精力保持整个内存页完好无损，这个内存页可以被整体回收。\n\n##### **C4算法真的是无暂停的么？**\n\n在C4算法的重映射阶段，正在跟踪引用的线程仅会被中断一次，而这次中断仅仅会持续到对该引用的检索和更新完成，在这次中断后，线程会继续运行。相比于其他并发算法来说，这种实现会带来巨大的性能提升，因为其他的并发立即回收算法需要等到每个线程都运行到一个安全点（safe point），然后同时挂起所有线程，再开始对所有的引用进行更新，完成后再恢复所有线程的运行。\n\n对于并发压缩垃圾回收器来说，由于垃圾回收所引起的暂停从来都不是问题。在C4算法的重定位阶段中，也不会有再出现更糟的碎片化场景了。实现了C4算法的垃圾回收器也不会出现背靠背（back-to-back）式的垃圾回收周期，或者是因垃圾回收而使应用程序暂停数秒甚至数分钟。如果你曾经体验过这种stop-the-world式的垃圾回收，那么很有可能是你给应用程序设置的内存太小了。你可以试用一下实现了C4算法的垃圾回收器，并为其分配足够多的内存，而完全不必担心暂停时间过长的问题。\n\n##### **评估C4算法和其他可选方案**\n\n像往常一样，你需要针对应用程序的需求选择一款JVM和垃圾回收器。C4算法在设计之初就是无论堆中活动数据有多少，只要应用程序还有足够的内存可用，暂停时间都始终保持在较低的水平。正因如此，对于那些有大量内存可用，而对响应时间比较敏感的应用程来说，选择实现了C4算法的垃圾回收器正是不二之选。\n\n而对于那些要求快速启动，内存有限的客户端应用程序来说，C4就不是那么适用。而对于那些对吞吐量有较高要求的应用程序来说，C4也并不适用。真正能够发挥C4威力的是那些为了提升应用程序工作负载而在每台服务器上部署了4到16个JVM实例的场景。此外，如果你经常要对垃圾回收器做调优的话，那么不妨考虑一下使用C4算法。综上所述，当响应时间比吞吐量占有更高的优先级时，C4是个不错的选择。而对那些不能接受长时间暂停的应用程序来说，C4是个理想的选择。\n\n如果你正考虑在生产环境中使用C4，那么你可能还需要重新考虑一下如何部署应用程序。例如，不必为每个服务器配置16个具有2GB堆的JVM实例，而是使用一个64GB的JVM实例（或者增加一个作为热备份）。C4需要尽可能大的内存来保证始终有一个空闲内存页来为新创建的对象分配内存。（记住，内存不再是昂贵的资源了！）\n\n如果你没有64GB，128GB，或1TB（或更多）内存可用，那么分布式的多JVM部署可能是一个更好的选择。在这种场景中，你可以考虑使用Oracle HotSpot JVM的G1垃圾回收器，或者IBM JVM的平衡垃圾回收策略（Balanced Garbage Collection Policy）。下面将对这两种垃圾回收器做简单介绍。\n\n##### **Gargabe-First （G1） 垃圾回收器**\n\nG1垃圾回收器是新近才出现的垃圾回收器，是Oracle HotSpot JVM的一部分，在最近的JDK1.6版本中首次出现（译者注，该文章写于2012-07-11）。在启动Oracle JDK时附加命令行选项_-XX:+UseG1GC_，可以启动G1垃圾回收器。\n\n与C4类似，这款标记-清理（mark-and-sweep）垃圾回收器也可作为对低延迟有要求的应用程序的备选方案。G1算法将堆分为固定大小区域，垃圾回收会作用于其中的某些区域。在应用程序线程运行的同时，启用后台线程，并发的完成标记工作。这点与其他并发标记算法相似。\n\nG1增量方法可以使暂停时间更短，但更频繁，而这对一些力求避免长时间暂停的应用程序来说已经足够了。另一方面，正如在本系列的[Part 3][4]中介绍的，使用G1垃圾回收器需要针对应用程序的实际需求做长时间的调优，而其GC中断又是stop-the-world式的。所以对那些对低延迟有很高要求的应用程序来说，G1并不是一个好的选择。进一步说，从暂停时间总长来看，G1长于CMS（Oracle JVM中广为人知的并发垃圾回收器）。\n\nG1使用拷贝算法（在Part 3中介绍过）完成部分垃圾回收任务。这样，每次垃圾回收器后，都会产生完全可用的空闲空间。G1垃圾回收器定义了一些区域的集合作为年轻代，剩下的作为老年代。\n\nG1已经吸引了足够多的注意，引起了不小的轰动，但是它真正的挑战在于如何应对现实世界的需求。正确的调优就是其中一个挑战 —— 回忆一下，对于动态应用程序负载来说，没有永远“正确的调优”。一个问题是如何处理与分区大小相近的大对象，因为剩余的空间会成为碎片而无法使用。还有一个性能问题始终困扰着低延迟垃圾回收器，那就是垃圾回收器必须管理额外的数据结构。就我来说，使用G1的关键问题在于如何解决stop-the-world式垃圾回收器引起的暂停。Stop-the-world式的垃圾回收引起的暂停使任何垃圾回收器的能力都受制于堆大小和活动数据数量的增长，对企业级Java应用程序的伸缩性来说是一大困扰。\n\n##### **IBM JVM的平衡垃圾回收策略（Balanced Garbage Collection Policy）**\n\nIBM JVM的平衡垃圾回收（Balanced Garbage Collection BGC）策略通过在启动IBM JDK时指定命令行选项_-Xgcpolicy:balanced_来启用。乍一看，BGC很像G1，它也是将Java堆划分成相同大小的空间，称为区间（region），执行垃圾回收时会对每个区间单独回收。为了达到最佳性能，在选择要执行垃圾回收的区间时使用了一些启发性算法。BGC中关于代的划分也与G1相似。\n\nIBM的平衡垃圾回收策略仅在64位平台得到实现，是一种NUMA架构（Non-Uniform Memory Architecture），设计之初是为了用于具有4GB以上堆的应用程序。由于拷贝算法或压缩算法的需要，BGC的部分垃圾回收工作是stop-the-world式的，并非完全并发完成。所以，归根结底，BGC也会遇到与G1和其他没有实现并发压缩选法的垃圾回收器相似的问题。\n\n##### **结论：回顾**\n\nC4是基于引用跟踪的、分代式的、并发的、协作式垃圾回收算法，目前只在Azul System公司的Zing JVM得到实现。C4算法的真正价值在于：\n\n*   消除了重标记可能引起的重标记无限循环，也就消除了在标记阶段出现OOM错误的风险。\n*   压缩，以自动、且不断重定位的方式消除了固有限制：堆中活动数据越多，压缩所引起的暂停越长。\n*   垃圾回收不再是stop-the-world式的，大大降低垃圾回收对应用程序响应时间造成的影响。\n*   没有了清理阶段，降低了在完成GC之前就因为空闲内存不足而出现OOM错误的风险。\n*   内存可以以页为单位立即回收，使那些需要使用较多内存的Java应用程序有足够的内存可用。\n\n并发压缩是C4独一无二的优势。使应用程序线程GC线程协作运行，保证了应用程序不会因GC而被阻塞。C4将内存分配和提供足够连续空闲内存的能力完全区分开。C4使你可以为JVM实例分配尽可能大的内存，而无需为应用程序暂停而烦恼。使用得当的话，这将是JVM技术的一项革新，它可以借助于当今的多核、TB级内存的硬件优势，大大提升低延迟Java应用程序的运行速度。\n\n如果你不介意一遍又一遍的调优，以及频繁的重启的话，如果你的应用程序适用于水平部署模型的话（即部署几百个小堆JVM实例而不是几个大堆JVM实例），G1也是个不错的选择。\n\n对于动态低延迟启发性自适应（dynamic low-latency heuristic adaption）算法而言，BGC是一项革新，JVM研究者对此算法已经研究了几十年。该算法可以应用于较大的堆。而动态自调优算法（ dynamic self-tuning algorithm）的缺陷是，它无法跟上突然出现的负载高峰。那时，你将不得不面对最糟糕的场景，并根据实际情况再分配相关资源。\n\n最后，为你的应用程序选择最适合的JVM和垃圾回收器时，最重要的考虑因素是应用程序中吞吐量和暂停时间的优先级次序。你想把时间和金钱花在哪？从纯粹的技术角度说，基于我十年来对垃圾回收的经验，我一直在寻找更多关于并发压缩的革新性技术，或其他可以以较小代价完成移动对象或重定位的方法。我想影响企业级Java应用程序伸缩性的关键就在于并发性。\n\n##### **JVM 性能优化系列**\n\n第一篇 《[JVM性能优化， Part 1 ―― JVM简介](https://chiyuanbo.github.io/2018/04/28/JVM%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%20Part%201%20%E2%80%95%E2%80%95%20JVM%E7%AE%80%E4%BB%8B/) 》\n\n第二篇《[JVM性能优化， Part 2 ―― 编译器](https://chiyuanbo.github.io/2018/04/28/JVM%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%20Part%202%20%E2%80%95%E2%80%95%20%E7%BC%96%E8%AF%91%E5%99%A8/)》\n\n第三篇[《JVM性能优化， Part 3 —— 垃圾回收》](https://chiyuanbo.github.io/2018/04/28/JVM%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%20Part%203%20%20%E2%80%95%E2%80%95%20%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/)\n\n第四篇[《JVM性能优化， Part 4 —— C4 垃圾回收》](https://chiyuanbo.github.io/2018/04/28/JVM%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%20Part%204%20%E2%80%95%E2%80%95%20C4%20%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/)\n\n第五篇[《JVM性能优化， Part 5 —— Java的伸缩性》](https://chiyuanbo.github.io/2018/04/28/JVM%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%20Part%205%20%20%E2%80%95%E2%80%95%20Java%E7%9A%84%E4%BC%B8%E7%BC%A9%E6%80%A7/)\n","tags":["Java","JVM"]},{"title":"《JAVA与模式》之抽象工厂模式","url":"/2022/10/27/cyb-mds/java/Design pattern/《JAVA与模式》之抽象工厂模式/","content":"\n==作者：YuanBo-Chi==\n\n[toc]\n\n场景问题\n　　举个生活中常见的例子——组装电脑，我们在组装电脑的时候，通常需要选择一系列的配件，比如CPU、硬盘、内存、主板、电源、机箱等。为讨论使用简单点，只考虑选择CPU和主板的问题。\n\n　　事实上，在选择CPU的时候，面临一系列的问题，比如品牌、型号、针脚数目、主频等问题，只有把这些问题都确定下来，才能确定具体的CPU。\n\n　　同样，在选择主板的时候，也有一系列问题，比如品牌、芯片组、集成芯片、总线频率等问题，也只有这些都确定了，才能确定具体的主板。\n\n　　选择不同的CPU和主板，是每个客户在组装电脑的时候，向装机公司提出的要求，也就是我们每个人自己拟定的装机方案。\n\n　　在最终确定这个装机方案之前，还需要整体考虑各个配件之间的兼容性。比如：CPU和主板，如果使用Intel的CPU和AMD的主板是根本无法组装的。因为Intel的CPU针脚数与AMD主板提供的CPU插口不兼容，就是说如果使用Intel的CPU根本就插不到AMD的主板中，所以装机方案是整体性的，里面选择的各个配件之间是有关联的。\n\n　　对于装机工程师而言，他只知道组装一台电脑，需要相应的配件，但是具体使用什么样的配件，还得由客户说了算。也就是说装机工程师只是负责组装，而客户负责选择装配所需要的具体的配件。因此，当装机工程师为不同的客户组装电脑时，只需要根据客户的装机方案，去获取相应的配件，然后组装即可。\n\n使用简单工厂模式的解决方案\n　　考虑客户的功能，需要选择自己需要的CPU和主板，然后告诉装机工程师自己的选择，接下来就等着装机工程师组装电脑了。\n\n　　对装机工程师而言，只是知道CPU和主板的接口，而不知道具体实现，很明显可以用上简单工厂模式或工厂方法模式。为了简单，这里选用简单工厂。客户告诉装机工程师自己的选择，然后装机工程师会通过相应的工厂去获取相应的实例对象。\n　　\n![enter description here](https://raw.githubusercontent.com/chiyuanbo/pic/master/v2-fa4cf24b47bd2d5bff17b0abd1de315e_720w.jpg)\n　　\n\n源代码\nCPU接口与具体实现\n\n````java\npublic interface Cpu {\n\tpublic void calculate();\n}\n\npublic class IntelCpu implements Cpu {\n\t/**\n\t * CPU的针脚数\n\t */\n\tprivate int pins = 0;\n\tpublic  IntelCpu(int pins){\n\t\tthis.pins = pins;\n\t}\n\t@Override\n\tpublic void calculate() {\n\t\tSystem.out.println(\"Intel CPU的针脚数：\" + pins);\n\t}\n\n}\n\npublic class AmdCpu implements Cpu {\n\t/**\n\t * CPU的针脚数\n\t */\n\tprivate int pins = 0;\n\tpublic  AmdCpu(int pins){\n\t\tthis.pins = pins;\n\t}\n\t@Override\n\tpublic void calculate() {\n\t\tSystem.out.println(\"AMD CPU的针脚数：\" + pins);\n\t}\n}\n````\n\n主板接口与具体实现\n\n````java\npublic interface Mainboard {\n\tpublic void installCPU();\n}\n\npublic class IntelMainboard implements Mainboard {\n\t/**\n\t * CPU插槽的孔数\n\t */\n\tprivate int cpuHoles = 0;\n\t/**\n\t * 构造方法，传入CPU插槽的孔数\n\t * @param cpuHoles\n\t */\n\tpublic IntelMainboard(int cpuHoles){\n\t\tthis.cpuHoles = cpuHoles;\n\t}\n\t@Override\n\tpublic void installCPU() {\n\t\t// TODO Auto-generated method stub\n\t\tSystem.out.println(\"Intel主板的CPU插槽孔数是：\" + cpuHoles);\n\t}\n\n}\n\npublic class AmdMainboard implements Mainboard {\n\t/**\n\t * CPU插槽的孔数\n\t */\n\tprivate int cpuHoles = 0;\n\t/**\n\t * 构造方法，传入CPU插槽的孔数\n\t * @param cpuHoles\n\t */\n\tpublic AmdMainboard(int cpuHoles){\n\t\tthis.cpuHoles = cpuHoles;\n\t}\n\t@Override\n\tpublic void installCPU() {\n\t\tSystem.out.println(\"AMD主板的CPU插槽孔数是：\" + cpuHoles);\n\t}\n}\n````\n\nCPU与主板工厂类\n\n````java\n    public class CpuFactory {\n        public static Cpu createCpu(int type){\n            Cpu cpu = null;\n            if(type == 1){\n                cpu = new IntelCpu(755);\n            }else if(type == 2){\n                cpu = new AmdCpu(938);\n            }\n            return cpu;\n        }\n    }\n\n    public class MainboardFactory {\n        public static Mainboard createMainboard(int type){\n            Mainboard mainboard = null;\n            if(type == 1){\n                mainboard = new IntelMainboard(755);\n            }else if(type == 2){\n                mainboard = new AmdMainboard(938);\n            }\n            return mainboard;\n        }\n    }\n````\n\n装机工程师类与客户类运行结果如下：\n\n````java\npublic class ComputerEngineer {\n\t/**\n\t * 定义组装机需要的CPU\n\t */\n\tprivate Cpu cpu = null;\n\t/**\n\t * 定义组装机需要的主板\n\t */\n\tprivate Mainboard mainboard = null;\n\tpublic void makeComputer(int cpuType , int mainboard){\n\t\t/**\n\t\t * 组装机器的基本步骤\n\t\t */\n\t\t//1:首先准备好装机所需要的配件\n\t\tprepareHardwares(cpuType, mainboard);\n\t\t//2:组装机器\n\t\t//3:测试机器\n\t\t//4：交付客户\n\t}\n\tprivate void prepareHardwares(int cpuType , int mainboard){\n\t\t//这里要去准备CPU和主板的具体实现，为了示例简单，这里只准备这两个\n\t\t//可是，装机工程师并不知道如何去创建，怎么办呢？\n\n\t\t//直接找相应的工厂获取\n\t\tthis.cpu = CpuFactory.createCpu(cpuType);\n\t\tthis.mainboard = MainboardFactory.createMainboard(mainboard);\n\n\t\t//测试配件是否好用\n\t\tthis.cpu.calculate();\n\t\tthis.mainboard.installCPU();\n\t}\n}\n\npublic class Client {\n\tpublic static void main(String[]args){\n\t\tComputerEngineer cf = new ComputerEngineer();\n\t\tcf.makeComputer(1,1);\n\t}\n}\n````\n\n运行结果如下：\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/003tP4NQly1gu7ympyu5qj60sc045jt502.jpg)\n\n　　上面的实现，虽然通过简单工厂方法解决了：对于装机工程师，只知CPU和主板的接口，而不知道具体实现的问题。但还有一个问题没有解决，那就是这些CPU对象和主板对象其实是有关系的，需要相互匹配的。而上面的实现中，并没有维护这种关联关系，CPU和主板是由客户任意选择，这是有问题的。比如在客户端调用makeComputer时，传入参数为(1,2)，运行结果如下：\n　　\n![](https://raw.githubusercontent.com/chiyuanbo/pic/master/v2-625e9ef4a5776cede606e609c11d7e37_720w.png)　　\n\n观察上面结果就会看出问题。客户选择的是Intel的CPU针脚数为755，而选择的主板是AMD，主板上的CPU插孔是938，根本无法组装，这就是没有维护配件之间的关系造成的。该怎么解决这个问题呢？　　\n\n引进抽象工厂模式\n　　每一个模式都是针对一定问题的解决方案。抽象工厂模式与工厂方法模式的最大区别就在于，工厂方法模式针对的是一个产品等级结构；而抽象工厂模式则需要面对多个产品等级结构。\n\n　　在学习抽象工厂具体实例之前，应该明白两个重要的概念：产品族和产品等级。\n\n　　所谓产品族，是指位于不同产品等级结构中，功能相关联的产品组成的家族。比如AMD的主板、芯片组、CPU组成一个家族，Intel的主板、芯片组、CPU组成一个家族。而这两个家族都来自于三个产品等级：主板、芯片组、CPU。一个等级结构是由相同的结构的产品组成，示意图如下：\n\n![](https://raw.githubusercontent.com/chiyuanbo/pic/master/v2-0fe88b93bdb5ef1e9c3749957ad52f9b_720w.jpg)\n\n　　显然，每一个产品族中含有产品的数目，与产品等级结构的数目是相等的。产品的等级结构与产品族将产品按照不同方向划分，形成一个二维的坐标系。横轴表示产品的等级结构，纵轴表示产品族，上图共有两个产品族，分布于三个不同的产品等级结构中。只要指明一个产品所处的产品族以及它所属的等级结构，就可以唯一的确定这个产品。\n\n　　上面所给出的三个不同的等级结构具有平行的结构。因此，如果采用工厂方法模式，就势必要使用三个独立的工厂等级结构来对付这三个产品等级结构。由于这三个产品等级结构的相似性，会导致三个平行的工厂等级结构。随着产品等级结构的数目的增加，工厂方法模式所给出的工厂等级结构的数目也会随之增加。如下图：\n\n![](https://raw.githubusercontent.com/chiyuanbo/pic/master/v2-3065a771ea6bb9dd0619aeb4d3814eb6_720w.jpg)\n\n　　　　那么，是否可以使用同一个工厂等级结构来对付这些相同或者极为相似的产品等级结构呢？当然可以的，而且这就是抽象工厂模式的好处。同一个工厂等级结构负责三个不同产品等级结构中的产品对象的创建。\n\n![](https://raw.githubusercontent.com/chiyuanbo/pic/master/v2-f8b0187b19b4b802897dd53555e6bbbc_720w.jpg)\n\n　　可以看出，一个工厂等级结构可以创建出分属于不同产品等级结构的一个产品族中的所有对象。显然，这时候抽象工厂模式比简单工厂模式、工厂方法模式更有效率。对应于每一个产品族都有一个具体工厂。而每一个具体工厂负责创建属于同一个产品族，但是分属于不同等级结构的产品。\n\n抽象工厂模式结构\n　　抽象工厂模式是对象的创建模式，它是工厂方法模式的进一步推广。\n\n　　假设一个子系统需要一些产品对象，而这些产品又属于一个以上的产品等级结构。那么为了将消费这些产品对象的责任和创建这些产品对象的责任分割开来，可以引进抽象工厂模式。这样的话，消费产品的一方不需要直接参与产品的创建工作，而只需要向一个公用的工厂接口请求所需要的产品。\n\n　　通过使用抽象工厂模式，可以处理具有相同（或者相似）等级结构中的多个产品族中的产品对象的创建问题。如下图所示：\n\n![](https://raw.githubusercontent.com/chiyuanbo/pic/master/v2-c67764368e9507d06c626f8a47a072a0_720w.jpg)\n\n　　由于这两个产品族的等级结构相同，因此使用同一个工厂族也可以处理这两个产品族的创建问题，这就是抽象工厂模式。\n\n　　根据产品角色的结构图，就不难给出工厂角色的结构设计图。\n\n![](https://raw.githubusercontent.com/chiyuanbo/pic/master/v2-56de06d50a7d906c0e7e0f1bfd1675b7_720w.jpg)\n\n　　可以看出，每一个工厂角色都有两个工厂方法，分别负责创建分属不同产品等级结构的产品对象。\n\n　![](https://raw.githubusercontent.com/chiyuanbo/pic/master/v2-09a31a2e99e3f76aff96eb6148df2583_720w.jpg)　\n\n源代码\n　　前面示例实现的CPU接口和CPU实现对象，主板接口和主板实现对象，都不需要变化。\n\n　　前面示例中创建CPU的简单工厂和创建主板的简单工厂，都不再需要。\n\n　　新加入的抽象工厂类和实现类：\n\n````java\npublic interface AbstractFactory {\n\t/**\n\t * 创建CPU对象\n\t * @return CPU对象\n\t */\n\tpublic Cpu createCpu();\n\t/**\n\t * 创建主板对象\n\t * @return 主板对象\n\t */\n\tpublic Mainboard createMainboard();\n}\n\npublic class IntelFactory implements AbstractFactory {\n\n\t@Override\n\tpublic Cpu createCpu() {\n\t\t// TODO Auto-generated method stub\n\t\treturn new IntelCpu(755);\n\t}\n\n\t@Override\n\tpublic Mainboard createMainboard() {\n\t\t// TODO Auto-generated method stub\n\t\treturn new IntelMainboard(755);\n\t}\n\n}\n\npublic class AmdFactory implements AbstractFactory {\n\n\t@Override\n\tpublic Cpu createCpu() {\n\t\t// TODO Auto-generated method stub\n\t\treturn new IntelCpu(938);\n\t}\n\n\t@Override\n\tpublic Mainboard createMainboard() {\n\t\t// TODO Auto-generated method stub\n\t\treturn new IntelMainboard(938);\n\t}\n\n}\n````\n\n　　装机工程师类跟前面的实现相比，主要的变化是：从客户端不再传入选择CPU和主板的参数，而是直接传入客户已经选择好的产品对象。这样就避免了单独去选择CPU和主板所带来的兼容性问题，客户要选就是一套，就是一个系列。\n\n````java\npublic class ComputerEngineer {\n\t/**\n\t * 定义组装机需要的CPU\n\t */\n\tprivate Cpu cpu = null;\n\t/**\n\t * 定义组装机需要的主板\n\t */\n\tprivate Mainboard mainboard = null;\n\tpublic void makeComputer(AbstractFactory af){\n\t\t/**\n\t\t * 组装机器的基本步骤\n\t\t */\n\t\t//1:首先准备好装机所需要的配件\n\t\tprepareHardwares(af);\n\t\t//2:组装机器\n\t\t//3:测试机器\n\t\t//4：交付客户\n\t}\n\tprivate void prepareHardwares(AbstractFactory af){\n\t\t//这里要去准备CPU和主板的具体实现，为了示例简单，这里只准备这两个\n\t\t//可是，装机工程师并不知道如何去创建，怎么办呢？\n\n\t\t//直接找相应的工厂获取\n\t\tthis.cpu = af.createCpu();\n\t\tthis.mainboard = af.createMainboard();\n\n\t\t//测试配件是否好用\n\t\tthis.cpu.calculate();\n\t\tthis.mainboard.installCPU();\n\t}\n}\n````\n客户端代码：\n\n````java\npublic class Client {\n\tpublic static void main(String[]args){\n\t\t//创建装机工程师对象\n\t\tComputerEngineer cf = new ComputerEngineer();\n\t\t//客户选择并创建需要使用的产品对象\n\t\tAbstractFactory af = new IntelFactory();\n\t\t//告诉装机工程师自己选择的产品，让装机工程师组装电脑\n\t\tcf.makeComputer(af);\n\t}\n}\n````\n　　抽象工厂的功能是为一系列相关对象或相互依赖的对象创建一个接口。一定要注意，这个接口内的方法不是任意堆砌的，而是一系列相关或相互依赖的方法。比如上面例子中的主板和CPU，都是为了组装一台电脑的相关对象。不同的装机方案，代表一种具体的电脑系列。\n　　\n![](https://raw.githubusercontent.com/chiyuanbo/pic/master/v2-9aca10e677ee2a6a8e276a09c5020239_720w.jpg)\n\n　　由于抽象工厂定义的一系列对象通常是相关或相互依赖的，这些产品对象就构成了一个产品族，也就是抽象工厂定义了一个产品族。\n\n　　这就带来非常大的灵活性，切换产品族的时候，只要提供不同的抽象工厂实现就可以了，也就是说现在是以一个产品族作为一个整体被切换。\n\n![](https://raw.githubusercontent.com/chiyuanbo/pic/master/v2-b6622285e54073d04577cb6590587728_720w.jpg)　　\n\n在什么情况下应当使用抽象工厂模式\n　　1.一个系统不应当依赖于产品类实例如何被创建、组合和表达的细节，这对于所有形态的工厂模式都是重要的。\n\n　　2.这个系统的产品有多于一个的产品族，而系统只消费其中某一族的产品。\n\n　　3.同属于同一个产品族的产品是在一起使用的，这一约束必须在系统的设计中体现出来。（比如：Intel主板必须使用Intel CPU、Intel芯片组）\n\n　　4.系统提供一个产品类的库，所有的产品以同样的接口出现，从而使客户端不依赖于实现。\n\n抽象工厂模式的起源\n　　抽象工厂模式的起源或者最早的应用，是用于创建分属于不同操作系统的视窗构建。比如：命令按键（Button）与文字框（Text)都是视窗构建，在UNIX操作系统的视窗环境和Windows操作系统的视窗环境中，这两个构建有不同的本地实现，它们的细节有所不同。\n\n　　在每一个操作系统中，都有一个视窗构建组成的构建家族。在这里就是Button和Text组成的产品族。而每一个视窗构件都构成自己的等级结构，由一个抽象角色给出抽象的功能描述，而由具体子类给出不同操作系统下的具体实现。\n  \n![](https://raw.githubusercontent.com/chiyuanbo/pic/master/v2-34d5566a2e756278f8dce113254438cb_720w.jpg)\n　　\n　　可以发现在上面的产品类图中，有两个产品的等级结构，分别是Button等级结构和Text等级结构。同时有两个产品族，也就是UNIX产品族和Windows产品族。UNIX产品族由UNIX Button和UNIX Text产品构成；而Windows产品族由Windows Button和Windows Text产品构成。\n\n![](https://raw.githubusercontent.com/chiyuanbo/pic/master/v2-50c9b2314a4eb4522f30f148cf463961_720w.jpg)\n\n　　系统对产品对象的创建需求由一个工程的等级结构满足，其中有两个具体工程角色，即UnixFactory和WindowsFactory。UnixFactory对象负责创建Unix产品族中的产品，而WindowsFactory对象负责创建Windows产品族中的产品。这就是抽象工厂模式的应用，抽象工厂模式的解决方案如下图：\n\n![](https://raw.githubusercontent.com/chiyuanbo/pic/master/v2-2c3ec6661e752c14f6ab1b4cca18ed30_720w.jpg)　　\n\n　　显然，一个系统只能够在某一个操作系统的视窗环境下运行，而不能同时在不同的操作系统上运行。所以，系统实际上只能消费属于同一个产品族的产品。\n\n　　在现代的应用中，抽象工厂模式的使用范围已经大大扩大了，不再要求系统只能消费某一个产品族了。因此，可以不必理会前面所提到的原始用意。\n\n抽象工厂模式的优点\n分离接口和实现\n　　客户端使用抽象工厂来创建需要的对象，而客户端根本就不知道具体的实现是谁，客户端只是面向产品的接口编程而已。也就是说，客户端从具体的产品实现中解耦。\n\n使切换产品族变得容易\n　　因为一个具体的工厂实现代表的是一个产品族，比如上面例子的从Intel系列到AMD系列只需要切换一下具体工厂。\n\n抽象工厂模式的缺点\n不太容易扩展新的产品\n　　如果需要给整个产品族添加一个新的产品，那么就需要修改抽象工厂，这样就会导致修改所有的工厂实现类。\n","tags":["Java"]},{"title":"J.U.C-原子操作","url":"/2022/10/27/cyb-mds/java/J.U.C/J.U.C-原子操作/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n#### 完整的类库结构\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylbnf163tj217f0kkgp9.jpg)\n\n#### 原子操作 part 1\n\n\n\n> 通常情况下，在Java里面，++i或者--i不是线程安全的，这里面有三个独立的操作：获取变量当前值，为该值+1/-1，然后写回新的值。在没有额外资源可以利用的情况下，只能使用加锁才能保证读-改-写这三个操作时“原子性”的。\n\n```powershell\n一切从java.util.concurrent.atomic.AtomicInteger开始。\n\nint addAndGet(int delta)\n          以原子方式将给定值与当前值相加。 实际上就是等于线程安全版本的i =i+delta操作。参考下图1。\n\nboolean compareAndSet(int expect, int update)\n          如果当前值 == 预期值，则以原子方式将该值设置为给定的更新值。 如果成功就返回true，否则返回false，并且不修改原值。\n\nint decrementAndGet()\n          以原子方式将当前值减 1。 相当于线程安全版本的--i操作。\n\nint get()\n          获取当前值。\n\nint getAndAdd(int delta)\n          以原子方式将给定值与当前值相加，返回值为定值过去值。 相当于线程安全版本的t=i;i+=delta;return t;操作。参考下图1。\n\nint getAndDecrement()\n          以原子方式将当前值减 1。 相当于线程安全版本的i--操作。\n\nint getAndIncrement()\n          以原子方式将当前值加 1。 相当于线程安全版本的i++操作。类似getAndAdd().\n\nint getAndSet(int newValue)\n          以原子方式设置为给定值，并返回旧值。 相当于线程安全版本的t=i;i=newValue;return t;操作。\n\nint incrementAndGet()\n          以原子方式将当前值加 1。 相当于线程安全版本的++i操作。 类似addAndGet().\n          \nvoid lazySet(int newValue)\n          最后设置为给定值。 延时设置变量值，这个等价于set()方法，但是由于字段是volatile类型的，因此次字段的修改会比普通字段（非volatile字段）有稍微的性能延时（尽管可以忽略），所以如果不是想立即读取设置的新值，允许在“后台”修改值，那么此方法就很有用。如果还是难以理解，这里就类似于启动一个后台线程如执行修改新值的任务，原线程就不等待修改结果立即返回（这种解释其实是不正确的，但是可以这么理解）。\n\nvoid set(int newValue)\n          设置为给定值。 直接修改原始值，也就是i=newValue操作。\n\nboolean weakCompareAndSet(int expect, int update)\n          如果当前值 == 预期值，则以原子方式将该设置为给定的更新值。JSR规范中说：以原子方式读取和有条件地写入变量但不 创建任何 happen-before 排序，因此不提供与除 weakCompareAndSet 目标外任何变量以前或后续读取或写入操作有关的任何保证。大意就是说调用weakCompareAndSet时并不能保证不存在happen-before的发生（也就是可能存在指令重排序导致此操作失败）。但是从Java源码来看，其实此方法并没有实现JSR规范的要求，最后效果和compareAndSet是等效的，都调用了unsafe.compareAndSwapInt()完成操作。          \n```\n`图①`\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylbnu66tuj20f10eagm7.jpg)\n\n","tags":["Java","J.U.C"]},{"title":"J.U.C-线程池","url":"/2022/10/27/cyb-mds/java/J.U.C/J.U.C-线程池/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n> 下图描述的是线程池API的一部分。广义上的完整线程池可能还包括Thread/Runnable、Timer/TimerTask等部分。这里只介绍主要的和高级的API以及架构和原理。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylbr8hrjnj20sg0e9tbb.jpg)\n\n大多数并发应用程序是围绕执行任务（Task）进行管理的。所谓任务就是抽象、离散的工作单元（unit of work）。把一个应用程序的工作（work）分离到任务中，可以简化程序的管理；这种分离还在不同事物间划分了自然的分界线，可以方便程序在出现错误时进行恢复；同时这种分离还可以为并行工作提供一个自然的结构，有利于提高程序的并发性。\n\n并发执行任务的一个很重要前提是拆分任务。把一个大的过程或者任务拆分成很多小的工作单元，每一个工作单元可能相关、也可能无关，这些单元在一定程度上可以充分利用CPU的特性并发的执行，从而提高并发性（性能、响应时间、吞吐量等）。\n\n所谓的任务拆分就是确定每一个执行任务（工作单元）的边界。理想情况下独立的工作单元有最大的吞吐量，这些工作单元不依赖于其它工作单元的状态、结果或者其他资源等。因此将任务尽可能的拆分成一个个独立的工作单元有利于提高程序的并发性。\n\n对于有依赖关系以及资源竞争的工作单元就涉及到任务的调度和负载均衡。工作单元的状态、结果或者其他资源等有关联的工作单元就需要有一个总体的调度者来协调资源和执行顺序。同样在有限的资源情况下，大量的任务也需要一个协调各个工作单元的调度者。这就涉及到任务执行的策略问题。\n\n任务的执行策略包括4W3H部分：\n\n- 任务在什么（What）线程中执行\n- 任务以什么（What）顺序执行（FIFO/LIFO/优先级等）\n- 同时有多少个（How Many）任务并发执行\n- 允许有多少个（How Many）个任务进入执行队列\n- 系统过载时选择放弃哪一个（Which）任务，如何（How）通知应用程序这个动作\n- 任务执行的开始、结束应该做什么（What）处理\n\n在后面会详细分写这些策略是如何实现的。我们先来简单回答些如何满足上面的条件。\n\n1. 首先明确一定是在Java里面可以供使用者调用的启动线程类是Thread。因此Runnable或者Timer/TimerTask等都是要依赖Thread来启动的，因此在ThreadPool里面同样也是靠Thread来启动多线程的。\n2. 默认情况下Runnable接口执行完毕后是不能拿到执行结果的，因此在ThreadPool里就定义了一个Callable接口来处理执行结果。\n3. 为了异步阻塞的获取结果，Future可以帮助调用线程获取执行结果。\n4. Executor解决了向线程池提交任务的入口问题，同时ScheduledExecutorService解决了如何进行重复调用任务的问题。\n5. CompletionService解决了如何按照执行完毕的顺序获取结果的问题，这在某些情况下可以提高任务执行的并发，调用线程不必在长时间任务上等待过多时间。\n6. 显然线程的数量是有限的，而且也不宜过多，因此合适的任务队列是必不可少的，BlockingQueue的容量正好可以解决此问题。\n7. 固定任务容量就意味着在容量满了以后需要一定的策略来处理过多的任务（新任务），RejectedExecutionHandler正好解决此问题。\n8. 一定时间内阻塞就意味着有超时，因此TimeoutException就是为了描述这种现象。TimeUnit是为了描述超时时间方便的一个时间单元枚举类。\n9. 有上述问题就意味了配置一个合适的线程池是很复杂的，因此Executors默认的一些线程池配置可以减少这个操作。\n\n\n\n> Java里面线程池的顶级接口是Executor，但是严格意义上讲Executor并不是一个线程池，而只是一个执行线程的工具。真正的线程池接口是ExecutorService。\n\n> 下面这张图完整描述了线程池的类体系结构\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylbrr2t33j20mj0l90ua.jpg)","tags":["Java","J.U.C"]},{"title":"面试整理","url":"/2022/10/27/cyb-mds/java/code/面试整理/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n#### jvm\n\n[jvm内存模型](https://chiyuanbo.github.io/2018/05/14/Jvm%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/#more)\n\n#### 集合（各类集合区别）\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fyleochi0ij21mh15b10v.jpg)\n\n#### 单例\n\n\n#### 连接池(种类、配置)\n\n\n#### 线程池（j.u.c）\n\n#### synchronized\n\n#### static\n\n#### 常见异常\n\n#### 常用jvm垃圾回收\n\n#### 多线程使用\n\n#### kafka和rabbitMQ区别\n\n#### \n\n#### OOMError\n\n#### SparkSql执行原理\n\n当执行SparkSQL语句的顺序为：\n1.对读入的SQL语句进行解析（Parse），分辨出SQL语句中哪些词是关键词（如SELECT、FROM、WHERE），哪些是表达式、哪些是Projection、哪些是Data Source等，从而判断SQL语句是否规范；\n\n2.将SQL语句和数据库的数据字典（列、表、视图等等）进行绑定（Bind），如果相关的Projection、Data Source等都是存在的话，就表示这个SQL语句是可以执行的；\n\n3.一般的数据库会提供几个执行计划，这些计划一般都有运行统计数据，数据库会在这些计划中选择一个最优计划（Optimize）；\n\n4.计划执行（Execute），按Operation-->Data Source-->Result的次序来进行的，在执行过程有时候甚至不需要读取物理表就可以返回结果，比如重新运行刚运行过的SQL语句，可能直接从数据库的缓冲池中获取返回结果。\n\nsqlContext的运行过程\nsqlContext总的一个过程如下图所示：\n\n1.SQL语句经过SqlParse解析成UnresolvedLogicalPlan；\n\n2.使用analyzer结合数据数据字典（catalog）进行绑定，生成resolvedLogicalPlan；\n\n3.使用optimizer对resolvedLogicalPlan进行优化，生成optimizedLogicalPlan；\n\n4.使用SparkPlan将LogicalPlan转换成PhysicalPlan；\n\n5.使用prepareForExecution()将PhysicalPlan转换成可执行物理计划；\n\n6.使用execute()执行可执行物理计划；\n\n7.生成SchemaRDD。\n\n#### mr\n\n#### 问题\n\n```\nHR：\n\t五险一金基数\n\t多少薪\n\t上班时间\n\t\n\t\nDEV：\n\t开发模式\n\t主负责业务\n```","tags":["Code","Java"]},{"title":"Java8新特性","url":"/2022/10/27/cyb-mds/java/code/Java8新特性/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n## Java语言的新特性\n\n### Lambda表达式和函数式接口\n\nLambda表达式（也叫做闭包）是Java 8中最大的也是期待已久的变化。它允许我们将一个函数当作方法的参数（传递函数），或者说把代码当作数据，这是每个函数式编程者熟悉的概念。很多基于JVM平台的语言一开始就支持Lambda表达式，但是Java程序员没有选择，只能使用匿名内部类来替代Lambda表达式。\n\nLambda表达式的设计被讨论了很久，而且花费了很多的功夫来交流。不过最后取得了一个折中的办法，得到了一个新的简明并且紧凑的Lambda表达式结构。最简单的Lambda表达式可以用逗号分隔的参数列表、->符号和功能语句块来表示。示例如下：\n\n`Arrays.asList( \"a\", \"b\", \"d\" ).forEach( e -> System.out.println( e ) );`\n\n请注意到编译器会根据上下文来推测参数的类型，或者你也可以显示地指定参数类型，只需要将类型包在括号里。举个例子：\n\n`Arrays.asList( \"a\", \"b\", \"d\" ).forEach( ( String e ) -> System.out.println( e ) );`\n\n如果Lambda的功能语句块太复杂，我们可以用大括号包起来，跟普通的Java方法一样，如下：\n````java\nString separator = \",\";\nArrays.asList( \"a\", \"b\", \"d\" ).forEach(\n    ( String e ) -> System.out.print( e + separator ) );\n````\nLambda表达式可能会引用类的成员或者局部变量（会被隐式地转变成final类型），下面两种写法的效果是一样的：\n````java\nString separator = \",\";\nArrays.asList( \"a\", \"b\", \"d\" ).forEach(\n    ( String e ) -> System.out.print( e + separator ) );\n\n和\n\nfinal String separator = \",\";\nArrays.asList( \"a\", \"b\", \"d\" ).forEach(\n    ( String e ) -> System.out.print( e + separator ) );\n````\nLambda表达式可能会有返回值，编译器会根据上下文推断返回值的类型。如果lambda的语句块只有一行，不需要return关键字。下面两个写法是等价的：\n````java\nArrays.asList( \"a\", \"b\", \"d\" ).sort( ( e1, e2 ) -> e1.compareTo( e2 ) );\n\n和\n\nArrays.asList( \"a\", \"b\", \"d\" ).sort( ( e1, e2 ) -> {\n    int result = e1.compareTo( e2 );\n    return result;\n} )\n````\n语言的设计者们思考了很多如何让现有的功能和lambda表达式友好兼容。于是就有了函数接口这个概念。函数接口是一种只有一个方法的接口，像这样地，函数接口可以隐式地转换成lambda表达式。\n\n`java.lang.Runnable 和java.util.concurrent.Callable`是函数接口两个最好的例子。但是在实践中，函数接口是非常脆弱的，只要有人在接口里添加多一个方法，那么这个接口就不是函数接口了，就会导致编译失败。Java 8提供了一个特殊的注解@FunctionalInterface来克服上面提到的脆弱性并且显示地表明函数接口的目的（java里所有现存的接口都已经加上了@FunctionalInterface）。让我们看看一个简单的函数接口定义：\n\n````java\n@FunctionalInterface\npublic interface Functional {\n    void method();\n}\n````\n我们要记住默认的方法和静态方法（下一节会具体解释）不会违反函数接口的约定，例子如下：\n\n````java\n@FunctionalInterface\npublic interface FunctionalDefaultMethods {\n    void method();\n\n    default void defaultMethod() {\n    }\n}\n````\n\n支持Lambda是Java 8最大的卖点，他有巨大的潜力吸引越来越多的开发人员转到这个开发平台来，并且在纯Java里提供最新的函数式编程的概念。对于更多的细节，请参考[官方文档](http://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html)。\n\n### 接口的默认方法和静态方法\n\nJava 8增加了两个新的概念在接口声明的时候：默认和静态方法。默认方法和Trait有些类似，但是目标不一样。默认方法允许我们在接口里添加新的方法，而不会破坏实现这个接口的已有类的兼容性，也就是说不会强迫实现接口的类实现默认方法。\n\n默认方法和抽象方法的区别是抽象方法必须要被实现，默认方法不是。作为替代方式，接口可以提供一个默认的方法实现，所有这个接口的实现类都会通过继承得倒这个方法（如果有需要也可以重写这个方法），让我们来看看下面的例子：\n````java\nprivate interface Defaulable {\n    // Interfaces now allow default methods, the implementer may or\n    // may not implement (override) them.\n    default String notRequired() {\n        return \"Default implementation\";\n    }\n}\n\nprivate static class DefaultableImpl implements Defaulable {\n}\n\nprivate static class OverridableImpl implements Defaulable {\n    @Override\n    public String notRequired() {\n        return \"Overridden implementation\";\n    }\n}\n````\n\n接口Defaulable使用default关键字声明了一个默认方法notRequired()，类DefaultableImpl实现了Defaulable接口，没有对默认方法做任何修改。另外一个类OverridableImpl重写类默认实现，提供了自己的实现方法。\n\nJava 8 的另外一个有意思的新特性是接口里可以声明静态方法，并且可以实现。例子如下：\n````java\nprivate interface DefaulableFactory {\n    // Interfaces now allow static methods\n    static Defaulable create( Supplier< Defaulable > supplier ) {\n        return supplier.get();\n    }\n}\n````\n\n下面是把接口的静态方法和默认方法放在一起的示例（::new 是构造方法引用，后面会有详细描述）：\n````java\npublic static void main( String[] args ) {\n    Defaulable defaulable = DefaulableFactory.create( DefaultableImpl::new );\n    System.out.println( defaulable.notRequired() );\n\n    defaulable = DefaulableFactory.create( OverridableImpl::new );\n    System.out.println( defaulable.notRequired() );\n}\n\n控制台的输出如下：\n\nDefault implementation\nOverridden implementation\n````\n\nJVM平台的接口的默认方法实现是很高效的，并且方法调用的字节码指令支持默认方法。默认方法使已经存在的接口可以修改而不会影响编译的过程。java.util.Collection中添加的额外方法就是最好的例子：**stream()**, **parallelStream()**, **forEach()**, **removeIf()**\n\n虽然默认方法很强大，但是使用之前一定要仔细考虑是不是真的需要使用默认方法，因为在层级很复杂的情况下很容易引起模糊不清甚至变异错误。更多的详细信息请参考[官方文档](http://docs.oracle.com/javase/tutorial/java/IandI/defaultmethods.html)。\n\n### 方法引用\n\n方法引用提供了一个很有用的语义来直接访问类或者实例的已经存在的方法或者构造方法。结合Lambda表达式，方法引用使语法结构紧凑简明。不需要复杂的引用。\n\n下面我们用Car 这个类来做示例，Car这个类有不同的方法定义。让我们来看看java 8支持的4种方法引用。\n````java\npublic static class Car {\n    public static Car create( final Supplier< Car > supplier ) {\n        return supplier.get();\n    }              \n\n    public static void collide( final Car car ) {\n        System.out.println( \"Collided \" + car.toString() );\n    }\n\n    public void follow( final Car another ) {\n        System.out.println( \"Following the \" + another.toString() );\n    }\n\n    public void repair() {\n        System.out.println( \"Repaired \" + this.toString() );\n    }\n}\n````\n第一种方法引用是构造方法引用，语法是：Class::new ，对于泛型来说语法是：Class<T >::new，请注意构造方法没有参数:\n````java\nfinal Car car = Car.create( Car::new );\nfinal List< Car > cars = Arrays.asList( car );\n````\n第二种方法引用是静态方法引用，语法是：Class::static_method请注意这个静态方法只支持一个类型为Car的参数。\n\n`cars.forEach( Car::collide );`\n第三种方法引用是类实例的方法引用，语法是：Class::method请注意方法没有参数。\n\ncars.forEach( Car::repair );\n最后一种方法引用是引用特殊类的方法，语法是：instance::method，请注意只接受Car类型的一个参数。\n````java\nfinal Car police = Car.create( Car::new );\ncars.forEach( police::follow );\n````\n运行这些例子我们将会在控制台得到如下信息（Car的实例可能会不一样）： \n\n````java\nCollided com.javacodegeeks.java8.method.references.MethodReferences$Car@7a81197d\nRepaired com.javacodegeeks.java8.method.references.MethodReferences$Car@7a81197d\nFollowing the com.javacodegeeks.java8.method.references.MethodReferences$Car@7a81197d\n````\n\n关于方法引用更多的示例和详细信息，请参考[官方文档](http://docs.oracle.com/javase/tutorial/java/javaOO/methodreferences.html)\n\n### 重复注释\n\n自从Java 5支持注释以来，注释变得特别受欢迎因而被广泛使用。但是有一个限制，同一个地方的不能使用同一个注释超过一次。 Java 8打破了这个规则，引入了重复注释，允许相同注释在声明使用的时候重复使用超过一次。 \n\n重复注释本身需要被@Repeatable注释。实际上，他不是一个语言上的改变，只是编译器层面的改动，技术层面仍然是一样的。让我们来看看例子：\n````java\npackage com.javacodegeeks.java8.repeatable.annotations;\n\nimport java.lang.annotation.ElementType;\nimport java.lang.annotation.Repeatable;\nimport java.lang.annotation.Retention;\nimport java.lang.annotation.RetentionPolicy;\nimport java.lang.annotation.Target;\n\npublic class RepeatingAnnotations {\n    @Target( ElementType.TYPE )\n    @Retention( RetentionPolicy.RUNTIME )\n    public @interface Filters {\n        Filter[] value();\n    }\n\n    @Target( ElementType.TYPE )\n    @Retention( RetentionPolicy.RUNTIME )\n    @Repeatable( Filters.class )\n    public @interface Filter {\n        String value();\n    };\n\n    @Filter( \"filter1\" )\n    @Filter( \"filter2\" )\n    public interface Filterable {\n    }\n\n    public static void main(String[] args) {\n        for( Filter filter: Filterable.class.getAnnotationsByType( Filter.class ) ) {\n            System.out.println( filter.value() );\n        }\n    }\n}\n````\n我们可以看到，注释Filter被@Repeatable( Filters.class )注释。Filters 只是一个容器，它持有Filter, 编译器尽力向程序员隐藏它的存在。通过这样的方式，Filterable接口可以被Filter注释两次。\n\n另外，反射的API提供一个新方法getAnnotationsByType() 来返回重复注释的类型（请注意Filterable.class.getAnnotation( Filters.class )将会返回编译器注入的Filters实例）。\n\n程序的输出将会是这样：\n````\nfilter1\nfilter2\n````\n更多详细信息请参考[官方文档](http://docs.oracle.com/javase/tutorial/java/annotations/repeating.html)。\n\n### 更好的类型推断\n\nJava 8在类型推断方面改进了很多，在很多情况下，编译器可以推断参数的类型，从而保持代码的整洁。让我们看看例子：\n````java\npackage com.javacodegeeks.java8.type.inference;\n\npackage com.javacodegeeks.java8.type.inference;\n\npublic class Value<T> {\n    public static<T> T defaultValue() {\n        return null;\n    }\n\n    public T getOrDefault( T value, T defaultValue ) {\n        return ( value != null ) ? value : defaultValue;\n    }\n}\n\n这里是Value< String >的用法\n\npackage com.javacodegeeks.java8.type.inference;\n\npublic class TypeInference {\n    public static void main(String[] args) {\n        final Value<String> value = new Value<>();\n        value.getOrDefault( \"22\", Value.defaultValue() );\n    }\n}\n````\n参数Value.defaultValue()的类型被编译器推断出来，不需要显式地提供类型。在java 7, 相同的代码不会被编译，需要写成：`Value.< String >defaultValue()`\n\n### 注解的扩展\n\nJava 8扩展了注解可以使用的范围，现在我们几乎可以在所有的地方：局部变量、泛型、超类和接口实现、甚至是方法的Exception声明。一些例子如下：\n````java\npackage com.javacodegeeks.java8.annotations;\n\nimport java.lang.annotation.ElementType;\nimport java.lang.annotation.Retention;\nimport java.lang.annotation.RetentionPolicy;\nimport java.lang.annotation.Target;\nimport java.util.ArrayList;\nimport java.util.Collection;\n\npublic class Annotations {\n    @Retention( RetentionPolicy.RUNTIME )\n    @Target( { ElementType.TYPE_USE, ElementType.TYPE_PARAMETER } )\n    public @interface NonEmpty {\n    }\n\n    public static class Holder< @NonEmpty T > extends @NonEmpty Object {\n        public void method() throws @NonEmpty Exception {\n        }\n    }\n\n    @SuppressWarnings( \"unused\" )\n    public static void main(String[] args) {\n        final Holder< String > holder = new @NonEmpty Holder< String >();\n        @NonEmpty Collection< @NonEmpty String > strings = new ArrayList<>();\n    }\n}\n````\nJava 8 新增加了两个注解的程序元素类型ElementType.TYPE_USE 和ElementType.TYPE_PARAMETER ，这两个新类型描述了可以使用注解的新场合。注解处理API（Annotation Processing API）也做了一些细微的改动，来识别这些新添加的注解类型。\n\n## Java编译器的新特性\n\n### 参数名字\n\n很长时间以来，Java程序员想尽办法把参数名字保存在java字节码里，并且让这些参数名字在运行时可用。Java 8 终于把这个需求加入到了Java语言（使用反射API和Parameter.getName() 方法）和字节码里（使用java编译命令javac的–parameters参数）。\n````java\npackage com.javacodegeeks.java8.parameter.names;\n\nimport java.lang.reflect.Method;\nimport java.lang.reflect.Parameter;\n\npublic class ParameterNames {\npublic static void main(String[] args) throws Exception {\nMethod method = ParameterNames.class.getMethod( \"main\", String[].class );\nfor( final Parameter parameter: method.getParameters() ) {\nSystem.out.println( \"Parameter: \" + parameter.getName() );\n}\n}\n}\n````\n如果你编译这个class的时候没有添加参数–parameters，运行的时候你会得到这个结果：\n\n`Parameter: arg0`\n\n编译的时候添加了–parameters参数的话，运行结果会不一样：\n\n`Parameter: args`\n\n对于有经验的Maven使用者，–parameters参数可以添加到maven-compiler-plugin的配置部分：\n````xml\n<plugin>\n\t<groupId>org.apache.maven.plugins</groupId>\n\t<artifactId>maven-compiler-plugin</artifactId>\n\t<version>3.1</version>\n\t<configuration>\n\t\t<compilerArgument>-parameters</compilerArgument>\n\t\t<source>1.8</source>\n\t\t<target>1.8</target>\n\t</configuration>\n</plugin>\n````\n最新版的Eclipse Kepler SR2 提供了编译设置项，如下图所示：\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylcmkpgl9j20r00lut9p.jpg)\n\n额外的，有一个方便的方法Parameter.isNamePresent() 来验证参数名是不是可用。\n\n## Java  库的新特性\n\nJava 8 新添加了很多类，并且扩展了很多现有的类来更好地支持现代并发、函数式编程、日期\\时间等等。\n\n### Optional\n\n著名的NullPointerException 是引起系统失败最常见的原因。很久以前Google Guava项目引入了Optional作为解决空指针异常的一种方式，不赞成代码被null检查的代码污染，期望程序员写整洁的代码。受Google Guava的鼓励，Optional 现在是Java 8库的一部分。\n\nOptional只是一个容器，它可以保存一些类型的值或者null。它提供很多有用的方法，所以没有理由不显式地检查null。请参照java 8的[文档](http://docs.oracle.com/javase/8/docs/api/java/util/Optional.html)查看详细信息。\n\n让我们看看两个Optional 用法的小例子：一个是允许为空的值，另外一个是不允许为空的值。\n\n````java\nOptional< String > fullName = Optional.ofNullable( null );\nSystem.out.println( \"Full Name is set? \" + fullName.isPresent() );        \nSystem.out.println( \"Full Name: \" + fullName.orElseGet( () -> \"[none]\" ) ); \nSystem.out.println( fullName.map( s -> \"Hey \" + s + \"!\" ).orElse( \"Hey Stranger!\" ) );\n````\n\n如果Optional实例有非空的值，方法 isPresent() 返回true否则返回false。方法orElseGet提供了回退机制，当Optional的值为空时接受一个方法返回默认值。map()方法转化Optional当前的值并且返回一个新的Optional实例。orElse方法和orElseGet类似，但是它不接受一个方法，而是接受一个默认值。上面代码运行结果如下：\n````\nFull Name is set? false\nFull Name: [none]\nHey Stranger!\n````\n让我们大概看看另外一个例子。\n````java\nOptional< String > firstName = Optional.of( \"Tom\" );\nSystem.out.println( \"First Name is set? \" + firstName.isPresent() );        \nSystem.out.println( \"First Name: \" + firstName.orElseGet( () -> \"[none]\" ) ); \nSystem.out.println( firstName.map( s -> \"Hey \" + s + \"!\" ).orElse( \"Hey Stranger!\" ) );\nSystem.out.println();\n输出如下：\n\nFirst Name is set? true\nFirst Name: Tom\nHey Tom!\n````\n更多详细信息请参考[官方文档](http://docs.oracle.com/javase/8/docs/api/java/util/Optional.html)。\n\n### Stream\n\n新增加的Stream API (java.util.stream)引入了在Java里可以工作的函数式编程。这是目前为止对java库最大的一次功能添加，希望程序员通过编写有效、整洁和简明的代码，能够大大提高生产率。\n\nStream API让集合处理简化了很多（我们后面会看到不仅限于Java集合类）。让我们从一个简单的类Task开始来看看Stream的用法。\n\n````java\npublic class Streams {\n\tprivate enum Status {\n\t\tOPEN, CLOSED\n\t};\n\n\tprivate static final class Task {\n\t\tprivate final Status status;\n\t\tprivate final Integer points;\n\n\t\tTask( final Status status, final Integer points ) {\n\t\t\tthis.status = status;\n\t\t\tthis.points = points;\n\t\t}\n\n\t\tpublic Integer getPoints() {\n\t\t\treturn points;\n\t\t}\n\n\t\tpublic Status getStatus() {\n\t\t\treturn status;\n\t\t}\n\n\t\t@Override\n\t\tpublic String toString() {\n\t\t\treturn String.format( \"[%s, %d]\", status, points );\n\t\t}\n\t}\n}\n````\nTask类有一个分数的概念（或者说是伪复杂度），其次是还有一个值可以为OPEN或CLOSED的状态.让我们引入一个Task的小集合作为演示例子：\n\n````java\nfinal Collection< Task > tasks = Arrays.asList(\n    new Task( Status.OPEN, 5 ),\n    new Task( Status.OPEN, 13 ),\n    new Task( Status.CLOSED, 8 ) \n);\n````\n第一个问题是所有的开放的Task的点数是多少？在java 8 之前，通常的做法是用foreach迭代。但是Java8里头我们会用Stream。Stream是多个元素的序列，支持串行和并行操作。\n\n````java\n// Calculate total points of all active tasks using sum()\nfinal long totalPointsOfOpenTasks = tasks\n    .stream()\n    .filter( task -> task.getStatus() == Status.OPEN )\n    .mapToInt( Task::getPoints )\n    .sum();\n\nSystem.out.println( \"Total points: \" + totalPointsOfOpenTasks );\n控制台的输出将会是：\n\nTotal points: 18\n````\n上面代码执行的流程是这样的，首先Task集合会被转化为Stream表示，然后filter操作会过滤掉所有关闭的Task，接下来使用Task::getPoints 方法取得每个Task实例的点数，mapToInt方法会把Task Stream转换成Integer Stream，最后使用Sum方法将所有的点数加起来得到最终的结果。\n\n在我们看下一个例子之前，我们要记住一些关于Stream的说明。Stream操作被分为中间操作和终点操作。\n\n中间操作返回一个新的Stream。这些中间操作是延迟的，执行一个中间操作比如filter实际上不会真的做过滤操作，而是创建一个新的Stream，当这个新的Stream被遍历的时候，它里头会包含有原来Stream里符合过滤条件的元素。\n\n终点操作比如说forEach或者sum会遍历Stream从而产生最终结果或附带结果。终点操作执行完之后，Stream管道就被消费完了，不再可用。在几乎所有的情况下，终点操作都是即时完成对数据的遍历操作。\n\nStream的另外一个价值是Stream创造性地支持并行处理。让我们看看下面这个例子，这个例子把所有task的点数加起来。\n\n````java\n// Calculate total points of all tasks\nfinal double totalPoints = tasks\n   .stream()\n   .parallel()\n   .map( task -> task.getPoints() ) // or map( Task::getPoints ) \n   .reduce( 0, Integer::sum );\n\nSystem.out.println( \"Total points (all tasks): \" + totalPoints );\n这个例子跟上面那个非常像，除了这个例子里使用了parallel()方法       并且计算最终结果的时候使用了reduce方法。\n\n输出如下：\n\nTotal points (all tasks): 26.0\n````\n经常会有这个一个需求：我们需要按照某种准则来对集合中的元素进行分组。Stream也可以处理这样的需求，下面是一个例子：\n\n````java\n// Group tasks by their status\nfinal Map< Status, List< Task > > map = tasks\n    .stream()\n    .collect( Collectors.groupingBy( Task::getStatus ) );\nSystem.out.println( map );\n控制台的输出如下：\n\n{CLOSED=[[CLOSED, 8]], OPEN=[[OPEN, 5], [OPEN, 13]]}\n````\n让我们来计算整个集合中每个task分数（或权重）的平均值来结束task的例子。\n\n````java\n// Calculate the weight of each tasks (as percent of total points) \nfinal Collection< String > result = tasks\n    .stream()                                        // Stream< String >\n    .mapToInt( Task::getPoints )                     // IntStream\n    .asLongStream()                                  // LongStream\n    .mapToDouble( points -> points / totalPoints )   // DoubleStream\n    .boxed()                                         // Stream< Double >\n    .mapToLong( weigth -> ( long )( weigth * 100 ) ) // LongStream\n    .mapToObj( percentage -> percentage + \"%\" )      // Stream< String> \n    .collect( Collectors.toList() );                 // List< String > \n\nSystem.out.println( result );\n控制台输出如下：\n\n[19%, 50%, 30%]\n````\n\n最后，就像前面提到的，Stream API不仅仅处理Java集合框架。像从文本文件中逐行读取数据这样典型的I/O操作也很适合用Stream API来处理。下面用一个例子来应证这一点。\n\n````java\nfinal Path path = new File( filename ).toPath();\ntry( Stream< String > lines = Files.lines( path, StandardCharsets.UTF_8 ) ) {\n    lines.onClose( () -> System.out.println(\"Done!\") ).forEach( System.out::println );\n}\n````\nStream的方法onClose 返回一个等价的有额外句柄的Stream，当Stream的close（）方法被调用的时候这个句柄会被执行。\n\nStream API、Lambda表达式还有接口默认方法和静态方法支持的方法引用，是Java 8对软件开发的现代范式的响应。\n\n### 日期时间API（JSR310）\n\n Java 8引入了新的日期时间API（JSR 310）改进了日期时间的管理。日期和时间管理一直是Java开发人员最痛苦的问题。java.util.Date和后来的java.util.Calendar一点也没有改变这个情况（甚至让人们更加迷茫）。\n\n因为上面这些原因，产生了Joda-Time ，可以替换Java的日期时间API。Joda-Time深刻影响了 Java 8新的日期时间API，Java 8吸收了Joda-Time 的精华。新的java.time包包含了所有关于日期、时间、日期时间、时区、Instant（跟日期类似但精确到纳秒）、duration（持续时间）和时钟操作的类。设计这些API的时候很认真地考虑了这些类的不变性（从java.util.Calendar吸取的痛苦教训）。如果需要修改时间对象，会返回一个新的实例。\n\n让我们看看一些关键的类和用法示例。第一个类是Clock，Clock使用时区来访问当前的instant, date和time。Clock类可以替换 System.currentTimeMillis() 和 TimeZone.getDefault().\n\n````java\n// Get the system clock as UTC offset\nfinal Clock clock = Clock.systemUTC();\nSystem.out.println( clock.instant() );\nSystem.out.println( clock.millis() );\n控制台输出如下：\n\n2014-04-12T15:19:29.282Z\n1397315969360\n````\n\n其他类我们看看LocalTime和LocalDate。LocalDate只保存有ISO-8601日期系统的日期部分，有时区信息，相应地，LocalTime只保存ISO-8601日期系统的时间部分，没有时区信息。LocalDate和LocalTime都可以从Clock对象创建。\n\n````java\n// Get the local date and local time\nfinal LocalDate date = LocalDate.now();\nfinal LocalDate dateFromClock = LocalDate.now( clock );\n\nSystem.out.println( date );\nSystem.out.println( dateFromClock );\n\n// Get the local date and local time\nfinal LocalTime time = LocalTime.now();\nfinal LocalTime timeFromClock = LocalTime.now( clock );\n\nSystem.out.println( time );\nSystem.out.println( timeFromClock );\n控制台输出如下：\n\n2014-04-12\n2014-04-12\n11:25:54.568\n15:25:54.568\n````\n\nLocalDateTime类合并了LocalDate和LocalTime，它保存有ISO-8601日期系统的日期和时间，但是没有时区信息。让我们看一个简单的例子。\n\n````java\n// Get the local date/time\nfinal LocalDateTime datetime = LocalDateTime.now();\nfinal LocalDateTime datetimeFromClock = LocalDateTime.now( clock );\n\nSystem.out.println( datetime );\nSystem.out.println( datetimeFromClock );\n输出如下：\n\n2014-04-12T11:37:52.309\n2014-04-12T15:37:52.309\n````\n\n如果您需要一个类持有日期时间和时区信息，可以使用ZonedDateTime，它保存有ISO-8601日期系统的日期和时间，而且有时区信息。让我们看一些例子：\n\n````java\n// Get the zoned date/time\nfinal ZonedDateTime zonedDatetime = ZonedDateTime.now();\nfinal ZonedDateTime zonedDatetimeFromClock = ZonedDateTime.now( clock );\nfinal ZonedDateTime zonedDatetimeFromZone = ZonedDateTime.now( ZoneId.of( \"America/Los_Angeles\" ) );\n\nSystem.out.println( zonedDatetime );\nSystem.out.println( zonedDatetimeFromClock );\nSystem.out.println( zonedDatetimeFromZone );\n输出如下：\n2014-04-12T11:47:01.017-04:00[America/New_York]\n2014-04-12T15:47:01.017Z\n2014-04-12T08:47:01.017-07:00[America/Los_Angeles]\n````\n\n最后让我们看看Duration类，Duration持有的时间精确到纳秒。它让我们很容易计算两个日期中间的差异。让我们来看一下：\n\n````java\n// Get duration between two dates\nfinal LocalDateTime from = LocalDateTime.of( 2014, Month.APRIL, 16, 0, 0, 0 );\nfinal LocalDateTime to = LocalDateTime.of( 2015, Month.APRIL, 16, 23, 59, 59 );\n\nfinal Duration duration = Duration.between( from, to );\nSystem.out.println( \"Duration in days: \" + duration.toDays() );\nSystem.out.println( \"Duration in hours: \" + duration.toHours() );\n上面的例子计算了两个日期（2014年4月16日和2014年5月16日）之间的持续时间（基于天数和小时）输出如下：\n\nDuration in days: 365\nDuration in hours: 8783\n````\n\n对于Java 8的新日期时间的总体印象还是比较积极的。一部分是因为有经历实战的Joda-Time的基础，还有一部分是因为日期时间终于被认真对待而且听取了开发人员的声音。关于更多的详细信息，请参考[官方文档](http://docs.oracle.com/javase/tutorial/datetime/index.html)。\n\n### Nashorn javascript引擎\n\nJava 8提供了一个新的Nashorn javascript引擎，它允许我们在JVM上运行特定的javascript应用。Nashorn javascript引擎只是javax.script.ScriptEngine另一个实现，而且规则也一样，允许Java和JavaScript互相操作。这里有个小例子：\n\n````java\nScriptEngineManager manager = new ScriptEngineManager();\nScriptEngine engine = manager.getEngineByName( \"JavaScript\" );\n\nSystem.out.println( engine.getClass().getName() );\nSystem.out.println( \"Result:\" + engine.eval( \"function f() { return 1; }; f() + 1;\" ) );\n输出如下：\n\njdk.nashorn.api.scripting.NashornScriptEngine\nResult: 2\n4.5   Base64\n````\n\n对Base64的支持最终成了Java 8标准库的一部分，非常简单易用：\n\n````java\npackage com.javacodegeeks.java8.base64;\n\nimport java.nio.charset.StandardCharsets;\nimport java.util.Base64;\n\npublic class Base64s {\npublic static void main(String[] args) {\nfinal String text = \"Base64 finally in Java 8!\";\n\nfinal String encoded = Base64\n.getEncoder()\n.encodeToString( text.getBytes( StandardCharsets.UTF_8 ) );\nSystem.out.println( encoded );\n\nfinal String decoded = new String(\nBase64.getDecoder().decode( encoded ),\nStandardCharsets.UTF_8 );\nSystem.out.println( decoded );\n}\n}\n控制台输出的编码和解码的字符串\n\nQmFzZTY0IGZpbmFsbHkgaW4gSmF2YSA4IQ==\nBase64 finally in Java 8!\n````\n\n新的Base64API也支持URL和MINE的编码解码。\n\n`(Base64.getUrlEncoder() / Base64.getUrlDecoder(), Base64.getMimeEncoder() / Base64.getMimeDecoder()).`\n\n### 并行数组\n\nJava 8新增加了很多方法支持并行的数组处理。最重要的大概是parallelSort()这个方法显著地使排序在多核计算机上速度加快。下面的小例子演示了这个新的方法（parallelXXX）的行为。\n\n````java\npackage com.javacodegeeks.java8.parallel.arrays;\n\nimport java.util.Arrays;\nimport java.util.concurrent.ThreadLocalRandom;\n\npublic class ParallelArrays {\n    public static void main( String[] args ) {\n        long[] arrayOfLong = new long [ 20000 ];\t\t\n\n        Arrays.parallelSetAll( arrayOfLong,\n            index -> ThreadLocalRandom.current().nextInt( 1000000 ) );\n        Arrays.stream( arrayOfLong ).limit( 10 ).forEach(\n            i -> System.out.print( i + \" \" ) );\n        System.out.println();\n\n        Arrays.parallelSort( arrayOfLong );\n        Arrays.stream( arrayOfLong ).limit( 10 ).forEach(\n            i -> System.out.print( i + \" \" ) );\n        System.out.println();\n    }\n}\n````\n这一小段代码使用parallelSetAll() t方法填充这个长度是2000的数组，然后使用parallelSort() 排序。这个程序输出了排序前和排序后的10个数字来验证数组真的已经被排序了。示例可能的输出如下（请注意这些数字是随机产生的）\n````\nUnsorted: 591217 891976 443951 424479 766825 351964 242997 642839 119108 552378\nSorted: 39 220 263 268 325 607 655 678 723 793\n````\n\n### 并发\n\n在新增Stream机制与lambda的基础之上，在java.util.concurrent.ConcurrentHashMap中加入了一些新方法来支持聚集操作。同时也在java.util.concurrent.ForkJoinPool类中加入了一些新方法来支持共有资源池（common pool）（请查看我们关于Java 并发的免费课程）。\n\n新增的java.util.concurrent.locks.StampedLock类提供一直基于容量的锁，这种锁有三个模型来控制读写操作（它被认为是不太有名的java.util.concurrent.locks.ReadWriteLock类的替代者）。\n\n在java.util.concurrent.atomic包中还增加了下面这些类：\n\n- DoubleAccumulator\n- DoubleAdder\n- LongAccumulator\n- LongAdder\n\n## 新的工具\n\nJava 8 提供了一些新的命令行工具，在这节里我们将会介绍它们中最有趣的部分。\n\n### Nashorn引擎：jjs\n\njjs是个基于Nashorn引擎的命令行工具。它接受一些JavaScript源代码为参数，并且执行这些源代码。例如，我们创建一个具有如下内容的func.js文件：\n\n````java\nfunction f() {\nreturn 1;\n};\n\nprint( f() + 1 );\n我们可以把这个文件作为参数传递给jjs使得这个文件可以在命令行中执行\n\njjs func.js\n输出结果如下\n\n2\n````\n更多的详细信息请参考[官方文档](http://docs.oracle.com/javase/8/docs/technotes/tools/unix/jjs.html)。\n\n### 类依赖分析工具：jdeps\n\nJdeps是一个功能强大的命令行工具，它可以帮我们显示出包层级或者类层级java类文件的依赖关系。它接受class文件、目录、jar文件作为输入，默认情况下，jdeps会输出到控制台。\n\n作为例子，让我们看看现在很流行的Spring框架的库的依赖关系报告。为了让报告短一些，我们只分析一个jar: org.springframework.core-3.0.5.RELEASE.jar.\n\njdeps org.springframework.core-3.0.5.RELEASE.jar 这个命令输出内容很多，我们只看其中的一部分，这些依赖关系根绝包来分组，如果依赖关系在classpath里找不到，就会显示not found.\n\n````java\norg.springframework.core-3.0.5.RELEASE.jar -> C:\\Program Files\\Java\\jdk1.8.0\\jre\\lib\\rt.jar\n   org.springframework.core (org.springframework.core-3.0.5.RELEASE.jar)\n      -> java.io\n      -> java.lang\n      -> java.lang.annotation\n      -> java.lang.ref\n      -> java.lang.reflect\n      -> java.util\n      -> java.util.concurrent\n      -> org.apache.commons.logging                         not found\n      -> org.springframework.asm                            not found\n      -> org.springframework.asm.commons                    not found\n   org.springframework.core.annotation (org.springframework.core-3.0.5.RELEASE.jar)\n      -> java.lang\n      -> java.lang.annotation\n      -> java.lang.reflect\n      -> java.util\n````\t  \n更多的详细信息请参考[官方文档](http://docs.oracle.com/javase/8/docs/technotes/tools/unix/jdeps.html)。\n\n## JVM的新特性\n\nJVM内存永久区已经被metaspace替换（JEP 122）。JVM参数 -XX:PermSize 和 –XX:MaxPermSize被XX:MetaSpaceSize 和 -XX:MaxMetaspaceSize代替。\n\t  ","tags":["Code","Java"]},{"title":"Java 集合详解","url":"/2022/10/27/cyb-mds/java/code/Java 集合详解/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n### 集合是什么？\n\n　　Java集合类存放于 java.util 包中，是一个用来存放对象的容器。\n\n**注意：**\n\n1. 集合只能存放对象。比如你存一个 int 型数据 1放入集合中，其实它是自动转换成 Integer 类后存入的，Java中每一种基本类型都有对应的引用类型。\n\n2. 集合存放的是多个对象的引用，对象本身还是放在堆内存中。\n\n3. 集合可以存放不同类型，不限数量的数据类型。\n\n### Java 集合框架图\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylchwv5gyj21mh15b10v.jpg)\n\n上述所有的集合类，除了 map 系列的集合，即左边集合都实现了 Iterator 接口，这是一个用于遍历集合中元素的接口，主要hashNext(),next(),remove()三种方法。它的一个子接口 ListIterator 在它的基础上又添加了三种方法，分别是 add(),previous(),hasPrevious()。也就是说如果实现 Iterator 接口，那么在遍历集合中元素的时候，只能往后遍历，被遍历后的元素不会再被遍历到，通常无序集合实现的都是这个接口，比如HashSet；而那些元素有序的集合，实现的一般都是 LinkedIterator接口，实现这个接口的集合可以双向遍历，既可以通过next()访问下一个元素，又可以通过previous()访问前一个 元素，比如ArrayList。\n\n　　还有一个特点就是抽象类的使用。如果要自己实现一个集合类，去实现那些抽象的接口会非常麻烦，工作量很大。这个时候就可以使用抽象类，这些抽象类中给我们提供了许多\n\n现成的实现，我们只需要根据自己的需求重写一些方法或者添加一些方法就可以实现自己需要的集合类，工作量大大降低。\n\n### 集合详解\n\n\n#### Iterator:\n\n迭代器，它是Java集合的顶层接口（不包括 map 系列的集合，Map接口 是 map 系列集合的顶层接口）\n\n- Object next()：返回迭代器刚越过的元素的引用，返回值是 Object，需要强制转换成自己需要的类型\n\n- boolean hasNext()：判断容器内是否还有可供访问的元素\n\n- void remove()：删除迭代器刚越过的元素\n\n所以除了 map 系列的集合，我们都能通过迭代器来对集合中的元素进行遍历。\n\n注意：我们可以在源码中追溯到集合的顶层接口，比如 Collection 接口，可以看到它继承的是类 Iterable\n\n````java\npublic interface Collection<E> extends Iterable<E> {\n\t//Query Operations\n````\n\n那这就得说明一下 Iterator 和 Iterable 的区别：\n\n **Iterable** ：存在于 java.lang 包中。\n\n````java\npublic interface Iterable<T> {\n\t/**\n\t * Return an iterator over elements of type {@code T}.\n\t * @return an Iterator.\n\t * /\n\t Iterator<T> interator();\n````\n\n我们可以看到，里面封装了 Iterator 接口。所以只要实现了只要实现了Iterable接口的类，就可以使用Iterator迭代器了。\n\n **Iterator** ：存在于 java.util 包中。核心的方法next(),hasnext(),remove()。\n\n 这里我们引用一个Iterator 的实现类 ArrayList 来看一下迭代器的使用：暂时先不管 List 集合是什么，只需要看看迭代器的用法就行了\n\n````java\n//产生一个 List 集合，典型实现为 ArrayList。\nList list = new ArrayList();\n//添加三个元素\nlist.add(\"Tom\");\nlist.add(\"Bob\");\nlist.add(\"Marry\");\n//构造 List 的迭代器\nIterator it = list.iterator();\n//通过迭代器遍历元素\nwhile(it.hasNext()){\n\tObject obj = it.next();\n\tSystem.out.println(obj);\n}\n````\n\n#### Collection:\n\nList 接口和 Set 接口的父接口\n\n````java\npublic interface Collection<E> extends Iterable<E> {\n````\n\n　　看一下 Collection 集合的使用例子：\n  \n````java\n//我们这里将 ArrayList集合作为 Collection 的实现类\nCollection collection = new ArrayList();\n\n//添加元素\ncollection.add(\"Tom\");\ncollection.add(\"Bob\");\n\n//删除指定元素\ncollection.remove(\"Tom\");\n\n//删除所有元素\nCollection c = new ArrayList();\nc.add(\"Bob\");\ncollection.removeAll(c);\n\n//检测是否存在某个元素\ncollection.contains(\"Tom\");\n\n//判断是否为空\ncollection.isEmpty();\n\n//利用增强for循环遍历集合\nfor(Object obj : collection){\n\tSystem.out.println(obj);\n}\n//利用迭代器 Iterator\nIterator iterator = collection.iterator();\nwhile(iterator.hasNext()){\n\tObject obj = iterator.next();\n\tSystem.out.println(obj);\n}\n````\n\n#### List :\n\n有序，可以重复的集合。\n\n````java\npublic interface List<E> extends Collection<E> {\n````\n\n由于 List 接口是继承于 Collection 接口，所以基本的方法如上所示。\n\nList 接口的三个典型实现：\n\n1. List list1 = new ArrayList();\n\n　　　　底层数据结构是数组，查询快，增删慢;线程不安全，效率高\n\n2. List list2 = new Vector();\n\n　　　　底层数据结构是数组，查询快，增删慢;线程安全，效率低,几乎已经淘汰了这个集合\n\n3. List list3 = new LinkedList();\n\n　　　　底层数据结构是链表，查询慢，增删快;线程不安全，效率高\n\n \n\n 怎么记呢？我们可以想象：\n\n　　数组就像身上编了号站成一排的人，要找第10个人很容易，根据人身上的编号很快就能找到。但插入、删除慢，要望某个位置插入或删除一个人时，后面的人身上的编号都要变。当然，加入或删除的人始终末尾的也快。\n\n　　链表就像手牵着手站成一圈的人，要找第10个人不容易，必须从第一个人一个个数过去。但插入、删除快。插入时只要解开两个人的手，并重新牵上新加进来的人的手就可以。删除一样的道理。\n\n除此之外，List 接口遍历还可以使用普通 for 循环进行遍历，指定位置添加元素，替换元素等等。\n\n````java\n//产生一个 List 集合，典型实现为 ArrayList\nList list = new ArrayList();\n//添加三个元素\nlist.add(\"Tom\");\nlist.add(\"Bob\");\nlist.add(\"Marry\");\n//构造 List 的迭代器\nIterator it = list.iterator();\n//通过迭代器遍历元素\nwhile(it.hasNext()){\n\tObject obj = it.next();\n\t//System.out.println(obj);\n}\n\n//在指定地方添加元素\nlist.add(2, 0);\n\n//在指定地方替换元素\nlist.set(2, 1);\n\n//获得指定对象的索引\nint i=list.indexOf(1);\nSystem.out.println(\"索引为：\"+i);\n\n//遍历：普通for循环\nfor(int j=0;j<list.size();j++){\n\t System.out.println(list.get(j));\n}\n ````\n\n#### Set：\n\n典型实现 HashSet()是一个无序，不可重复的集合\n\n**Set hashSet = new HashSet();**\n\n1. HashSet:不能保证元素的顺序；不可重复；不是线程安全的；集合元素可以为 NULL;\n\n2. 其底层其实是一个数组，存在的意义是加快查询速度。我们知道在一般的数组中，元素在数组中的索引位置是随机的，元素的取值和元素的位置之间不存在确定的关系，因此，在数组中查找特定的值时，需要把查找值和一系列的元素进行比较，此时的查询效率依赖于查找过程中比较的次数。而 HashSet 集合底层数组的索引和值有一个确定的关系：index=hash(value),那么只需要调用这个公式，就能快速的找到元素或者索引。\n\n3. 对于 HashSet: 如果两个对象通过 equals() 方法返回 true，这两个对象的 hashCode 值也应该相同。\n\n\t3.1 当向HashSet集合中存入一个元素时，HashSet会先调用该对象的hashCode（）方法来得到该对象的hashCode值，然后根据hashCode值决定该对象在HashSet中的存储位置\n\n\t3.1.1 如果 hashCode 值不同，直接把该元素存储到 hashCode() 指定的位置\n\t3.1.2. 如果 hashCode 值相同，那么会继续判断该元素和集合对象的 equals() 作比较\n\n\t3.2.1 hashCode 相同，equals 为 true，则视为同一个对象，不保存在 hashSet（）中\n\t3.2.2 hashCode 相同，equals 为 false，则存储在之前对象同槽位的链表上，这非常麻烦，我们应该约束这种情况，即保证：如果两个对象通过 equals() 方法返回 true，这两个对象的 hashCode 值也应该相同。\n \n注意：每一个存储到 哈希 表中的对象，都得提供 hashCode() 和 equals() 方法的实现，用来判断是否是同一个对象\n　　　对于 HashSet 集合，我们要保证如果两个对象通过 equals() 方法返回 true，这两个对象的 hashCode 值也应该相同。\n \n　　\n常见的 hashCode()算法：\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylciktirmj20p30akwi1.jpg)\n \n**Set linkedHashSet = new LinkedHashSet();**\n\n　　不可以重复，有序\n　　因为底层采用 链表 和 哈希表的算法。链表保证元素的添加顺序，哈希表保证元素的唯一性\n \n \n**Set treeSet = new TreeSet();**\n\n　　TreeSet:有序；不可重复，底层使用 红黑树算法，擅长于范围查询。\n\n* 如果使用 TreeSet() 无参数的构造器创建一个 TreeSet 对象, 则要求放入其中的元素的类必须实现 Comparable 接口所以, 在其中不能放入 null 元素\n  \n* 必须放入同样类的对象.(默认会进行排序) 否则可能会发生类型转换异常.我们可以使用泛型来进行限制\n\n````java\nSet treeSet = new TreeSet();\ntreeSet.add(1);  //添加一个 Integer 类型的数据\ntreeSet.add(\"a\");   //添加一个 String 类型的数据\nSystem.out.println(treeSet);  //会报类型转换异常的错误\n````\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylcj27u5bj218z04dwev.jpg)\n\n* 自动排序：添加自定义对象的时候，必须要实现 Comparable 接口，并要覆盖 compareTo(Object obj) 方法来自定义比较规则\n\n　　　　如果 this > obj,返回正数 1\n\n　　　　如果 this < obj,返回负数 -1\n\n　　　　如果 this = obj,返回 0 ，则认为这两个对象相等  \n\n两个对象通过 Comparable 接口 compareTo(Object obj) 方法的返回值来比较大小, 并进行升序排列\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylcjmymr3j20gb03e74y.jpg)\n\n* 定制排序: 创建 TreeSet 对象时, 传入 Comparator 接口的实现类. 要求: Comparator 接口的 compare 方法的返回值和 两个元素的 equals() 方法具有一致的返回值  \n* \n ````java\npublic class TreeSetTest {\n    public static void main(String[] args) {\n        Person p1 = new Person(1);\n        Person p2 = new Person(2);\n        Person p3 = new Person(3);\n         \n        Set<Person> set = new TreeSet<>(new Person());\n        set.add(p1);\n        set.add(p2);\n        set.add(p3);\n        System.out.println(set);  //结果为[1, 2, 3]\n    }\n \n}\n \nclass Person implements Comparator<Person>{\n    public int age;\n    public Person(){}\n    public Person(int age){\n        this.age = age;\n    }\n    @Override\n    /***\n     * 根据年龄大小进行排序\n     */\n    public int compare(Person o1, Person o2) {\n        // TODO Auto-generated method stub\n        if(o1.age > o2.age){\n            return 1;\n        }else if(o1.age < o2.age){\n            return -1;\n        }else{\n            return 0;\n        }\n    }\n\n    @Override\n    public String toString() {\n        // TODO Auto-generated method stub\n        return \"\"+this.age;\n    }\n}\n````\n\n \n当需要把一个对象放入 TreeSet 中，重写该对象对应的 equals() 方法时，应保证该方法与 compareTo(Object obj) 方法有一致的结果\n   \n    \n以上三个 Set 接口的实现类比较：\n　　共同点：1、都不允许元素重复\n　　　　　　2、都不是线程安全的类，解决办法：Set set = Collections.synchronizedSet(set 对象)\n \n \n　　不同点：\n　　　　HashSet:不保证元素的添加顺序，底层采用 哈希表算法，查询效率高。判断两个元素是否相等，equals() 方法返回 true,hashCode() 值相等。即要求存入 HashSet 中的元素要覆盖 equals() 方法和 hashCode()方法\n \n　　　　LinkedHashSet:HashSet 的子类，底层采用了 哈希表算法以及 链表算法，既保证了元素的添加顺序，也保证了查询效率。但是整体性能要低于 HashSet　　　　\n \n　　　　TreeSet:不保证元素的添加顺序，但是会对集合中的元素进行排序。底层采用 红-黑 树算法（树结构比较适合范围查询）\n \n \n#### Map：\n\nkey-value 的键值对，key 不允许重复，value 可以\n\n1. 严格来说 Map 并不是一个集合，而是两个集合之间 的映射关系。\n\n2. 这两个集合没每一条数据通过映射关系，我们可以看成是一条数据。即 Entry(key,value）。Map 可以看成是由多个 Entry 组成。\n\n3. 因为 Map 集合即没有实现于 Collection 接口，也没有实现 Iterable 接口，所以不能对 Map 集合进行 for-each 遍历。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylck74iyzj20de0bxtat.jpg)\n\n````java\nMap<String,Object> hashMap = new HashMap<>();\n        //添加元素到 Map 中\n        hashMap.put(\"key1\", \"value1\");\n        hashMap.put(\"key2\", \"value2\");\n        hashMap.put(\"key3\", \"value3\");\n        hashMap.put(\"key4\", \"value4\");\n        hashMap.put(\"key5\", \"value5\");\n         \n        //删除 Map 中的元素,通过 key 的值\n        hashMap.remove(\"key1\");\n         \n        //通过 get(key) 得到 Map 中的value\n        Object str1 = hashMap.get(\"key1\");\n         \n        //可以通过 添加 方法来修改 Map 中的元素\n        hashMap.put(\"key2\", \"修改 key2 的 Value\");\n         \n        //通过 map.values() 方法得到 Map 中的 value 集合\n        Collection<Object> value = hashMap.values();\n        for(Object obj : value){\n            //System.out.println(obj);\n        }\n         \n        //通过 map.keySet() 得到 Map 的key 的集合，然后 通过 get(key) 得到 Value\n        Set<String> set = hashMap.keySet();\n        for(String str : set){\n            Object obj = hashMap.get(str);\n            //System.out.println(str+\"=\"+obj);\n        }\n         \n        //通过 Map.entrySet() 得到 Map 的 Entry集合，然后遍历\n        Set<Map.Entry<String, Object>> entrys = hashMap.entrySet();\n        for(Map.Entry<String, Object> entry: entrys){\n            String key = entry.getKey();\n            Object value2 = entry.getValue();\n            System.out.println(key+\"=\"+value2);\n        }\n         \n        System.out.println(hashMap);\n````\n\n　　Map 的常用实现类：\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylckkl8kkj20gq08rjwz.jpg)\n\n#### Map 和 Set 集合的关系\n\n1. 都有几个类型的集合。HashMap 和 HashSet ，都采 哈希表算法；TreeMap 和 TreeSet 都采用 红-黑树算法；LinkedHashMap 和 LinkedHashSet 都采用 哈希表算法和红-黑树算法。\n\n2. 分析 Set 的底层源码，我们可以看到，Set 集合 就是 由 Map 集合的 Key 组成。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylckvqxh0j20c306oq4p.jpg)\n\n　　　　　　　　\n\n\n","tags":["Code","Java"]},{"title":"Java 异常处理的 9 个最佳实践","url":"/2022/10/27/cyb-mds/java/code/Java 异常处理的 9 个最佳实践/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n在 Java 中，异常处理是个很麻烦的事情。初学者觉得它很难理解，甚至是经验丰富的开发者也要花费很长时间决定异常是要处理掉和抛出。\n\n所以很多开发团队约定一些原则处理异常。如果你是一个团队的新成员，你可能会很惊讶，因为他们约定的规则可能和你以前使用的规则不一样。\n\n不过，有很多最佳实践的规则，被大部分团队接受。这里有 9 大重要的约定，帮助你学习或者改进异常处理。\n------\n\n\n\n#### **1、在 Finally 清理资源或者使用 Try-With-Resource 特性**\n\n大部分情况下，在 try 代码块中使用资源后需要关闭资源，例如 *InputStream 。*在这些情况下，一种常见的失误就是在 try 代码块的最后关闭资源。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylbz0tsm9j216c0f4jtl.jpg)\n\n问题就是，只有没有异常抛出的时候，这段代码才可以正常工作。try 代码块内代码会正常执行，并且资源可以正常关闭。但是，使用 try 代码块是有原因的，一般调用一个或多个可能抛出异常的方法，而且，你自己也可能会抛出一个异常，这意味着代码可能不会执行到 try 代码块的最后部分。结果就是，你并没有关闭资源。\n\n所以，你应该把清理工作的代码放到 finally 里去，或者使用 try-with-resource 特性。\n\n\n\n与前面几行 try 代码块不同，finally 代码块总是会被执行。不管 try 代码块成功执行之后还是你在 catch 代码块中处理完异常后都会执行。因此，你可以确保你清理了所有打开的资源。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylbzlw90aj216a0j4mzo.jpg)\n\n##### **Java 7 的 Try-With-Resource 语法**\n\n另一个可选的方案是 try-with-resource 语法，我在[介绍 Java 的异常处理](https://stackify.com/specify-handle-exceptions-java/#tryWithResource)里更详细的介绍了它。\n\n如果你的资源实现了 [AutoCloseable](https://docs.oracle.com/javase/8/docs/api/java/lang/AutoCloseable.html) 接口，你可以使用这个语法。大多数的 Java 标准资源都继承了这个接口。当你在 try 子句中打开资源，资源会在 try 代码块执行后或异常处理后自动关闭。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylc0ejab0j21680b2q4n.jpg)\n\n#### **2、优先明确异常**\n\n你抛出的异常越明确越好，永远记住，你的同事或者几个月之后的你，将会调用你的方法并且处理异常。\n\n因此需要保证提供给他们尽可能多的信息。这样你的 API 更容易被理解。你的方法的调用者能够更好的处理异常并且避免[额外的检查](https://stackify.com/top-java-software-errors/)。\n\n因此，总是尝试寻找最适合你的异常事件的类，例如，抛出一个 [NumberFormatException](https://docs.oracle.com/javase/8/docs/api/java/lang/NumberFormatException.html) 来替换一个 [IllegalArgumentException](https://docs.oracle.com/javase/8/docs/api/java/lang/IllegalArgumentException.html) 。避免抛出一个不明确的异常。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylc3jxi2bj216a072mxu.jpg)\n\n#### **3、记录指定的异常**\n\n每当你在方法签名中[指定异常](https://stackify.com/specify-handle-exceptions-java/#specify)，你也应该在 [Javadoc 中记录它](http://blog.joda.org/2012/11/javadoc-coding-standards.html)。 这与上一个最佳实践具有相同的目标：尽可能多地向调用者提供信息，以便避免或处理异常。\n\n因此，请确保向 Javadoc 添加 @throws 声明并描述可能导致异常的情况。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylc47hqh4j216a0a2gms.jpg)\n\n#### **4、使用描述性消息抛出异常**\n\n这个最佳实践背后的想法与前两个类似。但这一次，你不会将信息提供给方法的调用者。每个必须了解在日志文件或监视工具中报告异常情况时发生了什么情况的人都可以读取异常消息。\n\n因此，应该尽可能精确地描述问题，并提供最相关的信息来了解异常事件。\n\n不要误会我的意思，你不用去写一段文字。但你也应该在1-2个短句中解释异常的原因。这有助于你的运营团队了解问题的严重性，并且还可以让你更轻松地分析任何服务突发事件。\n\n如果抛出一个特定的异常，它的类名很可能已经描述了这种错误。所以，你不需要提供很多额外的信息。一个很好的例子是 NumberFormatException 。当你以错误的格式提供 String 时，它将被 java.lang.Long 类的构造函数抛出。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylc4xh6vaj216a06274p.jpg)\n\nNumberFormatException 类的名称已经告诉你这种问题。它的消息表示只需要提供导致问题的输入字符串。如果异常类的名称不具有表达性，则需要在消息中提供所需的信息。\n\n```java\n17:17:26,386 ERROR TestExceptionHandling:52 - java.lang.NumberFormatException: For input string: \"xyz\"\n```\n\n#### **5、优先捕获最具体的异常**\n\n大多数 IDE 都可以帮助你实现这个最佳实践。当你尝试首先捕获较不具体的异常时，它们会报告无法访问的代码块。\n\n但问题在于，只有匹配异常的第一个 catch 块会被执行。 因此，如果首先捕获 IllegalArgumentException ，则永远不会到达应该处理更具体的 NumberFormatException 的 catch 块，因为它是 IllegalArgumentException 的子类。\n\n总是优先捕获最具体的异常类，并将不太具体的 catch 块添加到列表的末尾。\n\n你可以在下面的代码片断中看到这样一个 try-catch 语句的例子。 第一个 catch 块处理所有 NumberFormatException 异常，第二个处理所有非 NumberFormatException 异常的  IllegalArgumentException 异常。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylc5sr15sj216c0a4myh.jpg)\n\n#### **6、不要捕获 Throwable 类**\n\n[Throwable](https://docs.oracle.com/javase/8/docs/api/java/lang/Throwable.html)* *是所有异常和错误的超类。你可以在 catch 子句中使用它，但是你永远不应该这样做！\n\n如果在 catch 子句中使用 Throwable ，它不仅会捕获所有异常，也将捕获所有的错误。JVM 抛出错误，指出不应该由应用程序处理的严重问题。 典型的例子是 [OutOfMemoryError](https://docs.oracle.com/javase/8/docs/api/java/lang/OutOfMemoryError.html) 或者 [StackOverflowError](https://docs.oracle.com/javase/8/docs/api/java/lang/StackOverflowError.html) 。 两者都是由应用程序控制之外的情况引起的，无法处理。\n\n所以，最好不要捕获 Throwable ，除非你确定自己处于一种特殊的情况下能够处理错误。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylc68jxw3j216c0843z7.jpg)\n\n\n\n#### **7、不要忽略异常**\n\n你曾经有去分析过一个只执行了你用例的第一部分的 bug 报告吗？\n\n这通常是由于一个被忽略的异常造成的。开发者可能会非常肯定，它永远不会被抛出，并添加一个 catch 块，不做处理或不记录它。而当你发现这个块时，你很可能甚至会发现其中有一个“这永远不会发生”的注释。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylc6o72h2j2168084aav.jpg)\n\n那么，你可能正在分析一个不可能发生的问题。\n\n所以，请不要忽略任何一个异常。 你不知道代码将来如何改变。有人可能会在没有意识到会造成问题的情况下，删除阻止异常事件的验证。或者是抛出异常的代码被改变，现在抛出同一个类的多个异常，而调用的代码并不能阻止所有异常。\n\n你至少应该写一条日志信息，告诉大家这个不可思议的事发生了，而且有人需要检查它。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylc78di7vj21680820tm.jpg)\n\n#### **8、不要记录日志和抛出错误**\n\n这可能是该文章中最常被忽略的最佳实践。 你可以找到很多的其中有一个异常被捕获的代码片段，甚至是一些代码库，被记录和重新抛出。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylc7lo9ryj216a074q3g.jpg)\n\n在发生异常时记录异常可能会感觉很直观，然后重新抛出异常，以便调用者可以适当地处理异常。但它会为同一个异常重复写入多个错误消息。\n\n```java\n17:44:28,945 ERROR TestExceptionHandling:65 - java.lang.NumberFormatException: For input string: \"xyz\"\nException in thread \"main\" java.lang.NumberFormatException: For input string: \"xyz\"\nat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\nat java.lang.Long.parseLong(Long.java:589)\nat java.lang.Long.(Long.java:965)\nat com.stackify.example.TestExceptionHandling.logAndThrowException(TestExceptionHandling.java:63)\nat com.stackify.example.TestExceptionHandling.main(TestExceptionHandling.java:58)\n```\n\n附加消息也不会添加任何信息。正如在最佳实践＃4中所解释的那样，异常消息应该描述异常事件。 堆栈跟踪告诉你在哪个类，方法和行中抛出异常。\n\n如果你需要添加其他信息，则应该捕获异常并将其包装在自定义的信息中。 但请务必遵循最佳实践9。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylc8clc59j216a0843zs.jpg)\n\n所以，只捕获你想处理的异常。 否则，在方法签名中指定它，并让调用者处理它。\n\n\n\n#### **9、封装好的异常类而不使用**\n\n有时候，最好是捕获一个标准异常并将其封装成一定制的异常。一个典型的例子是应用程序或框架特定的业务异常。允许你添加些额外的信息，并且你也可以为你的异常类实现一个特殊的处理。\n\n在你这样做时，请确保将原始异常设置为原因（注：参考下方代码 NumberFormatException e 中的原始异常 e ）。*Exception *类提供了特殊的构造函数方法，它接受一个 *Throwable *作为参数。另外，你将会丢失堆栈跟踪和原始异常的消息，这将会使分析导致异常的异常事件变得困难。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylcgreilcj216a0823zs.jpg)\n\n\n\n#### 总结\n\n如你所见，当你抛出或捕获异常的时候，有很多不同的事情需要考虑，而且大部分事情都是为了改善代码的可读性或者 API 的可用性。\n\n异常通常都是一种异常处理技巧，同时也是一种通信媒介。因此，为了和同事更好的合作，一个团队必须要制定出一个最佳实践和规则，只有这样团队成员才能理解这些通用概念，同时在工作中使用它。","tags":["Code","Java"]},{"title":"Exception","url":"/2022/10/27/cyb-mds/java/code/Exception/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n\n#### Unsupported major.minor version 52.0\nJava虚拟机报错，jdk版本不匹配，右键项目properties\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytivfw95lj20r70jbgqd.jpg)\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytiw5izp1j20jh07smxq.jpg)\n当时jre7是灰色的，是不可用的，show all runtimes后选择jre8版本可以解决，或者等会再打开这个界面，会出现原来的版本jre7\n\n#### 空指针\n今天出了个空指针的错，给String泛型的list添加(0+\"\")竟然给报错，经检查，是声明list的时候没有给它开辟空间，list添加0+\"\"不会报错，会把0添加进去。\n\n#### 连接服务器使用hive连接时FileNotFoundException\n\n    log4j:ERROR setFile(null,true) call failed.\n    java.io.FileNotFoundException: /home/yarn/hadoop-2.6.0/logs/flume-hadoop/flume-hadoop.log (Permission denied)\n        at java.io.FileOutputStream.open(Native Method)\n        at java.io.FileOutputStream.<init>(FileOutputStream.java:221)\n        at java.io.FileOutputStream.<init>(FileOutputStream.java:142)\n        at org.apache.log4j.FileAppender.setFile(FileAppender.java:294)\n        at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)\n        at org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:223)\n        at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)\n        at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172)\n        at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104)\n        at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842)\n        at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)\n        at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:648)\n        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:514)\n        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)\n        at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)\n        at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)\n        at org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:66)\n        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:270)\n        at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:155)\n        at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:132)\n        at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:657)\n        at org.apache.hadoop.util.VersionInfo.<clinit>(VersionInfo.java:37)\n    log4j:ERROR Either File or DatePattern options are not set for appender [File].\n    SLF4J: Class path contains multiple SLF4J bindings.\n    SLF4J: Found binding in [jar:file:/home/yarn/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n这个是因为使用的用户的权限不够\nnoClassFound一般是jar包依赖\n\n#### Java操作phoenix连接需要配置host文件\nurl：C:\\Windows\\System32\\drivers\\etc\n\n连接hivejdbc的时候一个报错\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytizces9rj20y70793zd.jpg)\n一番查找之后发现是phoenix-4.6.0-HBase-1.1-client.jar导致的\n\n#### fat-jar打包\n用fat-jar打成jar包放在服务器上运行，有可能会丢失依赖的jar包导致程序运行出错，或者不报错却查不出数据，使用eclipse本身自带的打jar工具，右键export,选择Java里的runnable JAR file\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytizw6csrj20gu0fa765.jpg)\n改变项目路径的时候，需要把新的路径--build path--use as source folder\nbuild的路径会成为固定路径 比如com.baidu.demo\n如果只有com包就build，.java的package为baidu.demo\n\n上传maven项目时\n\n    -Dmaven.multiModuleProjectDirectory system property is not set. Check $M2_HOME environment variable and mvn script match.\n    .添加M2_HOME的环境变量\n    2.Preference->Java->Installed JREs->Edit 选择一个jdk,\n    添加  -Dmaven.multiModuleProjectDirectory=$M2_HOME\n    \n    -Dmaven.multiModuleProjectDirectory=$M2_HOME\n    \n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytj0j4y3wj20hz0fltag.jpg)\n#### 执行hive查询时 Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask\n\n    java.sql.SQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask\n        at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:296)\n        at org.apache.hive.jdbc.HiveStatement.executeQuery(HiveStatement.java:392)\n        at org.apache.hive.jdbc.HivePreparedStatement.executeQuery(HivePreparedStatement.java:109)\n        at com.sgcc.epri.dcloud.dm_hive_datacheck.query.HiveQuery.queryHiveLoadCount(HiveQuery.java:39)\n        at com.sgcc.epri.dcloud.dm_hive_datacheck.common.ReadExcelAndCompare.readLoad(ReadExcelAndCompare.java:90)\n        at com.sgcc.epri.dcloud.dm_hive_datacheck.main.MainLoad_Forecast.main(MainLoad_Forecast.java:14)\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytj11j1l6j20uh072dgt.jpg)\n这是hdfs没有空间了\n\n#### hive查询聚合函数 找不到COUNT这个列 \n使用count = rs.getString(1);来取 不要用count = rs.getString(\"COUNT\");\n\n    select count(1) as COUNT from fjudm4.hbase_md_load_bus_fc_96lc_submit_balance where date = '2016-01-01'\n    java.sql.SQLException\n            at org.apache.hadoop.hive.jdbc.HiveBaseResultSet.findColumn(HiveBaseResultSet.java:81)\n            at org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getString(HiveBaseResultSet.java:484)\n            at com.sgcc.epri.dcloud.dm_hive_datacheck.query.HiveQuery.queryHiveLoadCount(HiveQuery.java:47)\n            at com.sgcc.epri.dcloud.dm_hive_datacheck.common.ReadExcelAndCompare.readLoad(ReadExcelAndCompare.java:120)\n            at com.sgcc.epri.dcloud.dm_hive_datacheck.main.MainLoad_ForecastCount.main(MainLoad_ForecastCount.java:16)\n            at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n            at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n            at java.lang.reflect.Method.invoke(Method.java:606)\n            at org.eclipse.jdt.internal.jarinjarloader.JarRsrcLoader.main(JarRsrcLoader.java:58)\n#### maven配置ojdbc\n\n因为ojdbc是收费的！(/哭)只能自己下载ojdbc对应的版本jar包，在当前路径打开dos，比如我是在桌面，我执行命令将它安装到本地仓库\n\n    mvn install:install-file -DgroupId=com.Oracle -DartifactId=ojdbc14 -Dversion=10.2.0.2.0 -Dpackaging=jar -Dfile=C:\\Users\\Administrator\\Desktop\\ojdbc14-10.2.0.2.0.jar","tags":["Code","Java"]},{"title":"ClickHouse单机版部署和简介","url":"/2022/10/27/cyb-mds/database/ClickHouse/ClickHouse单机版部署和简介/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n\n\n#### yum在线安装\n\nroot用户下\n\n```shell\n# 关闭selinux\nvim /etc/selinux/config\nSELINUX=disabled\n# 重启\n\nsudo yum install -y yum-utils\nsudo yum-config-manager --add-repo https://packages.clickhouse.com/rpm/clickhouse.repo\nsudo yum install -y clickhouse-server clickhouse-client\n\nsudo /etc/init.d/clickhouse-server start\nclickhouse-client # or \"clickhouse-client --password\" if you set up a password.\n```\n\n```shell\n#注意 cdh会占用9000端口导致ck启动失败 可以改为9002  需要修改好几个地方\nvim /etc/clickhouse-server/config.xml\n\n启动\n/etc/init.d/clickhouse-server start\n\n客户端\nclickhouse-client --port 9002 -m\n```\n\n#### 添加用户\n\n```python\n# 假设密码为xxxxx，利用以下python代码可以得到sha256加密后的字符串：\nimport hashlib\nhashlib.sha256('xxxxx').hexdigest()\n```\n\n修改 users.xml 文件，在< users >与 </ users > 之间加入新创建的用户信息。\n\n```xml\n</users>\n    <default>\n    *****\n    </default>\n\t<pangu>       \n\t\t<password_sha256_hex>f99ab85c305b4dd4a85ebbaf0138a6c17a15cd3c9f5888809655d17ba8674c91</password_sha256_hex>\n\t    <networks incl=\"networks\" replace=\"replace\">\n\t    <ip>::/0</ip>\n\t    </networks>\n\t    <profile>default</profile>\n\t    <quota>default</quota>\n\t    <allow_databases>\n\t    <database>pangu</database>\n\t    <database>slave_db</database>\n\t    <database>system</database>\n\t    </allow_databases>\n\t    <access_management>1</access_management>\n    </pangu>\n</users>\n```\n\n\n\n#### Tabix可视化web\n\n```shell\n停止server\nservice clickhouse-server stop\n修改配置\n解开注释<listen_host>::</listen_host>\n解开注释<http_server_default_response><![CDATA[<html ng-app=\"SMI2\"><head><base href=\"http://ui.tabix.io/\"></head><body><div ui-view=\"\" class=\"content-ui\"></div><script src=\"http://loader.tabix.io/master.js\"></script></body></html>]]></http_server_default_response>\n启动服务\nservice clickhouse-server start\nps -ef|grep click查看进程\n如果进程不存在，查看日志\ncat /var/log/clickhouse-server/clickhouse-server.err.log\n\nweb_url:\nhttp://10.9.1.103:8123/\nuser:default\npwd:空\n```\n\n\n\n#### CK数据类型\n\n！！！数据类型关键字区分大小写\n\n![img](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594faly1h3q9jseldxj21js0y0jyp.jpg)\n\n\n\n#### Spark写入CK\n\n```java\n<dependency>\n    <groupId>com.github.housepower</groupId>\n    <artifactId>clickhouse-native-jdbc</artifactId>\n    <version>2.6.5</version>\n</dependency>\n\npublic static Properties getCkProperties(){\n    Properties properties = new Properties();\n    properties.put(\"driver\", \"com.github.housepower.jdbc.ClickHouseDriver\");\n    properties.put(\"user\", \"default\");\n    properties.put(\"password\", \"\");\n    properties.put(\"batchsize\", \"1000\");\n    properties.put(\"socket_timeout\", \"300000\");\n    properties.put(\"numPartitions\", \"8\");\n    properties.put(\"rewriteBatchedStatements\", \"true\");\n    return properties;\n}\n\npublic static void write2CK(Dataset<Row> resDf, String tableName) {\n    resDf.write().mode(SaveMode.Append).option(JDBCOptions.JDBC_BATCH_FETCH_SIZE(), 1000).jdbc(\"jdbc:clickhouse://10.9.1.103:9002/default\", tableName, getCkProperties());\n}\n```\n\n","tags":["Linux","Clickhouse"]},{"title":"MYSQL实战-1-sql如何执行的","url":"/2022/10/27/cyb-mds/database/sql/mysql/MYSQL实战-1-sql如何执行的/","content":"\n==作者：YB-Chi==\n\n\n\n**mysql基本架构示意图**\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h5s7wyhhf4j21hc1404go.jpg\" alt=\"image\" style=\"zoom: 50%;\" />","tags":["msql"]},{"title":"mysql操作命令","url":"/2022/10/27/cyb-mds/database/mysql操作命令/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n#### 查询位置\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylb9y5x9xj211g0333yo.jpg)\n\n#### 进入mysql\n\n```powershell\n/data1/cupid/database/bin/mysql -utsoc -pfake.PWD.fool.4.U -P3308 -hlocalhost --socket /data1/cupid/databse/data/mysql.sock\n```\n\n#### 导出表\n\n```powershell\n/run/data/cupid/database/bin/mysqldump -utsoc -pfake.PWD.fool.4.U -P3308 -hlocalhost --socket /run/data/cupid/database/data/mysql.sock tsoc zd_windows2017111 > /data/zd_windows2017111.sql\n```\n\n\n\n#### 导入表\n\n```powershell\n/run/data/cupid/database/bin/mysql -utsoc -pfake.PWD.fool.4.U -P3308 -hlocalhost --socket /run/data/cupid/database/data/mysql.sock tsoc < /data/test123.sql\n```\n\n#### 杀死任务\n\n`show processlist;`\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylbat1qp7j211y0gx0ua.jpg)\n\n**找到在执行的sql  id为57**\n\n**`kill 57;`**\n\n#### 导出命令\n\n```shell\nmysqldump -uroot -pJtcmcc@139.com -d omc T_BASE_FTP > /root/T_BASE_FTP.sql\n```\n\n#### 导入\n\n```shell\n# 导入\nmysqldump -uroot -p mysql user > ./user.sql\n\n数据导入报错：Got a packet bigger than‘max_allowed_packet’bytes的问题\n数据导入报错：Got a packet bigger than‘max_allowed_packet’bytes的问题  \n\n2个解决方法：  \n\n1.临时修改：mysql>set global max_allowed_packet=524288000;修改 #512M  \n\n2.修改my.cnf，需重启mysql。 \n\n   在 [MySQLd] 部分添加一句（如果存在，调整其值就可以）：  \n\n   max_allowed_packet=10M\n   \n   \n   ./zkCli.sh -server mdw32:2181,sdw33:2181\n   \n   ./zkCli.sh -server 10.243.228.71:2181,10.243.228.72:2181,10.243.228.73:2181\n```\n\n","tags":["Mysql"]},{"title":"ClickHouse集群版部署","url":"/2022/10/27/cyb-mds/database/ClickHouse/ClickHouse集群版部署/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n#### 部署结构\n\n3节点3实例3分片1副本\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594faly1h6ff6xwhg5j20ti0a8af6.jpg)\n\n#### 元素讲解\n\n##### Shard\n\n和es的概念类似,将一份数据分割为多个shard去存储,\n\n##### Replica\n\n副本这个名词有歧义,官方介绍一个副本就是1份数据,而不是传统的主数据+副本数据的概念.\n\n##### Layer\n\n和es的cluster.name类似,因为ck可以是多主架构的,所以可以根据不同的layer指定不同的集群类别.\n\n如果是一个整体集群,那么所有node填写相同的一个layer.\n\n#### 安装流程\n\n```shell\nsudo yum install -y yum-utils\nsudo yum-config-manager --add-repo https://packages.clickhouse.com/rpm/clickhouse.repo\nsudo yum install -y clickhouse-server clickhouse-client\n\nsudo /etc/init.d/clickhouse-server start\nclickhouse-client # or \"clickhouse-client --password\" if you set up a password.\n```\n\ntabix参照单机版文档随便找个节点开启\n\n修改配置文件\n\n`/etc/clickhouse-server/metrika.xml`\n\n```\n<?xml version=\"1.0\"?>\n<clickhouse>\n\t<clickhouse_remote_servers>\n\t\t<ckcluster_3shard_3replica>\n\t\t\t<shard>\n\t\t\t\t<internal_replication>true</internal_replication>\n\t\t\t\t<replica>\n\t\t\t\t\t<host>node1</host>\n\t\t\t\t\t<port>9000</port>\n\t\t\t\t</replica>\n\t\t\t</shard>\n\t\t\t<shard>\n\t\t\t\t<internal_replication>true</internal_replication>\n\t\t\t\t<replica>\n\t\t\t\t\t<host>node2</host>\n\t\t\t\t\t<port>9000</port>\n\t\t\t\t</replica>\n\t\t\t</shard>\n\t\t\t<shard>\n\t\t\t\t<internal_replication>true</internal_replication>\n\t\t\t\t<replica>\n\t\t\t\t\t<host>node3</host>\n\t\t\t\t\t<port>9000</port>\n\t\t\t\t</replica>\n\t\t\t</shard>\n\t\t</ckcluster_3shard_3replica>\n\t</clickhouse_remote_servers>\n     <macros>\n        <layer>ckcluster_3shard_3replica</layer>\n        <!--根据前面shard的配置，例子中总共3个shard-->\n        <shard>shard01</shard>\n        <!--每个节点配置本地主机名即可，或者唯一的数字id-->\n        <replica>node1</replica>\n    </macros>\n    <networks>\n       <ip>::/0</ip>\n    </networks>\n    <!-- 数据压缩算法  -->\n    <clickhouse_compression>\n        <case>\n            <min_part_size>10000000000</min_part_size>\n            <min_part_size_ratio>0.01</min_part_size_ratio>\n            <method>lz4</method>\n        </case>\n    </clickhouse_compression>\n</clickhouse>\n\n```\n\n其他节点差异化的参数就俩` <shard>shard01</shard>`和` <replica>node1</replica>`\n\n`config.xml`\n\n```shell\n<remote_servers incl=\"clickhouse_remote_servers\" />\n<include_from>/etc/clickhouse-server/metrika.xml</include_from>\n```\n\n后续\n\n```shell\nchown clickhouse:clickhouse /etc/clickhouse-server/metrika.xml\n#启动\nsystemctl start clickhouse-server\n#开机自启\nsystemctl enable clickhouse-server\n```\n\n","tags":["Linux","Clickhouse"]},{"title":"sql","url":"/2022/10/27/cyb-mds/database/sql/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n#### 时间分区表查询出指定日期(精确到天)的数据量\n数据库有一列为\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytj3d43hnj203h066400.jpg)\n\n````java\nString sql = \"select count(*) from \"+libraryName+\".\"+schemaName+\".\" +tableName+\" where to_char(OCCUR_TIME,'yyyy-mm-dd')='\"+dateStr+\"'\";\n````\n#### 小松鼠执行hive sql的时候出现missing EFO near…\n注意sql栏的状态是否上边藏了几个sql，有的话清掉。\n\n#### Hbase的sql时间查询  查询某天的 HAPPENTIME类型为:TIMESTAMP\n\n    select * from FJUDM4.HBASE_MD_OMS_T_COMBINEMONITOR_BUG where HAPPENTIME>=to_date('2016-10-12 00:00:00') and HAPPENTIME<to_date('2016-10-13 00:00:00')\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytj3ueuz5j210v08rt9n.jpg)\n\n#### hive的sql时间分区查询\n````java\nselect * from fjudm4.hbase_md_all_op_ctrl where occur_time >= 2016-11-01 and occur_time < 2016-11-02\n没有加单引号报错\n\nselect * from fjudm4.hbase_md_all_op_ctrl where occur_time = '2016-11-01'\n不可以直接用=\n\nselect * from fjudm4.hbase_md_all_op_ctrl where occur_time >= '2016-11-01' and occur_time < '2016-11-02'\n正确语法\n````","tags":["Sql","Code"]},{"title":"单机Kafka服务器部署","url":"/2022/10/27/cyb-mds/bigdata/Kafka/单机Kafka服务器部署/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n#### 所需组件\n\n```json\nJDK1.8\nZookeeper3.4.6\nkafka_2.11-0.10.0.0\n```\n\n\n\n#### 安装jdk\n\n配置jdk要先卸载centos自带的openjdk 先查看 rpm -qa | grep java 显示如下信息：\n\n```shell\n[root@localhost /]# rpm -qa | grep java\njava-1.8.0-openjdk-headless-1.8.0.65-3.b17.el7.x86_64\njavapackages-tools-3.4.1-11.el7.noarch\njava-1.8.0-openjdk-1.8.0.65-3.b17.el7.x86_64\ntzdata-java-2015g-1.el7.noarch\njava-1.7.0-openjdk-1.7.0.91-2.6.2.3.el7.x86_64\njava-1.7.0-openjdk-headless-1.7.0.91-2.6.2.3.el7.x86_64\npython-javapackages-3.4.1-11.el7.noarch\n```\n\n卸载\n\n```shell\nrpm -e --nodeps java-1.8.0-openjdk-headless-1.8.0.65-3.b17.el7.x86_64\nrpm -e --nodeps java-1.8.0-openjdk-1.8.0.65-3.b17.el7.x86_64\nrpm -e --nodeps java-1.7.0-openjdk-1.7.0.91-2.6.2.3.el7.x86_64\nrpm -e --nodeps java-1.7.0-openjdk-headless-1.7.0.91-2.6.2.3.el7.x86_64\n```\n\n解压jdk在/usr/local并配置环境变量\n\n```powershell\nvim /etc/profile\n在最后添加\nJAVA_HOME=/usr/local/jdk_1.8\nPATH=$PATH:$JAVA_HOME/bin:\nCLASSPATH=.:$JAVA_HOME/lib\nexport JAVA_HOME  PATH CLASSPATH\n更新配置文件\nsource /etc/profile\n验证\njava -version\njavac\n```\n\n\n\n#### 安裝Zookeeper\n\n```powershell\n\n解压tar zxvf zookeeper-3.4.6.tar.gz\n\n重命名mv zookeeper-3.4.6 zookeeper\n\n修改配置文件\ncp zoo_simple.cfg zoo.cfg\nvim zoo.cfg\n修改dataDir\tzk的目录\ndataDir=/usr/local/zookeeper/data\n创建data目录\n\n配置zk的环境变量(参考添加)\nvim /etc/profile\nexport ZK_HOME=/usr/local/zookeeper\nPATH=$PATH:$JAVA_HOME/bin:$ZK_HOME/bin:\n\n更新配置文件\nsource /etc/profile\n\n启动\nzkServer.sh start\nzk使用\n可以使用命令zkCli.sh进入zk客户端\n```\n\n图示成功\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylauero27j20bx020wec.jpg)\n\n\n\n#### 安装Kafka\n\n解压在/usr/local\t\t重命名为kafka方便修改\n\n```powershell\nmkdir -p /var/local/kafka/data/#创建kafka的数据目录\ncd /usr/local/kafka/config #进入配置目录 \n\nvi server.properties #编辑修改相应的参数 \n\nbroker.id=0 #注意此id为kafka服务器的唯一标示  不要与客户kafka 的id重复  只要是整数即可\nport=9092\t\t\t\t\t\t\t\t #这个参数很有意思  即使listeners里有端口号  也必须配置port 否则偶尔会出现问题\nlisteners = PLAINTEXT://192.168.56.212:9092\nlog.dirs=/var/local/kafka/data/ #日志存放路径，上面创建的目录 \nzookeeper.connect=192.168.56.212:2181 #zookeeper地址和端口，单机配置部署，ip:2181 \n\n启动\ncd /usr/local/kafka\nbin/kafka-server-start.sh config/server.properties\n```\n\n#### 脚本\n\n```powershell\ncd /usr/local/kafka\n\nvim kafkastart.sh\n#启动kafka\n/usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server.properties &\n\nvim kafkastop.sh\n#关闭kafka\n/usr/local/kafka/bin/kafka-server-stop.sh /usr/local/kafka/config/server.properties &\n\n#添加脚本执行权限\nchmod +x kafkastart.sh\nchmod +x kafkastop.sh\n```\n\n#### 开启SASL认证(未成功)\n\n- ##### 添加配置文件\n\n```powershell\ncd config/\n#服务端配置\nvim kafka_server_jaas.conf\n\nKafkaServer{\n    org.apache.kafka.common.security.plain.PlainLoginModule required\n    username=\"admin\"\n    password=\"admin\"\n    user_admin=\"admin\"\n    user_alice=\"alice\";\n};#此处的用户和密码和linux账号无任何关系   \n\n#客户端配置\nvim kafka_client_jaas.conf\n\nKafkaClient {\n  org.apache.kafka.common.security.plain.PlainLoginModule required\n  username=\"admin\"\n  password=\"admin\";\n};#必须为服务端配置里存在的账号和密码\n```\n\n- ##### 更改资源文件\n\n```powershell\n#核心文件server.properties\nvim server.properties\n#在修改原有的listeners=PLAINTEXT://192.168.56.212:9092   并在其下加入sasl认证的配置\nlisteners=SASL_PLAINTEXT://192.168.56.212:9092\nsecurity.inter.broker.protocol=SASL_PLAINTEXT\nsasl.mechanism.inter.broker.protocol=PLAIN\nsasl.enabled.mechanisms=PLAIN\nauthorizer.class.name=kafka.security.auth.SimpleAclAuthorizer\nsuper.users=User:admin\n\n#客户端资源文件   用于命令启动sh客户端进行验证使用\nvim producer.properties\n#在最后加入\nsecurity.protocol = SASL_PLAINTEXT\nsasl.mechanism = PLAIN\n\nvim consumer.properties\n#在最后加入\nsecurity.protocol = SASL_PLAINTEXT\nsasl.mechanism = PLAIN\n```\n\n- ##### 添加启动脚本\n\n```powershell\ncd ../bin/\n\nvim kafka-run-class.sh\n#在最后加入KAFKA_SASL_OPTS变量值并在下方引用    注意变量引用的配置文件路径\n\n# Launch mode\nKAFKA_SASL_OPTS='-Djava.security.auth.login.config=/usr/local/kafka/config/kafka_server_jaas.conf'\nif [ \"x$DAEMON_MODE\" = \"xtrue\" ]; then\n  nohup $JAVA $KAFKA_HEAP_OPTS $KAFKA_JVM_PERFORMANCE_OPTS $KAFKA_GC_LOG_OPTS $KAFKA_SASL_OPTS $KAFKA_JMX_OPTS $KAFKA_LOG4J_OPTS -cp $CLASSPATH $KAFKA_OPTS \"$@\" > \"$CONSOLE_OUTPUT_FILE\" 2>&1 < /dev/null &\nelse\n  exec $JAVA $KAFKA_HEAP_OPTS $KAFKA_JVM_PERFORMANCE_OPTS $KAFKA_GC_LOG_OPTS $KAFKA_SASL_OPTS $KAFKA_JMX_OPTS $KAFKA_LOG4J_OPTS -cp $CLASSPATH $KAFKA_OPTS \"$@\"\nfi\n\n#建立sasl认证的生产者客户端\nvim kafka-console-producer-sasl.sh\n\n#!/bin/bash\n#启用sasl验证生产脚本\nexport KAFKA_OPTS=\"-Djava.security.auth.login.config=/usr/local/kafka/config/kafka_client_jaas.conf\"\n/usr/local/kafka/bin/kafka-console-producer.sh \"$@\"\n\n#建立sasl认证的消费者客户端\nvim kafka-console-consumer-sasl.sh\n\n#!/bin/bash\n#启用sasl验证消费脚本\nexport KAFKA_OPTS=\"-Djava.security.auth.login.config=/usr/local/kafka/config/kafka_client_jaas.conf\"\n/usr/local/kafka/bin/kafka-console-consumer.sh --new-consumer \"$@\"\n```\n\n- ##### 验证\n\n```powershell\n使用命令对topic赋予指定用户的读写权限\n启动sasl的客户端进行认证 \n本文不提供命令    参考kafka记录中SHELL标题\n```","tags":["Bigdata","Kafka"]},{"title":"CLOUDERA_MANAGER安装KAFKA","url":"/2022/10/27/cyb-mds/bigdata/Kafka/CLOUDERA_MANAGER安装KAFKA/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n### 版本说明\n\n`CDH 5.5.1`\n\n`KAFKA-2.1.1-0.10.0`\n\n### 准备软件\n\n**下载地址:**\n\n**<u>http://archive.cloudera.com/kafka/parcels/2.1/</u>**\n\n`KAFKA-2.1.1-1.2.1.1.p0.18-el6.parcel.sha1`\n\n`KAFKA-2.1.1-1.2.1.1.p0.18-el6.parcel`\n\n**el6对应centos6系统版本**\n\n### 安装步奏\n\n#### 初步安装\n\n使用ftp工具上传两个文件至CM节点/opt/cloudera/parcel-repo\n\n重命名sha1码文件\n\n`mv KAFKA-2.1.1-1.2.1.1.p0.18-el6.parcel.sha1 KAFKA-2.1.1-1.2.1.1.p0.18-el6.parcel.sha`\n\n点击主机进入界面然后点击parcel\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytjzr45tnj211h0gx762.jpg)\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytk0aadbtj211w0jhdhb.jpg)\n\n点击右上\"检查新Parcel\"\n\n成功后会在左边栏CDH5的下边出现KAFKA（此图是已经激活好的页面）\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytk0qkjckj211w0i7wfb.jpg)\n\n点击KAFKA然后点击右边的配置，配置完成后，点击激活\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytk1apl9qj211w0i4aat.jpg)\n\n回到主界面点击左边栏右上▽添加服务\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytk1tme7mj211w0i4q56.jpg)\n\n选择Kafka\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytk2e8olqj211w0i4q5j.jpg)\n\n选择Kafka Broker(其实就是kafka的实例，一台kafka服务器就是一个broker)\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytk8t1cl9j211w0i4aav.jpg)\n\n勾选主机\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytk7oevltj211w0i475g.jpg)\n\n点击继续进入配置页面\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytk9d4x0pj211w0i40tu.jpg)\n\n然后等待启动完毕即可完成初步安装\n\n#### 更改配置\n\n`管理页面停止zk和kafka服务`\n\n##### Zookeeper\n\n```powershell\n#查看myid\t   myid在zk配置文件$ZK_HOME/conf/zoo.cfg里的dataDir=/var/lib/zookeeper/\ncat /var/lib/zookeeper/myid    \n```\n\n在Cloudera Manager里添加服务添加,然后在zoo.cfg最下方加入\n\n```powershell\nserver.1= 192.168.12.103:2888:3888\nserver.2= 192.168.12.104:2888:3888\nserver.3= 192.168.12.102:2888:3888\n#server.A=B:C:D中的A是一个数字,表示这个是第几号服务器,B是这个服务器的IP地址，C第一个端口用来集群成员的信息交换,表示这个服务器与集群中的leader服务器交换信息的端口，D是在leader挂掉时专门用来进行选举leader所用的端口。      简单来讲    A对应myid\n  \n```\n\n##### Kafka\n\n`$KAFKA_HOME/config/server.properties`\n\n**注意！Cloudera Manager不会自动更改server.properties内的broker.id**\n**查看id**\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fylaorecp8j20la04et8o.jpg)\n\n\n\n```powershell\n#根据查出的id去修改配置文件里server.properties的broker.id\n\n#监视\nhost.name=192.168.12.104\nport=9092\t\t\t\t\t\t\t\t #这个参数很有意思  即使listeners里有端口号  也必须配置port 否则偶尔会出现问题\nlisteners=PLAINTEXT://192.168.12.104:9092#本机ip\n\n\n#接收网络请求的线程数\nnum.network.threads=6\n\n#zk\nzookeeper.connect=192.168.12.103:2181,192.168.12.102:2181,192.168.12.104:2181\n```\n\n```powershell\n#data&log存储位置不用改  cloudera manager会覆盖这些配置\n（1）存放kafka的log文件的位置\n\n\t默认是/var/log/kafka/ \n\t\n（2）存放kafka的data的位置\n\n\t默认是/var/local/kafka/data\n```\n\n更改完成后需要重启机器\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytkds8afsj211w0jijtq.jpg)\n\n启动kafka时请注意启动日志  刚启动的时候回打印启动时kafka的参数  注意zookeeper是否是使用的集群里的zookeeper而不是kafka自带的。如果是使用kafka自带的,可以先停止kafka服务   去kafka目录使用shell启动\n\n```powershell\ncd /opt/cloudera/parcels/KAFKA-2.1.1-1.2.1.1.p0.18/lib/kafka\nbin/kafka-server-start.sh config/server.properties\n```\n\n然后在ctrl+c停止三台机器的kafka服务区cloudera manager页面去启动 再次注意启动日志\n\n### 问题说明\n\n值得一提的是  如果没有下载Kafka的parcel就去添加服务然后去启动，会出现如下的错误\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytkeef985j20wx0g7t9f.jpg)\n\n点击查看日志的话会新开页面提示错误 `[Errno 2] No such file or directory: '/var/log/kafka/server.log'`\n\n点击右上`查看完整日志`的话会新开页面503\n\n\n\n**重新安装kafka需要执行命令删除zk上的topic并删除物理数据并**\n\n\n\n","tags":["Bigdata","CDH","Kafka"]},{"title":"ZK集群部署","url":"/2022/10/27/cyb-mds/bigdata/Zookeeper/ZK集群部署/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n#### 安装包\n\n官网下载,git只有souce code版\n\nhttps://www.apache.org/dyn/closer.lua/zookeeper/zookeeper-3.8.0/apache-zookeeper-3.8.0-bin.tar.gz\n\n#### 部署\n\n1. 所有节点上传解压重命名安装包\n2. 修改所有节点zoo.cfg\n\n```shell\ncp zookeeper/conf/zoo_sample.cfg zoo.cfg\nmkdir -p zookeeper/data\n\nvim zookeeper/conf/zoo.cfg\ndataDir=/home/cyb/zookeeper/data/\nserver.1=192.168.33.128:2888:3888\nserver.2=192.168.33.129:2888:3888\nserver.3=192.168.33.130:2888:3888\n```\n\n3. data目录下生成myid,每个节点不同,如1 2 3\n4. 修改所有节点环境变量\n\n```shell\nvim /etc/profile\n\nJAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.342.b07-1.el7_9.x86_64/jre\nZK_HOME=/home/cyb/zookeeper\nPATH=$PATH:$JAVA_HOME/bin:$ZK_HOME/bin\n\nsource /etc/profile\n```\n\n4. 写集群启动脚本\n5. 启动~     `./start-cluster.sh`\n6. 每个节点执行zkServer.sh status测试是否成功\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594faly1h6fi8icpxmj20iz03fq4y.jpg)\n\n7. 加到systemctl的service中\n\n   `vim /usr/lib/systemd/system/zookeeper.service`\n\n```shell\n[Unit]\n# 服务描述\nDescription=cosmo-bdp zookeeper\n# 在网络服务启动后运行\nAfter=network.target\n\n[Service]\nType=forking\n\n#\nEnvironment=JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.342.b07-1.el7_9.x86_64/jre ZOO_LOG_DIR=/home/cyb/zookeeper/logs\n# 启动命令\nExecStart=/home/cyb/zookeeper/bin/zkServer.sh start\n# 停止命令\nExecStop=/home/cyb/zookeeper/bin/zkServer.sh stop\n# 重载命令\nExecReload=/home/cyb/zookeeper/bin/zkServer.sh restart\n\n# [Install]\nWantedBy=multi-user.target\n```\n\n```\nsystemctl reload zookeeper.service\n​systemctl start zookeeper.service\n​systemctl is-enabled zookeeper.service \n\n​zkCli.sh -server node1:2181,node2:2181,node3:2181\n```\n\n","tags":["Linux","zookeeper"]},{"title":"Hbase数据迁移","url":"/2022/10/27/cyb-mds/bigdata/Hbase/Hbase数据迁移/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n\n|   旧CDH  |  新CDH   |\n| --- | --- |\n|   CDH-5.16.1  |  CDH-5.16.1   |\n|   Hadoop\t2.6.0  |  Hadoop\t2.6.0   |\n|   HBase\t1.2.0  |  HBase\t1.2.0   |\n|   Hive\t1.1.0  |  Hive\t1.1.0  |\n|   greenplum-db-4.3.11.3-rhel5-x86_64  |  greenplum-db-4.3.11.3-rhel5-x86_64   |\n|   JDK1.8  |  JDK1.8   |\n|   CentOS7.6  |  ？   |\n\n\nCDH-hadoop资源池，涉及到版本差异，以及ker认证\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594faly1gj0nfum7nvj204d0m80tp.jpg)\n\n\n#### 因素\n##### 1.kerberos认证\n查看集群是否开启krb\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594faly1gisi0m0lhnj20rj0bdgma.jpg)\n\n这里会有三种可能情况：一是都未开启认证，二是都开启认证，三是一个开了，另一个没开。前两种情况按照正常的认证（或非认证）的配置操作即可，第三种情况则需要在开启了认证的集群上开启ipc.client.fallback-to-simple-auth-allowed 参数，意即在使用Kerberos的方式访问非Kerberos的集群时，系统自动转换成简单认证，否则会出现认证问题。\n\n\n##### 2.新旧集群间通讯\n\n - 可通讯的情况下可以采用hdfs的distcp或hbase提供的ExportSnapshot\n - 不可通讯需要手动操作\n\n##### 3.在线/离线\n\n若业务允许做离线迁移，可以先将该表Disable后再做迁移，然后在新集群上重新clone成新表即可；但若需要在线进行迁移，则需要提前新集群上生成对应的HBase表，开启ACL权限等操作，并让业务开启数据双写，确保两个集群的数据在迁移时刻之后的数据是一致的。\n\n##### 4.表数量\n\n位置/大小/数量\n`hdfs dfs -ls /hbase/data/default`\n`hdfs dfs -du -h /hbase/data/default`\n\n#### 步骤\n\n##### 旧cdh-新cdh\n\n互通|无kerberos|表30+|物理空间~30TB\n方案 采用ExportSnapshot方式 <cp快照然后clone恢复>\n\n###### snapshot\n`hbase shell`\n- 在源集群中创建快照\n`snapshot 'table_name','snapshot_name'`\n\n- copy快照\n  \n使用ExportSnapshot  将快照从源集群导出到目标集群\n`hbase org.apache.hadoop.hbase.snapshot.ExportSnapshot -snapshot <table_name> -copy-from hdfs://<source_cluster>/hbase -copy-to hdfs://<target_cluster>/hbase -mappers 200 -bandwidth 20 -chuser hbase -chgroup hbase`\n\n补充：有些时候执行完任务，在目标集群并不能找到对应快照，再执行一次，会在很短时间内完成。\n\n- 恢复快照\n`clone_snapshot 'snapshot_name', 'namespace_name:table_name'`\n\n- 统计数据\n\n```\nhbase org.apache.hadoop.hbase.mapreduce.RowCounter 'tablename'\n# 后台执行，将打印的信息写入到count_mdt.evt.date_gh这个文件中\nhbase org.apache.hadoop.hbase.mapreduce.RowCounter 'v2.mdt.evt.date_gh' 2>&1 | tee count_mdt.evt.date_gh &\n```\n脚本文件`create_file.sh`,`exportSnapshot.sh`\n\n\n##### cdh-hadoop资源池\n网络通|有kerberos|表30+|物理空间~30TB\n方案 采用export 表到hdfs，然后hdfs到本地，再利用厂商提供的客户端传到hdfs资源池，最后使用厂商客户端hbase import\n\n#创建临时目录\n`hadoop fs -mkdir /hbase/bak`\n#导出表到hdfs\n`hbase org.apache.hadoop.hbase.mapreduce.Export tableName /hbase/bak/tableName`\n#hdfs导出到本地\n`hadoop fs -copyToLocal /hbase/bak/tableName /usr/local/`\n#备份更改/etc/krb5.conf,上传厂商的*.keytab,执行kerberos认证 wyzdbz为厂商提供的用户\n`kinit -kt /home/jianlu/hdp/wyzdbz.keytab wyzdbz`\n#使用厂商客户端传到hdfs\n`/usr/hdp/3.1.0.0-78/hadoop/bin/hadoop fs -copyFromLocal /hadooppath /usr/local/`\n#使用厂商客户端创建放表的目录，执行写好的建表脚本，导入表\n----------------------\n\n\n\n#### 文章参考\n[Hbase3.0中文官方文档](https://hbase.apachecn.org/#/docs/22)\n[Hbase1.2官方文档](http://hbase.apache.org/1.2/book.html#arch.bulk.load)\n[HBase 数据迁移实战](https://zhuanlan.zhihu.com/p/40308014)\n","tags":["Bigdata,Hbase"]},{"title":"HDFS","url":"/2022/10/27/cyb-mds/bigdata/Hadoop/HDFS/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n### HDFS的工作机制\n工作机制的学习主要是为加深对分布式系统的理解，以及增强遇到各种问题时的分析解决能力，形成一定的集群运维能力。\n\n很多不是真正理解hadoop技术体系的人会常常觉得HDFS可用于网盘类应用，但实际并非如此。要想将技术准确用在恰当的地方，必须对技术有深刻的理解。\n\n1. HDFS集群分为两大角色：NameNode、DataNode\n\n2. NameNode负责管理整个文件系统的元数据（元数据就是文件数据块放置在DataNode位置和数量等信息）\n\n3. DataNode 负责管理用户的文件数据块\n\n4. 文件会按照固定的大小（blocksize）切成若干块后分布式存储在若干台datanode上\n\n5. 每一个文件块可以有多个副本，并存放在不同的datanode上\n\n6. Datanode会定期向Namenode汇报自身所保存的文件block信息，而namenode则会负责保持文件的副本数量\n\n7. HDFS的内部工作机制对客户端保持透明，客户端请求访问HDFS都是通过向namenode申请来进行\n\n### HDFS 写数据流程\n\n客户端要向HDFS写数据，首先要跟Namenode通信以确认可以写文件并获得接收文件block的Datanode，然后，客户端按顺序将文件逐个block传递给相应Datanode，并由接收到block的Datanode负责向其他Datanode复制block的副本\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fyl50mqcw9j20ov0fiwgu.jpg)\n\n写数据步骤详解\n\n1. Client向Namenode通信请求上传文件，Namenode检查目标文件是否已存在，父目录是否存在\n\n2. Namenode返回是否可以上传\n\n3. Client请求第一个 block该传输到哪些Datanode服务器上\n\n4. Namenode返回3个(主副本数量)Datanode服务器ABC\n\n5. Client请求3台DataNode中的一台A上传数据（本质上是一个RPC调用，建立pipeline），A收到请求会继续调用B，然后B调用C，将整个pipeline建立完成，逐级返回客户端\n\n6. Client开始往A上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，A收到一个packet就会传给B，B传给C；A每传一个packet会放入一个应答队列等待应答\n\n7. 当一个block传输完成之后，Client再次请求Namenode上传第二个block的服务器。\n\n### HDFS 读数据流程\n\n客户端将要读取的文件路径发送给namenode，namenode获取文件的元信息（主要是block的存放位置信息）返回给客户端，客户端根据返回的信息找到相应datanode逐个获取文件的block并在客户端本地进行数据追加合并从而获得整个文件\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fyl53k503lj20pv0f9tb6.jpg)\n\n读数据步骤详解\n1. 跟namenode通信查询元数据，找到文件块所在的datanode服务器\n\n2. 挑选一台datanode（就近原则，然后随机）服务器，请求建立socket流\n\n3. datanode开始发送数据（从磁盘里面读取数据放入流，以packet为单位来做校验）\n\n4. 客户端以packet为单位接收，现在本地缓存，然后写入目标文件\n\n### NAMENODE工作机制\n\n#### NAMENODE职责\n\n- 负责客户端请求的响应\n\n- 元数据的管理（查询，修改）\n\n#### 元数据管理\nnamenode对数据的管理采用了三种存储形式：\n\n- 内存元数据(NameSystem)\n\n- 磁盘元数据镜像文件\n\n- 数据操作日志文件（可通过日志运算出元数据）\n\n#### 元数据存储机制\n\n1. 内存中有一份完整的元数据(内存meta data)\n\n2. 磁盘有一个“准完整”的元数据镜像（fsimage）文件(在namenode的工作目录中)\n\n3. 用于衔接内存metadata和持久化元数据镜像fsimage之间的操作日志（edits文件）\n\n注：当客户端对hdfs中的文件进行新增或者修改操作，操作记录首先被记入edits日志文件中，当客户端操作成功后，相应的元数据会更新到内存meta.data中\n\n#### 元数据手动查看\n\n可以通过hdfs的一个工具来查看edits中的信息\n````shell\nbin/hdfs oev -i edits -o edits.xml \nbin/hdfs oiv -i fsimage_0000000000000000087 -p XML -o fsimage.xml\n````\n\n#### 元数据的checkpoint\n\n每隔一段时间，会由secondary namenode将namenode上积累的所有edits和一个最新的fsimage下载到本地，并加载到内存进行merge（这个过程称为checkpoint）\n\n#### checkpoint的详细过程\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fyl54ejcsrj20y70erjuz.jpg)\n\n#### checkpoint操作的触发条件配置参数\n````powershell\ndfs.namenode.checkpoint.check.period=60 #检查触发条件是否满足的频率，60秒\ndfs.namenode.checkpoint.dir=file://${hadoop.tmp.dir}/dfs/namesecondary\ndfs.namenode.checkpoint.edits.dir=${dfs.namenode.checkpoint.dir} #以上两个参数做checkpoint操作时，secondary namenode的本地工作目录\ndfs.namenode.checkpoint.max-retries=3 #最大重试次数\ndfs.namenode.checkpoint.period=3600 #两次checkpoint之间的时间间隔3600秒\ndfs.namenode.checkpoint.txns=1000000 #两次checkpoint之间最大的操作记录\n````\n#### checkpoint的附带作用\n\nnamenode和secondary namenode的工作目录存储结构完全相同，所以，当namenode故障退出需要重新恢复时，可以从secondary namenode的工作目录中将fsimage拷贝到namenode的工作目录，以恢复namenode的元数据\n\n### DATANODE的工作机制\n\n#### Datanode工作职责\n\n1. 存储管理用户的文件块数据\n2. 定期向namenode汇报自身所持有的block信息（通过心跳信息上报）（这点很重要，因为，当集群中发生某些block副本失效时，集群如何恢复block初始副本数量的问题）\n\n````xml\n<property>  \n\t <name>dfs.blockreport.intervalMsec</name>\n\t <value>3600000</value> \n\t <description>Determines block reporting interval in milliseconds. </description>   \n</property>\n````\n\n#### Datanode掉线判断时限参数\n\ndatanode进程死亡或者网络故障造成datanode无法与namenode通信，namenode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为:\n\n**`timeout = 2 * heartbeat.recheck.interval + 10 * dfs.heartbeat.interval`**\n\n而默认的heartbeat.recheck.interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。\n需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。所以，举个例子，如果heartbeat.recheck.interval设置为5000（毫秒），dfs.heartbeat.interval设置为3（秒，默认），则总的超时时间为40秒。\n\n````xml\n       <property>  \n           <name>heartbeat.recheck.interval</name>\n           <value>2000</value>  \n       </property>  \n       <property> \n           <name>dfs.heartbeat.interval</name>  \n           <value>1</value>\n       </property>\n````\n#### 观察验证DATANODE功能\n上传一个文件，观察文件的block具体的物理存放情况：\n在每一台datanode机器上的这个目录中能找到文件的切块：\n/home/hadoop/app/hadoop-2.7.3/tmp/dfs/data/current/BP-193442119-192.168.88.3-1432458743457/current/finalized","tags":["Bigdata","Hadoop","HDFS"]},{"title":"Hadoop单机安装","url":"/2022/10/27/cyb-mds/bigdata/Hadoop/Hadoop单机安装/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n### 准备软件\n\n    jdk-7u80-linux-x64.tar.gz,hadoop-2.6.4.tar.gz\n### 安装步奏\n#### 安装jdk\n\n配置jdk要先卸载centos自带的openjdk\n先查看 rpm -qa | grep java\n显示如下信息：\n\n    [root@localhost /]# rpm -qa | grep java\n    java-1.8.0-openjdk-headless-1.8.0.65-3.b17.el7.x86_64\n    javapackages-tools-3.4.1-11.el7.noarch\n    java-1.8.0-openjdk-1.8.0.65-3.b17.el7.x86_64\n    tzdata-java-2015g-1.el7.noarch\n    java-1.7.0-openjdk-1.7.0.91-2.6.2.3.el7.x86_64\n    java-1.7.0-openjdk-headless-1.7.0.91-2.6.2.3.el7.x86_64\n    python-javapackages-3.4.1-11.el7.noarch\n卸载\n\n    rpm -e --nodeps java-1.8.0-openjdk-headless-1.8.0.65-3.b17.el7.x86_64\n    rpm -e --nodeps java-1.8.0-openjdk-1.8.0.65-3.b17.el7.x86_64\n    rpm -e --nodeps java-1.7.0-openjdk-1.7.0.91-2.6.2.3.el7.x86_64\n    rpm -e --nodeps java-1.7.0-openjdk-headless-1.7.0.91-2.6.2.3.el7.x86_64\n解压jdk并配置环境变量\n\n    vim /etc/profile\n    在最后添加\n    JAVA_HOME=/usr/local/jdk_1.7\n    PATH=$PATH:$JAVA_HOME/bin:\n    CLASSPATH=.:$JAVA_HOME/lib\n    export JAVA_HOME  PATH CLASSPATH\n#### 安装SSH、配置SSH无密码登陆\n\n    执行如下命令进行检验：\n    [spark@localhost ~]$ rpm -qa | grep ssh\n    openssh-6.6.1p1-22.el7.x86_64\n    libssh2-1.4.3-10.el7.x86_64\n    openssh-server-6.6.1p1-22.el7.x86_64\n    openssh-clients-6.6.1p1-22.el7.x86_64\n    此时是已经安装了\n若需要安装，则可以通过 yum 进行安装（安装过程中会让你输入 [y/N]，输入 y 即可）：\n\n    sudo yum install openssh-clients\n    sudo yum install openssh-server\n接着执行如下命令测试一下 SSH 是否可用：\n\n    [spark@localhost ~]$ ssh localhost\n    spark@localhost's password: \n    Last login: Tue Feb  7 23:34:34 2017 from 192.168.15.1\n    -bash: : No such file or directory\n但这样登陆是需要每次输入密码的，我们需要配置成SSH无密码登陆比较方便。\n\n首先输入 exit 退出刚才的 ssh，就回到了我们原先的终端窗口，然后利用 ssh-keygen 生成密钥，并将密钥加入到授权中：\n\n    [spark@localhost ~]$ exit# 退出刚才的 ssh localhost\n    logout\n    Connection to localhost closed.\n    \n    [spark@localhost ~]$ cd ~/.ssh/# 若没有该目录，请先执行一次ssh localhost\n    [spark@localhost .ssh]$ ls\n    known_hosts\n    \n    [spark@localhost .ssh]$ ssh-keygen -t rsa# 会有提示，都按回车就可以\n    Generating public/private rsa key pair.\n    Enter file in which to save the key (/home/spark/.ssh/id_rsa):    \n    Enter passphrase (empty for no passphrase): \n    Enter same passphrase again: \n    Your identification has been saved in /home/spark/.ssh/id_rsa.\n    Your public key has been saved in /home/spark/.ssh/id_rsa.pub.\n    The key fingerprint is:\n    23:b2:30:77:8a:33:d8:93:84:c4:3a:57:4b:3e:bb:d4 spark@localhost.localdomain\n    The key's randomart image is:\n    +--[ RSA 2048]----+\n    |                 |\n    |.                |\n    | o  o            |\n    |o. + .           |\n    |+ = * o S        |\n    | * * O . .       |\n    |. B = E          |\n    |   = .           |\n    |    .            |\n    +-----------------+\n    \n    [spark@localhost .ssh]$ cat id_rsa.pub >> authorized_keys# 加入授权\n    [spark@localhost .ssh]$ chmod 600 ./authorized_keys# 修改文件权限\n    在 Linux 系统中，~ 代表的是用户的主文件夹，即 “/home/用户名” 这个目录，如你的用户名为 hadoop，则 ~ 就代表 “/home/hadoop/”\n    [spark@localhost .ssh]$ pwd\n    /home/spark/.ssh\n此时再用 ssh localhost 命令，无需输入密码就可以直接登陆了\n\n    [spark@localhost .ssh]$ ssh localhost\n    Last login: Tue Feb  7 23:35:12 2017 from localhost\n    -bash: : No such file or directory\n    [spark@localhost ~]$ exit\n    logout\n    Connection to localhost closed. \n#### 安装Hadoop\nHadoop 解压后即可使用。输入如下命令来检查 Hadoop 是否可用，成功则会显示 Hadoop 版本信息：\n\n    更改文件夹名方便配置文件\n    mv hadoop-2.6.4 hadoop\n    \n    cd /usr/local/hadoop\n    ./bin/hadoop version\n设置 HADOOP 环境变量    \n\n    vim /etc/profile\n    # Hadoop Environment Variables\n    export HADOOP_HOME=/usr/local/hadoop\n    export HADOOP_INSTALL=$HADOOP_HOME\n    export HADOOP_MAPRED_HOME=$HADOOP_HOME\n    export HADOOP_COMMON_HOME=$HADOOP_HOME\n    export HADOOP_HDFS_HOME=$HADOOP_HOME\n    export YARN_HOME=$HADOOP_HOME\n    export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native\n    export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin    \n执行如下命令使配置生效\n\n    source /etc/profile \n修改配置文件 core-site.xml\n\n    <configuration>\n        <property>\n            <name>hadoop.tmp.dir</name>\n            <value>file:/usr/local/hadoop/tmp</value>\n            <description>Abase for other temporary directories.</description>\n        </property>\n        <property>\n            <name>fs.defaultFS</name>\n            <value>hdfs://localhost:9000</value>\n        </property>\n    </configuration>\n修改配置文件 hdfs-site.xml\n\n    <configuration>\n        <property>\n            <name>dfs.replication</name>\n            <value>1</value>\n        </property>\n        <property>\n            <name>dfs.namenode.name.dir</name>\n            <value>file:/usr/local/hadoop/tmp/dfs/name</value>\n        </property>\n        <property>\n            <name>dfs.datanode.data.dir</name>\n            <value>file:/usr/local/hadoop/tmp/dfs/data</value>\n        </property>\n    </configuration>\n配置完成后，执行 NameNode 的格式化:\n\n    ./bin/hdfs namenode -format\n     成功的话，会看到 “successfully formatted” 和 “Exitting with status 0” 的提示，\n     若为 “Exitting with status 1” 则是出错。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytj700arqj20ie05twf5.jpg)\n\n==接着开启 NaneNode 和 DataNode 守护进程：==\n\n    ./sbin/start-dfs.sh\n    若出现如下 SSH 的提示 “Are you sure you want to continue connecting”，输入 yes 即可。\n    localhost: Error: JAVA_HOME is not set and could not be found.\n    \n    修改/etc/hadoop/hadoop-env.sh中设JAVA_HOME\n    export JAVA_HOME=/usr/local/jdk_1.7 \n启动完成后，可以通过命令 jps 来判断是否成功启动，若成功启动则会列出如下进程: “NameNode”、”DataNode”和SecondaryNameNode（如果 SecondaryNameNode 没有启动，请运行 sbin/stop-dfs.sh 关闭进程，然后再次尝试启动尝试）。如果没有 NameNode 或 DataNode ，那就是配置不成功，请仔细检查之前步骤，或通过查看启动日志排查原因。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytj7jfqnhj20i0062aaj.jpg)\n\n==通过查看启动日志分析启动失败原因==\n有时 Hadoop 无法正确启动，如 NameNode 进程没有顺利启动，这时可以查看启动日志来排查原因，注意几点：\n\n    启动时会提示形如 “dblab: starting namenode, logging to /usr/local/hadoop/logs/hadoop-hadoop-namenode-dblab.out”，其中 dblab 对应你的主机名，但启动的日志信息是记录在 /usr/local/hadoop/logs/hadoop-hadoop-namenode-dblab.log 中，所以应该查看这个后缀为 .log 的文件；\n    每一次的启动日志都是追加在日志文件之后，所以得拉到最后面看，看下记录的时间就知道了。\n    一般出错的提示在最后面，也就是写着 Fatal、Error 或者 Java Exception 的地方。\n    可以在网上搜索一下出错信息，看能否找到一些相关的解决方法。\n上面的单机模式，grep 例子读取的是本地数据，伪分布式读取的则是 HDFS 上的数据。要使用 HDFS，首先需要在 HDFS 中创建用户目录：\n\n    ./bin/hdfs dfs -mkdir -p /user/spark\n\n接着将 ./etc/hadoop 中的 xml 文件作为输入文件复制到分布式文件系统中，即将 /usr/local/hadoop/etc/hadoop 复制到分布式文件系统中的 /user/hadoop/input 中。我们使用的是 spark 用户，并且已创建相应的用户目录 /user/spark ，因此在命令中就可以使用相对路径如 input，其对应的绝对路径就是 /user/spark/input:\n\n    ./bin/hdfs dfs -mkdir /user/spark/input\n    ./bin/hdfs dfs -put ./etc/hadoop/*.xml /user/spark/input\n复制完成后，可以通过如下命令查看 HDFS 中的文件列表：\n\n    ./bin/hdfs dfs -ls /user/spark/input\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytj85hti5j20i303fq30.jpg)\n\n我们也可以将运行结果取回到本地：\n\n    rm -r ./output    # 先删除本地的 output 文件夹（如果存在）\n    ./bin/hdfs dfs -get /user/spark/output ./output     # 将 HDFS 上的 output 文件夹拷贝到本机\n    cat ./output/*\nHadoop 运行程序时，输出目录不能存在，否则会提示错误 “org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://localhost:9000/user/hadoop/output already exists” ，因此若要再次执行，需要执行如下命令删除 output 文件夹:\n​    \n    ./bin/hdfs dfs -rm -r /user/spark/output    # 删除 output 文件夹    \n运行程序时，输出目录不能存在\n\n运行 Hadoop 程序时，为了防止覆盖结果，程序指定的输出目录（如 output）不能存在，否则会提示错误，因此运行前需要先删除输出目录。在实际开发应用程序时，可考虑在程序中加上如下代码，能在每次运行时自动删除输出目录，避免繁琐的命令行操作：    \n​    \n    Configuration conf = new Configuration();\n    Job job = new Job(conf);\n     \n    /* 删除输出目录 */\n    Path outputPath = new Path(args[1]);\n    outputPath.getFileSystem(conf).delete(outputPath, true);        \n==若要关闭 Hadoop，则运行==\n\n    ./sbin/stop-dfs.sh\n#### 启动YARN   \n有的读者可能会疑惑，怎么启动 Hadoop 后，见不到书上所说的 JobTracker 和 TaskTracker，  \n这是因为新版的 Hadoop 使用了新的 MapReduce 框架（MapReduce V2，  \n也称为 YARN，Yet Another Resource Negotiator）。  \nYARN 是从 MapReduce 中分离出来的，负责资源管理与任务调度。  \nYARN 运行于 MapReduce 之上，提供了高可用性、高扩展性，YARN 的更多介绍在此不展开，有兴趣的可查阅相关资料。  \n上述通过 ./sbin/start-dfs.sh 启动 Hadoop，仅仅是启动了 MapReduce 环境，我们可以启动 YARN ，让 YARN 来负责资源管理与任务调度。  \n首先修改配置文件 mapred-site.xml，这边需要先进行重命名：\n\n    mv ./etc/hadoop/mapred-site.xml.template ./etc/hadoop/mapred-site.xml\n然后再进行编辑，vim ./etc/hadoop/mapred-site.xml ： \n\n    <configuration>\n        <property>\n            <name>mapreduce.framework.name</name>\n            <value>yarn</value>\n        </property>\n    </configuration\n接着修改配置文件 yarn-site.xml：    \n\n    vim ./etc/hadoop/yarn-site.xml \n\n    <configuration>\n        <property>\n            <name>yarn.nodemanager.aux-services</name>\n            <value>mapreduce_shuffle</value>\n         </property>\n    </configuration>\n==然后就可以启动 YARN 了（需要先执行过 ./sbin/start-dfs.sh）：==\n\n    ./sbin/start-yarn.sh      $ 启动YARN\n    ./sbin/mr-jobhistory-daemon.sh start historyserver  # 开启历史服务器，才能在Web中查看任务运行情况\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytj8olghmj20if07s756.jpg)\n\n启动 YARN 之后，运行实例的方法还是一样的，仅仅是资源管理方式、任务调度不同。  \n观察日志信息可以发现，不启用 YARN 时，是 “mapred.LocalJobRunner” 在跑任务，  \n启用 YARN 之后，是 “mapred.YARNRunner” 在跑任务。  \n启动 YARN 有个好处是可以通过 Web 界面查看任务的运行情况：http://192.168.15.131:8088/cluster，如下图所示。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytj954uz8j20sh0i1tcg.jpg)\n\n但 YARN 主要是为集群提供更好的资源管理与任务调度，然而这在单机上体现不出价值，反而会使程序跑得稍慢些。因此在单机上是否开启 YARN 就看实际情况了。  \n==不启动 YARN 需重命名 mapred-site.xml==\n\n如果不想启动 YARN，务必把配置文件 mapred-site.xml 重命名，  \n改成 mapred-site.xml.template，需要用时改回来就行。否则在该配置文件存在，  \n而未开启 YARN 的情况下，运行程序会提示 “Retrying connect to server: 0.0.0.0/0.0.0.0:8032” 的错误，  \n这也是为何该配置文件初始文件名为 mapred-site.xml.template。\n\n#### 同样的，关闭 YARN 的脚本如下：\n\n    ./sbin/stop-yarn.sh\n    ./sbin/mr-jobhistory-daemon.sh stop historyserver\n### 配置变量\n\n在前面我们设置 HADOOP 环境变量时，我们已经顺便设置了 PATH 变量\n\n    （即 “export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin”），\n那么以后我们在任意目录中都可以直接通过执行start-dfs.sh 来启动 Hadoop 或者  \n执行 hdfs dfs -ls input 查看 HDFS 文件了，执行 hdfs dfs -ls input 试试看。\n\n### 命令整理\n#### 开启\n##### 开启 NaneNode 和 DataNode 守护进程：\n\n```powershell\n./sbin/start-dfs.sh\n```\n\n##### 启动 YARN（需要先执行过 ./sbin/start-dfs.sh）：\n\n```powershell\n./sbin/start-yarn.sh      $ 启动YARN\n./sbin/mr-jobhistory-daemon.sh start historyserver  # 开启历史服务器，才能在Web中查看任务运行情况\n```\n\n#### 关闭\n##### 关闭 YARN\n\n```powershell\n./sbin/stop-yarn.sh\n./sbin/mr-jobhistory-daemon.sh stop historyserver\n```\n\n##### 关闭 Hadoop\n\n```powershell\n./sbin/stop-dfs.sh\n```","tags":["Bigdata","Hadoop"]},{"title":"Hadoop群集安装（未写完）","url":"/2022/10/27/cyb-mds/bigdata/Hadoop/Hadoop群集安装（未写完）/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n### 流程\n\nHadoop 集群的安装配置大致为如下流程:\n\n1. 选定一台机器作为 Master\n2. 在 Master 节点上配置 hadoop 用户、安装 SSH server、安装 Java 环境\n3. 在 Master 节点上安装 Hadoop，并完成配置\n4. 在其他 Slave 节点上配置 hadoop 用户、安装 SSH server、安装 Java 环境\n5. 将 Master 节点上的 /usr/local/hadoop 目录复制到其他 Slave 节点上\n6. 在 Master 节点上开启 Hadoop\n\n### 准备工作\n\n其他工作见我所写的Hadoop单机部署，此处只讲解ssh无密登陆\n\n这个操作是要让 Master 节点可以无密码 SSH 登陆到各个 Slave 节点上。\n\n首先生成 Master 节点的公匙，在 Master 节点的终端中执行（因为改过主机名，所以还需要删掉原有的再重新生成一次）：\n\n```powershell\ncd ~/.ssh               # 如果没有该目录，先执行一次ssh localhostrm ./id_rsa*            # 删除之前生成的公匙（如果有）ssh-keygen -t rsa       # 一直按回车就可以\n```\n\n让 Master 节点需能无密码 SSH 本机，在 Master 节点上执行：\n\n```powershell\ncat ./id_rsa.pub >> ./authorized_keys\n```\n\n完成后可执行 `ssh localhost`证一下（可能需要输入 yes，成功后执行 `exit` 返回原来的终端）。接着在 Master 节点将上公匙传输到 Slave1 节点：\n\n```powershell\nscp ~/.ssh/id_rsa.pub root@cluster_node1:/root/.ssh\n```\n\nscp 是 secure copy 的简写，用于在 Linux 下进行远程拷贝文件，类似于 cp 命令，不过 cp 只能在本机中拷贝。执行 scp 时会要求输入 Slave1 上 root用户的密码(也可以使用其他用户，比如我的chi用户)，输入完成后会提示传输完毕\n\n\n接着在 Slave1 节点上，将 ssh 公匙加入授权：\n\n```powershell\nmkdir ~/.ssh       # 如果不存在该文件夹需先创建，若已存在则忽略\ncat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keysrm ~/.ssh/id_rsa.pub    # 用完就可以删掉了\n```\n\n如果有其他 Slave 节点，也要执行将 Master 公匙传输到 Slave 节点、在 Slave 节点上加入授权这两步。\n\n这样，在 Master 节点上就可以无密码 SSH 到各个 Slave 节点了，可在 Master 节点上执行如下命令进行检验，如下图所示：\n\n```powershell\nssh cluster_node1\n```\n\n![微信截图_20170824164948.png](https://raw.githubusercontent.com/chiyuanbo/pic/master/599e9351b1fd5.png)\n\n### 配置文件\n\n集群/分布式模式需要修改 /usr/local/hadoop/etc/hadoop 中的5个配置文件，更多设置项可点击查看官方说明，这里仅设置了正常启动所必须的设置项： **slaves、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml** 。\n\n1, 文件 slaves，将作为 DataNode 的主机名写入该文件，每行一个，默认为 localhost，所以在伪分布式配置时，节点即作为 NameNode 也作为 DataNode。分布式配置可以保留 localhost，也可以删掉，让 Master 节点仅作为 NameNode 使用。\n\n本教程让 Master 节点仅作为 NameNode 使用，因此将文件中原来的 localhost 删除，添加如下内容\n\n```powershell\ncluster_node1\ncluster_node2\n```\n\n2,文件 **core-site.xml** 改为下面的配置：\n\n```powershell\n<configuration>\n  <property>\n    <name>hadoop.tmp.dir</name>\n    <value>/usr/local/hadoop2/tmp</value>\n    <description>temp dir</description>\n  </property>\n  <property>\n    <name>fs.defaultFS</name>\n    <value>hdfs://cluster_model:9000</value>\n  </property>\n  <property>\n    <name>io.file.buffer.size</name>\n    <value>131072</value>\n  </property>\n</configuration>\n```\n\n3, 文件 **hdfs-site.xml**，dfs.replication 一般设为 3，但我们只有两个 Slave 节点，所以 dfs.replication 的值还是设为 2：\n\n```powershell\n<configuration> \n    <property>\n        <name>dfs.namenode.secondary.http-address</name>\n        <value>Master:50090</value>\n    </property> \n    <property>\n        <name>dfs.replication</name>\n        <value>2</value>\n    </property>\n    <property> \n        <name>dfs.namenode.name.dir</name>\n        <value>file:/usr/local/hadoop/tmp/dfs/name</value>\n    </property>\n    <property> \n        <name>dfs.datanode.data.dir</name>\n        <value>file:/usr/local/hadoop/tmp/dfs/data</value>\n    </property>\n</configuration>\n```\n\n4, 文件 **mapred-site.xml** （可能需要先重命名，默认文件名为 mapred-site.xml.template），然后配置修改如下：","tags":["Bigdata","Hadoop"]},{"title":"CDH安装手册","url":"/2022/10/27/cyb-mds/bigdata/Hadoop/CDH安装手册基础篇/","content":"\n==作者：YB-Chi==\n\n[TOC]\n\nCDH安装包括两类镜像，分别是Cloudera Manager管理节点镜像和普通镜像。管理节点镜像比普通镜像多CDH，CM安装包，CM只安装在一台机器上即可。\n\n## 安装CM管理节点ISO \n\n下面操作只是针对CM管理结点,其它普通结点不用执行\n\n插入Cloudera Manager管理节点ISO，安装系统时确保只有一块磁盘（**若有多个磁盘一起安装，那么默认的磁盘扩展工具将无法使用，需要人工判断并且扩展磁盘**），但需要确保此磁盘不小于200G。默认操作系统安装在一个磁盘\n\n开机F1进入界面选择cd-rom\n\n进入安装界面，回车即可一步安装完毕。后续是与磁盘和IP这些不确定因素相关的配置\n\n默认IP ：192.168.12.95\n\n网关： 192.168.12.1\n\n用户名：root\n\n密码： venus123\n\n安装完系统后，自动重启，请弹出光盘，以防再次安装。\n\n请先配置IP，后续配置可以通过SSH远程进行操作。\n\n### 修改IP\n\n> 命令ModifyIP修改IP，默认CM IP 192.168.12.95\n>\n> ![25.png](https://raw.githubusercontent.com/chiyuanbo/pic/master/595234ef52c2b.png)\n>\n> 例如：ModifyIP -e eth0 -p 192.168.12.103 -g 192.168.12.1\n>\n> 执行此命令后，需要执行\n>\n> service network restart\n>\n\n### 添加新磁盘\n\n> 若只是一个磁盘安装系统，那么系统会安装在同一磁盘。\n>\n> **此处需要根据不同服务器类型灵活处理  最终目的是挂载分区**\n>\n> 安装后，加入磁盘，`fdisk –l` 查看磁盘状况，确认新加入磁盘个数(总共6块的话加入5块)\n>\n> `cat /proc/partitions查看情况`\n>\n> 一般情况下直接\t`ExtendDisk` \n>\n> 如果有sdb1 和sdb2之类的存在 \n>\n> 使用fdisk sdb格式化\n>\n> ```\n> fdisk /dev/sdb\n> \td\n> \tw\n> ```\n>\n> 查看是否存在以往的物理卷或逻辑卷\n>\n> `lvscan`\n>\n> `pvscan`\n>\n> 如果有\n>\n> `lvremove (卷名)`\n>\n> `pvremove (卷名)`\n>\n> **如果lvremove不好使  使用最后一行提示的命令**\n>\n> 执行命令ExtendDisk \n>\n> ![26.png](https://raw.githubusercontent.com/chiyuanbo/pic/master/595234ef5e79f.png)\n>\n> 会自动挂载格式化磁盘，但是需要我们使用命令挂载到固定的目录。等待挂载好,此过程较慢。添加的磁盘主要是作为数据磁盘，需要用在hdfs文件系统，挂载目录依次为data\\[1到n\\]\n>\n> 先查看磁盘 df –h，确认挂载点。\n>\n> ![27.png](https://raw.githubusercontent.com/chiyuanbo/pic/master/595234ef5f47b.png)\n>\n> [root@BDS /]# MountDisk /data1 /dev/sdb1 \n> [root@BDS /]# MountDisk /data2 /dev/sdc1 \n> [root@BDS /]# MountDisk /data3 /dev/sdd1 \n> [root@BDS /]# MountDisk /data4 /dev/sde1 \n> [root@BDS /]# MountDisk /data5 /dev/sdf1 \n>\n> 其中第一个参数是挂载点，一般为/data1 ,/data2等等\n>\n> 第二个参数是需要挂载的盘符\n>\n> 请确认后再执行这个命令，此命令会固化挂载点，同一磁盘不能同时执行。\n>\n> 后续这个挂载点是需要配置到cloudera manager的配置界面\n\n### 配置主机信息\n\n> 需要配置本地主机信息，也需要配置cluster中其它设备的主机信息，可以在还没有安装其它节点前就配置完成。但必须确保这些信息确定，等确定后同步到cluster上其它主机上,在非CM管理节点上都是从此主机中同步。\n\n-   修改主机名\n\n-   主机名不可以存在中横线外的特殊字符\n\n    ```powershell\n    使用ModifyHostName 参数1（hostname）\n    cd /root/confbin/\n    ./ModifyHostName BDS\n    ```\n\n\n-   添加主机配置信息\n\n    ```powershell\n    使用CnfHostInfo 参数1（IP）\n    参数2（hostname）\n    cd /root/confbin/\n    ./CnfHostInfo 192.168.12.95 BDS\n    此命令主要修改配置文件/etc/hosts内容\n    ```\n\n\n-   配置内置数据库\n\n若是修改主机名称，那么请先***重启设备***后再配置数据库信息，因为在数据库配置中需要用到主机名，若是主机名称不对，会导致安装CDH时连接的数据库主机名称错误。\n\n```\ncd /root/confbin\n./CnfSql\n```\n\n> 此命令是弥补数据库自动安装时配置文件配置内容不全的问题。\n>\n> 执行完成后，***再重启系统（必须）*，**确保数据库重新读取正确信息\n\n### 时间同步配置\n\n![28.png](https://raw.githubusercontent.com/chiyuanbo/pic/master/595234ef740b1.png)\n\n\n这个是配置管理节点的时间同步。\n\n命令为ModifyNtpConf参数为网关，比如192.168.12.1，但其中最后一位改为0\n\nCM的管理节点的时间同步配置完成，并且会自动生成文件/etc/ntp.conf\\_back是各个非管理节点的时间同步配置文件，届时跟主机信息文件/etc/hosts一起同步到各个节点即可。\n\n```powershell\ncd /root/confbin/\n./ModifyNtpConf 192.168.12.0\nservice ntpd restart\n```\n\n## 二、安装普通节点ISO\n\n确保是一个磁盘，以便能够很方便的扩展磁盘，进入安装界面，回车即可一步安装完毕。后续是与磁盘和IP这些不确定因素相关的配置。\n\n默认IP ：192.168.12.95\n\n> 网关： 192.168.12.1\n>\n> 用户名：root\n>\n> 密码： venus123\n\n安装完系统后，请把光盘取回，以防再次安装。\n\n请先配置IP，后续配置可以通过SSH远程进行操作。\n\n### 修改IP\n\n> 命令ModifyIP修改IP，默认IP 192.168.12.94\n\n### 添加新磁盘\n\n> 同CM管理节点\n\n### 修改主机名\n\n-   修改主机名\n\n> 使用ModifyHostName 参数1（hostname）\n>\n> cd /root/confbin/\n>\n> ./ModifyHostName BDS-1\n>\n> (依次排序例:\n>\n> ./ModifyHostName BDS-2\n>\n> )\n\n## 三、信息同步\n\n当cluster中CM管理节点和普通节点系统安装完成后，并且已经进行了上面的配置工作。那么就需要把主机信息和时间信息进行同步。\n\n首先确保CM管理节点上已经配置了所有主机的信息，可以通过命令cat /etc/hosts\n\n![29.png](https://raw.githubusercontent.com/chiyuanbo/pic/master/595234ef72fec.png)\n\n查看是否已经配置全主机信息，以及主机信息是否都正确。\n\n确保一切正常后。进入CM管理节点的ssh连接界面，执行命令\n\nmv /etc/ntp.conf\\_back /root/ntp.conf\n\nls -l /root/ntp.conf\n\nscp /root/ntp.conf root@**BDS-1**:/etc/ \\#copy时间同步信息到节点\n\nscp /etc/hosts root@BDS-1:/etc/ \\#同步主机信息到节点\n\n![30.png](https://raw.githubusercontent.com/chiyuanbo/pic/master/595234ef750a4.png)\n\n同步主机信息到普通节点，重启普通节点系统，确保主机名修改生效\n\n## 四、普通节点时间同步\n\n在CM管理节点通过 ssh BDS-1(BDS-2……) 进入各个节点，执行命令\n\nservice ntpd stop\n\nntpdate BDS\n\nservice ntpd start\n\n## 五、通过CM安装CDH\n\n用firefox 或者 chrome访问网址，若是设备刚重启已经能连上SSH后请耐心等待一会，服务启动需要几分钟。\n\n[*http://CMIP:7180/cmf/login*](http://cmip:7180/cmf/login)\n\n![1.png](https://raw.githubusercontent.com/chiyuanbo/pic/master/59523279b4085.png)\n\n使用帐号admin，密码admin登录\n\n![2.png](https://raw.githubusercontent.com/chiyuanbo/pic/master/5952327a184e0.png)\n\n按照提示“继续”\n\n![3.png](https://raw.githubusercontent.com/chiyuanbo/pic/master/595232779bd25.png)\n\n按照提示“继续”\n\n![4.png](https://raw.githubusercontent.com/chiyuanbo/pic/master/59523277b0a90.png)\n\n在显示框中写入集群主机名，点击“搜索”（对于集群不能少于4太设备）\n\n![5.png](https://raw.githubusercontent.com/chiyuanbo/pic/master/59523277bc3d3.png)\n\n按提示，点击“继续”\n\n![6.png](https://raw.githubusercontent.com/chiyuanbo/pic/master/59523277c840a.png)\n\n点击页面中的 “更多选项”\n\n![7.png](https://raw.githubusercontent.com/chiyuanbo/pic/master/59523277d1820.png)\n\n删除所有“远程Parcel存储库路径”，最后添加空白一项\n\n![8.png](https://raw.githubusercontent.com/chiyuanbo/pic/master/59523277d0e1a.png)\n\n其中远程Parcel存储库URL为[*http://CM*](http://cm/)管理IP/cdh，如上图，只有一个 远程Parcel存储库URL\n\n点击 “确定”\n\n![9.png](https://raw.githubusercontent.com/chiyuanbo/pic/master/59523277de1a8.png)\n\nCM Agent 选择自定义存储库，存储库地址为[*http://CM*](http://cm/)管理IP/cm\n\n点击“继续”\n\n![5952327a19a64.png](https://raw.githubusercontent.com/chiyuanbo/pic/master/5952327a19a64.png)\n\n不勾选安装Oracle Java Se 开发工具包 点击“继续”\n\n![11.png](https://raw.githubusercontent.com/chiyuanbo/pic/master/595232897c1cc.png)\n\n点击“继续”\n\n![12.png](https://raw.githubusercontent.com/chiyuanbo/pic/master/59523289aba7b.png)\n\n输入相同的root密码，点击“继续”\n\n![13.png](https://raw.githubusercontent.com/chiyuanbo/pic/master/59523289be88f.png)\n\n安装Agent，需要几分钟，请耐心等待，点击“继续”\n\n![14.png](https://raw.githubusercontent.com/chiyuanbo/pic/master/59523287bd14e.png)\n\nCDH的安装包的下载，安装需要几分钟，请耐心等待。\n\n等下载完，点击“继续”\n\n![15.png](https://raw.githubusercontent.com/chiyuanbo/pic/master/59523287e91af.png)\n\n点击“完成”\n\n![16.png](https://raw.githubusercontent.com/chiyuanbo/pic/master/59523289c01da.png)\n\n选择自定义服务，选择服务HDFS,Hive,Hue,Impala,Spark,Yarn,Zookeeper（就像OOzie这些是依赖包，我不选择也会被安装），点击“继续”\n\n![17.png](https://raw.githubusercontent.com/chiyuanbo/pic/master/595232884f33c.png)\n\n点击“继续”\n\n![18.png](https://raw.githubusercontent.com/chiyuanbo/pic/master/595232887bdcf.png)\n\n点击“测试连接” ，成功后，点击“继续”\n\n![19.png](https://raw.githubusercontent.com/chiyuanbo/pic/master/5952328882515.png)\n\n磁盘的配置，好像5.2会自动配置磁盘，点击“继续”\n\n![20.png](https://raw.githubusercontent.com/chiyuanbo/pic/master/595232897d51e.png)\n\n这个安装过程的时间比较长，等完成后，点击“完成” 即安装完成。\n\n## 六、针对CPUID配置\n\n在根据Cloudera Manager安装CDH后,还有一些配置工作要做。\n\n### 确认结点信息\n\n> 访问\n>\n> [*http://CMIP:7180/cmf/login*](http://cmip:7180/cmf/login)\n>\n> 点击主机\n>\n> ![21.png](https://raw.githubusercontent.com/chiyuanbo/pic/master/595232ad3fd24.png)\n>\n> 进入主机界面，CDH安装时namenode是CM自动选择，CDH也不建议修改自动分配结点分布。其中下图中角色多的为namenode,角色少的为datanode，一般正常情况数据结点不会少于3个。\n>\n> ![22.png](https://raw.githubusercontent.com/chiyuanbo/pic/master/595232ad3e61c.png)\n\n根据namenode ,datanode配置Cupid平台需要的结点信息\n\n使用SSH，登陆CMIP（clodera manager 访问的IP）\n\n执行(必须添加所有的数据结点)\n\ncd confbin\n\n./CnfHdfs -NameNode 192.168.12.95\n\n./CnfHdfs -DataNode 192.168.12.94\n\n./CnfHdfs -DataNode 192.168.12.96\n\n……\n\n![23.png](https://raw.githubusercontent.com/chiyuanbo/pic/master/595232ad3ca38.png)\n\n可以通过访问http://cmip/hdfsinfo获取文件cluster.properties ,此文件是USM接口函数来调用\n\n### 建立公用数据目录\n\nssh namenode IP（在上面的步骤中我们已经知道了namenode IP） 建立连接（下面执行脚本直接copy就行,这些命令写在脚本执行无法正常运行,就直接copy执行）\n\nsu hdfs\n\n> hadoop fs -mkdir /data;hadoop fs -chmod 1777 /data;exit\n\n### ES服务命令\n\n> EsService 带参数,具体参数如下\n>\n> Commands:\n\nconsole Launch in the current console.\n\nstart Start in the background as a daemon process.\n\nstop Stop if running as a daemon or in another console.\n\nrestart Stop if running and then start.\n\nstatus Query the current status.\n\n> 例如：\n>\n> EsService restart\n\n### HDFS设置\n\n\n\n## 问题集锦\n\n1.  遇到ssh连接不上问题\n\n![24.png](https://raw.githubusercontent.com/chiyuanbo/pic/master/595232acc0e01.png)\n\n直接删除文件rm -f /root/.ssh/known\\_hosts\n\n2.重启服务或者服务器访问cm界面出现无法访问host monitor   点击左下角Cloudera Manager Service  重启之\n\n重启后若Impala和其他的变成红色  按照顺序 重启之\n\n3.安装步骤中节点处于受管状态导致无法继续\n\n关闭所有的cloudera-scm-agent   点击左上Cloudera manager进入界面  点击主机  然后勾选所有机器 点击倒三角选择删除  确定   域名输入cmf/login  重新登录即可","tags":["Bigdata","linux"]},{"title":"CDH启用snappy压缩","url":"/2022/10/27/cyb-mds/bigdata/Hadoop/cdh启用snappy/","content":"\nCDH自带了snappy压缩包，无需手动安装\n\n确认snappy\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594faly1gmgi6f3wenj20si05i74t.jpg)\n\n打开CDHweb，点击yarn2-配置，搜索io.compression.codecs\n按图改\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594faly1gmgi7vnbuwj21xa0uvn1u.jpg)\n\nhbase已有表可以通过\n` alter 'test', {NAME => 'cf', COMPRESSION => 'SNAPPY'}`\n\n未有表可以通过\n`create 'dcs:t_dev_history',{NAME => 'f', COMPRESSION => 'SNAPPY'}`","tags":["新建,模板,小书匠"]},{"title":"Elasticsearch-Head插件","url":"/2022/10/27/cyb-mds/bigdata/ES/Elasticsearch-Head插件/","content":"\n==作者：YB-Chi==\n\n[toc]\n\nApache Lucene将所有信息写到一个成为倒排索引的结构中。\n\n### ES与MYSQL对应关系\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytjlm05q8j20m80ecdi0.jpg)\n\n以上表为依据，\nES中的新建文档（在Index/type下）相当于Mysql中（在某Database的Table）下插入一行数据。\n\n### 新增\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytjm50l70j20fn08k0t2.jpg)\n\n### 检索\n\n查询全部文档如下： \n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytjmxsl3rj20ka095wf1.jpg)\n\n复合查询 \n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytjnngr9nj210l0ebt9f.jpg)\n\n### 更新\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytjp646wcj20a00btdft.jpg)\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytjprgm7ij20t20e5q45.jpg)\n\n### 删除\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytjqbty22j20kx0g5mxs.jpg)","tags":["Bigdata","ES"]},{"title":"虚拟机配置可供其他电脑访问","url":"/2022/10/27/cyb-mds/linux/虚拟机配置可供其他电脑访问/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n以下在宿主机操作.\n\nvmware-编辑-虚拟网络编辑器\n\n![](https://raw.githubusercontent.com/chiyuanbo/pic/master/20221027160549.png)\n\n填写网关(*.2),和转发配置(和xshell隧道一个意思)\n\n![](https://raw.githubusercontent.com/chiyuanbo/pic/master/20221027160728.png)\n\n网络适配器修改vmnet8\n\n![](https://raw.githubusercontent.com/chiyuanbo/pic/master/20221027160827.png)\n\n防火墙-高级设置-增加入站规则,\n\n![](https://raw.githubusercontent.com/chiyuanbo/pic/master/20221027161000.png)\n\n以下在另一台电脑操作\n\n名称为虚拟机ip,\n\n主机ip为宿主机ip,\n\n端口号为转发端口\n\n![](https://raw.githubusercontent.com/chiyuanbo/pic/master/20221027161204.png)","tags":["Linux","Shell"]},{"title":"mariadb10.3.31序列","url":"/2022/10/08/cyb-mds/database/sql/mysql/mariadb10.3.31序列/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n#### 案例\n\n创建序列\n\n```sql\nCREATE SEQUENCE test_seq START WITH 1 MINVALUE 1 MAXVALUE 16777215 INCREMENT BY 1 CYCLE;\n```\n\n创建表,主键使用序列值\n\n```sql\nCREATE TABLE `test` (\n  `id` mediumint(8) UNSIGNED NOT NULL DEFAULT nextval(`tsoc`.`test_seq`) COMMENT '序列自增主键',\n\t`name` varchar(10) DEFAULT NULL,\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB;\n```\n\n#### 文档\n\nCREATE SEQUENCE\n\n##### MariaDB starting with [10.3](https://mariadb.com/kb/en/what-is-mariadb-103/)\n\nCREATE SEQUENCE was introduced in [MariaDB 10.3](https://mariadb.com/kb/en/what-is-mariadb-103/).\n\n```sql\nCREATE [OR REPLACE] [TEMPORARY] SEQUENCE [IF NOT EXISTS] sequence_name\n[ INCREMENT [ BY | = ] increment ]\n[ MINVALUE [=] minvalue | NO MINVALUE | NOMINVALUE ]\n[ MAXVALUE [=] maxvalue | NO MAXVALUE | NOMAXVALUE ]\n[ START [ WITH | = ] start ] \n[ CACHE [=] cache | NOCACHE ] [ CYCLE | NOCYCLE] \n[table_options]\n```\n\n#### 参数\n\n| Option    | Default value                                                | Description                                                  |\n| :-------- | :----------------------------------------------------------- | :----------------------------------------------------------- |\n| INCREMENT | 1                                                            | Increment to use for values. May be negative. Setting an increment of 0 causes the sequence to use the value of the [auto_increment_increment](https://mariadb.com/kb/en/replication-and-binary-log-server-system-variables/#auto_increment_increment) system variable at the time of creation, which is always a positive number. (see [MDEV-16035](https://jira.mariadb.org/browse/MDEV-16035)). |\n| MINVALUE  | 1 if INCREMENT > 0 and -9223372036854775807 if INCREMENT < 0 | Minimum value for the sequence                               |\n| MAXVALUE  | 9223372036854775806 if INCREMENT > 0 and -1 if INCREMENT < 0 | Max value for sequence                                       |\n| START     | MINVALUE if INCREMENT > 0 and MAX_VALUE if INCREMENT< 0      | First value that the sequence will generate                  |\n| CACHE     | 1000                                                         | Number of values that should be cached. 0 if no CACHE. The underlying table will be updated first time a new sequence number is generated and each time the cache runs out.应该缓存的值的数量。如果没有缓存，则为0。底层表将在第一次生成新序列号和每次缓存用完时更新。 |\n\nIf `CYCLE` is used then the sequence should start again from `MINVALUE` after it has run out of values. Default value is `NOCYCLE`.\n\n如果使用CYCLE，则序列应该在耗尽值后从MINVALUE重新开始。默认值为NOCYCLE。","tags":["sql","msql"]},{"title":"Flink算子","url":"/2022/09/23/cyb-mds/bigdata/Flink/Flink算子/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n","tags":["Linux","Bigdata","Flink"]},{"title":"多机信任","url":"/2022/09/22/cyb-mds/linux/多机信任/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n每个节点执行`ssh-keygen -t rsa`,一路回车\n\n随便一台机器touch ~/.ssh/authorized_keys\n\n合并所有节点的~/.ssh/id_rsa.pub到authorized_keys中,然后分发给所有的节点:~/.ssh/","tags":["Linux","Shell"]},{"title":"Docker安装DataGear","url":"/2022/09/20/cyb-mds/linux/Docker安装DataGear/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n#### 基础组件\n\n##### Docker\n\n```shell\n# 如果没有docker先安装docker\nsudo yum install -y yum-utils\nsudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\nsudo yum install docker-ce docker-ce-cli containerd.io\nsystemctl start docker\n```\n\n##### Docker-compose\n\n```shell\ncurl -L \"https://get.daocloud.io/docker/compose/releases/download/v2.5.0/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\nchmod +x /usr/local/bin/docker-compose\nln -s /usr/local/bin/docker-compose /usr/bin/docker-compose\ndocker-compose --version\n```\n\n#### DataGear\n\nhttps://gitee.com/xinruozhishui/datagear-docker\n\n安装:\n\n```shell\nunzip datagear-docker-master.zip\ncd datagear-docker-master\ndocker-compose up -d\n等待下载最新版完成\ndocker start datagear\n```\n\n浏览器访问`http://192.168.33.128:50401/`","tags":["Linux","DataGear","Docker"]},{"title":"Flink_DEMO1_WordCount","url":"/2022/09/16/cyb-mds/bigdata/Flink/Flink_DEMO1_WordCount/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n引入依赖\n\n```xml\n<properties>\n    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n    <maven.compiler.source>1.8</maven.compiler.source>\n    <maven.compiler.target>1.8</maven.compiler.target>\n    <maven.compiler.compilerVersion>1.8</maven.compiler.compilerVersion>\n</properties>\n\n<dependencies>\n    <dependency>\n        <groupId>org.apache.flink</groupId>\n        <artifactId>flink-streaming-java</artifactId>\n        <version>1.15.2</version>\n    </dependency>\n    <dependency>\n        <groupId>org.apache.flink</groupId>\n        <artifactId>flink-clients</artifactId>\n        <version>1.15.2</version>\n    </dependency>\n</dependencies>    \n```\n\ncode\n\n```java\npublic class WindowWordCount {\n\n    public static void main(String[] args) throws Exception {\n\n        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\n        DataStream<Tuple2<String, Integer>> dataStream = env\n                .socketTextStream(\"localhost\", 9999)\n                .flatMap(new Splitter())\n                .keyBy(value -> value.f0)\n                .window(TumblingProcessingTimeWindows.of(Time.seconds(5)))\n                .sum(1);\n\n        dataStream.print();\n\n        env.execute(\"Window WordCount\");\n    }\n\n    public static class Splitter implements FlatMapFunction<String, Tuple2<String, Integer>> {\n        @Override\n        public void flatMap(String sentence, Collector<Tuple2<String, Integer>> out) throws Exception {\n            for (String word: sentence.split(\" \")) {\n                out.collect(new Tuple2<String, Integer>(word, 1));\n            }\n        }\n    }\n\n}\n```\n\nwindows安装netcat,配置环境变量\n\ncmd启动netcat:\n\n`nc -L -p 9999 -v`\n\n只需输入一些单词，然后按回车键即可传入新单词。这些将作为单词统计程序的输入。如果想查看大于 1 的计数，在 5 秒内重复输入相同的单词即可","tags":["Bigdata","Flink"]},{"title":"Flink单机安装","url":"/2022/09/16/cyb-mds/bigdata/Flink/Flink单机安装/","content":"\n==作者：YB-Chi==\n\n#### 安装\n\n下载:https://mirrors.cloud.tencent.com/apache/flink/\n\nflink-1.15.2-bin-scala_2.12.tgz\n\n上传至服务器解压\n\nbin/start-cluster.sh启动单机\n\n#### 元素\n\n```javascript\nFlink Application Cluster #\nA Flink Application Cluster is a dedicated Flink Cluster that only executes Flink Jobs from one Flink Application. The lifetime of the Flink Cluster is bound to the lifetime of the Flink Application.\n\nFlink Job Cluster #\nA Flink Job Cluster is a dedicated Flink Cluster that only executes a single Flink Job. The lifetime of the Flink Cluster is bound to the lifetime of the Flink Job.\n\nFlink Cluster #\n一般情况下，Flink 集群是由一个 Flink JobManager 和一个或多个 Flink TaskManager 进程组成的分布式系统。\n\nEvent #\nEvent 是对应用程序建模的域的状态更改的声明。它可以同时为流或批处理应用程序的 input 和 output，也可以单独是 input 或者 output 中的一种。Event 是特殊类型的 Record。\n\nExecutionGraph #\n见 Physical Graph。\n\nFunction #\nFunction 是由用户实现的，并封装了 Flink 程序的应用程序逻辑。大多数 Function 都由相应的 Operator 封装。\n\nInstance #\nInstance 常用于描述运行时的特定类型(通常是 Operator 或者 Function)的一个具体实例。由于 Apache Flink 主要是用 Java 编写的，所以，这与 Java 中的 Instance 或 Object 的定义相对应。在 Apache Flink 的上下文中，parallel instance 也常用于强调同一 Operator 或者 Function 的多个 instance 以并行的方式运行。\n\nFlink Application #\nA Flink application is a Java Application that submits one or multiple Flink Jobs from the main() method (or by some other means). Submitting jobs is usually done by calling execute() on an execution environment.\n\nThe jobs of an application can either be submitted to a long running Flink Session Cluster, to a dedicated Flink Application Cluster, or to a Flink Job Cluster.\n\nFlink Job #\nA Flink Job is the runtime representation of a logical graph (also often called dataflow graph) that is created and submitted by calling execute() in a Flink Application.\n\nJobGraph #\n见 Logical Graph。\n\nFlink JobManager #\nFlink JobManager 是 Flink Cluster 的主节点。它包含三个不同的组件：Flink Resource Manager、Flink Dispatcher、运行每个 Flink Job 的 Flink JobMaster。\n\nFlink JobMaster #\nJobMaster 是在 Flink JobManager 运行中的组件之一。JobManager 负责监督单个作业 Task 的执行。以前，整个 Flink JobManager 都叫做 JobManager。\n\nLogical Graph #\nA logical graph is a directed graph where the nodes are Operators and the edges define input/output-relationships of the operators and correspond to data streams or data sets. A logical graph is created by submitting jobs from a Flink Application.\n\nLogical graphs are also often referred to as dataflow graphs.\n\nManaged State #\nManaged State 描述了已在框架中注册的应用程序的托管状态。对于托管状态，Apache Flink 会负责持久化和重伸缩等事宜。\n\nOperator #\nLogical Graph 的节点。算子执行某种操作，该操作通常由 Function 执行。Source 和 Sink 是数据输入和数据输出的特殊算子。\n\nOperator Chain #\n算子链由两个或多个连续的 Operator 组成，两者之间没有任何的重新分区。同一算子链内的算子可以彼此直接传递 record，而无需通过序列化或 Flink 的网络栈。\n\nPartition #\n分区是整个数据流或数据集的独立子集。通过将每个 Record 分配给一个或多个分区，来把数据流或数据集划分为多个分区。在运行期间，Task 会消费数据流或数据集的分区。改变数据流或数据集分区方式的转换通常称为重分区。\n\nPhysical Graph #\nPhysical graph 是一个在分布式运行时，把 Logical Graph 转换为可执行的结果。节点是 Task，边表示数据流或数据集的输入/输出关系或 partition。\n\nRecord #\nRecord 是数据集或数据流的组成元素。Operator 和 Function接收 record 作为输入，并将 record 作为输出发出。\n\nFlink Session Cluster #\n长时间运行的 Flink Cluster，它可以接受多个 Flink Job 的执行。此 Flink Cluster 的生命周期不受任何 Flink Job 生命周期的约束限制。以前，Flink Session Cluster 也称为 session mode 的 Flink Cluster，和 Flink Application Cluster 相对应。\n\nState Backend #\n对于流处理程序，Flink Job 的 State Backend 决定了其 state 是如何存储在每个 TaskManager 上的（ TaskManager 的 Java 堆栈或嵌入式 RocksDB），以及它在 checkpoint 时的写入位置（ Flink JobManager 的 Java 堆或者 Filesystem）。\n\nSub-Task #\nSub-Task 是负责处理数据流 Partition 的 Task。“Sub-Task\"强调的是同一个 Operator 或者 Operator Chain 具有多个并行的 Task 。\n\nTask #\nTask 是 Physical Graph 的节点。它是基本的工作单元，由 Flink 的 runtime 来执行。Task 正好封装了一个 Operator 或者 Operator Chain 的 parallel instance。\n\nFlink TaskManager #\nTaskManager 是 Flink Cluster 的工作进程。Task 被调度到 TaskManager 上执行。TaskManager 相互通信，只为在后续的 Task 之间交换数据。\n\nTransformation #\nTransformation 应用于一个或多个数据流或数据集，并产生一个或多个输出数据流或数据集。Transformation 可能会在每个记录的基础上更改数据流或数据集，但也可以只更改其分区或执行聚合。虽然 Operator 和 Function 是 Flink API 的“物理”部分，但 Transformation 只是一个 API 概念。具体来说，大多数（但不是全部）Transformation 是由某些 Operator 实现的。\n```\n\n","tags":["Linux","Bigdata","Flink"]},{"title":"12实例15node多实例集群部署方案","url":"/2022/09/08/cyb-mds/database/ClickHouse/12实例15node多实例集群部署方案/","content":"\n==作者：YB-Chi==\n\n#### 资源\n\n|      ip       | host           |\n| :-----------: | -------------- |\n| 10.209.18.201 | pod3-mmp1-a-13 |\n| 10.209.18.202 | pod3-mmp1-a-14 |\n| 10.209.18.205 | pod3-mmp1-a-18 |\n| 10.209.18.208 | pod3-mmp1-a-39 |\n| 10.209.18.21  | pod3-mmp1-e-26 |\n| 10.209.18.212 | pod3-mmp1-a-32 |\n| 10.209.18.219 | pod3-mmp1-a-21 |\n| 10.209.18.22  | pod3-mmp1-e-6  |\n| 10.209.18.23  | pod3-mmp1-d-4  |\n| 10.209.18.24  | pod3-mmp1-e-17 |\n| 10.209.18.25  | pod3-mmp1-d-12 |\n| 10.209.18.26  | pod3-mmp1-e-7  |\n| 10.209.18.27  | pod3-mmp1-d-28 |\n| 10.209.18.28  | pod3-mmp1-d-29 |\n| 10.209.18.29  | pod3-mmp1-e-22 |\n\n`root/WY#cluster@2021`\n\n**依赖**\n\nzk三台:\n\n- 10.209.18.201\n- 10.209.18.202\n- 10.209.18.205\n\n\n\n**安装包**\n\n```powershell\n# rpm包 https://github.com/ClickHouse/ClickHouse/\nchmod 755 /data0/software/clickhouse/*\n\nclickhouse-client-21.1.2.15-2.noarch.rpm\nclickhouse-server-21.1.2.15-2.noarch.rpm\nclickhouse-common-static-21.1.2.15-2.x86_64.rpm\nclickhouse-common-static-dbg-21.1.2.15-2.x86_64.rpm\n\nyum -y install expect\nyum install -y perl perl-devel autoconf libaio\n\n# 安装包路径\n/data0/software/clickhouse/\n# 数据路径\n/data0/clickhouse_data\n# 配置文件路径\n/data0/software/clickhouse/config\n# 日志路径\n/data0/clickhouse_data/logs/\n```\n\n\n\n#### 安装\n\n```shell\n\nrpm -ivh /data0/software/clickhouse/clickhouse-common-static-21.1.2.15-2.x86_64.rpm\nrpm -ivh /data0/software/clickhouse/clickhouse-common-static-dbg-21.1.2.15-2.x86_64.rpm\nrpm -ivh /data0/software/clickhouse/clickhouse-server-21.1.2.15-2.noarch.rpm\nrpm -ivh /data0/software/clickhouse/clickhouse-client-21.1.2.15-2.noarch.rpm\n\n\n# 配置文件\n/data0/software/clickhouse/config/config_node0.xml~config_node11.xml\n/data0/software/clickhouse/config/metrika_node0.xml~metrika_node11.xml\n/data0/software/clickhouse/config/user.xml\n```\n\n\n\n#### 配置\n\n```shell\n# config.xml 端口改动\n# 9000 tcp\nnetstat -ntlp|grep 9001\nnetstat -ntlp|grep 9002\nnetstat -ntlp|grep 9003\nnetstat -ntlp|grep 9004\nnetstat -ntlp|grep 9005\nnetstat -ntlp|grep 9006\nnetstat -ntlp|grep 9007\nnetstat -ntlp|grep 9008\nnetstat -ntlp|grep 9009\nnetstat -ntlp|grep 9010\nnetstat -ntlp|grep 9011\nnetstat -ntlp|grep 9012\n\n# 9004 interserver_http\nnetstat -ntlp|grep 9021\nnetstat -ntlp|grep 9022\nnetstat -ntlp|grep 9023\nnetstat -ntlp|grep 9024\nnetstat -ntlp|grep 9025\nnetstat -ntlp|grep 9026\nnetstat -ntlp|grep 9027\nnetstat -ntlp|grep 9028\nnetstat -ntlp|grep 9029\nnetstat -ntlp|grep 9030\nnetstat -ntlp|grep 9031\nnetstat -ntlp|grep 9032\n\n\n# 8123 http\nnetstat -ntlp|grep 8121\nnetstat -ntlp|grep 8122\nnetstat -ntlp|grep 8123\nnetstat -ntlp|grep 8124\nnetstat -ntlp|grep 8125\nnetstat -ntlp|grep 8126\nnetstat -ntlp|grep 8127\nnetstat -ntlp|grep 8128\nnetstat -ntlp|grep 8129\nnetstat -ntlp|grep 8130\nnetstat -ntlp|grep 8131\nnetstat -ntlp|grep 8132\n\n# 9363 prometheus\nnetstat -ntlp|grep 9361\nnetstat -ntlp|grep 9362\nnetstat -ntlp|grep 9363\nnetstat -ntlp|grep 9364\nnetstat -ntlp|grep 9365\nnetstat -ntlp|grep 9366\nnetstat -ntlp|grep 9367\nnetstat -ntlp|grep 9368\nnetstat -ntlp|grep 9369\nnetstat -ntlp|grep 9370\nnetstat -ntlp|grep 9371\n```\n\n```shell\n# 所有节点执行 创建数据目录 配置文件目录 所有的配置文件和日志在一个目录 \nmkdir -p /data0/clickhouse_data\nmkdir -p /data1/clickhouse_data\nmkdir -p /data2/clickhouse_data\nmkdir -p /data3/clickhouse_data\nmkdir -p /data4/clickhouse_data\nmkdir -p /data5/clickhouse_data\nmkdir -p /data6/clickhouse_data\nmkdir -p /data7/clickhouse_data\nmkdir -p /data8/clickhouse_data\nmkdir -p /data9/clickhouse_data\nmkdir -p /data10/clickhouse_data\nmkdir -p /data11/clickhouse_data\n\nmkdir -p /data0/software/clickhouse/config\n\ncp /etc/clickhouse-server/users.xml /data0/software/clickhouse/config/\n\n```\n\n\n\n```shell\n# 复制改名配置文件\ncp /etc/clickhouse-server/config.xml /data0/software/clickhouse/config/\ncp config.xml config_node0.xml\n\n# 修改node0【修改不做记录，详见下个代码块配置文件】  并复制其他实例的配置文件\ncp config_node0.xml config_node1.xml\ncp config_node0.xml config_node2.xml\ncp config_node0.xml config_node3.xml\ncp config_node0.xml config_node4.xml\ncp config_node0.xml config_node5.xml\ncp config_node0.xml config_node6.xml\ncp config_node0.xml config_node7.xml\ncp config_node0.xml config_node8.xml\ncp config_node0.xml config_node9.xml\ncp config_node0.xml config_node10.xml\ncp config_node0.xml config_node11.xml\n\n# 更改路径\nsed -i \"s:/data0/clickhouse_data:/data1/clickhouse_data:g\" /data0/software/clickhouse/config/config_node1.xml\nsed -i \"s:/data0/clickhouse_data:/data2/clickhouse_data:g\" /data0/software/clickhouse/config/config_node2.xml\nsed -i \"s:/data0/clickhouse_data:/data3/clickhouse_data:g\" /data0/software/clickhouse/config/config_node3.xml\nsed -i \"s:/data0/clickhouse_data:/data4/clickhouse_data:g\" /data0/software/clickhouse/config/config_node4.xml\nsed -i \"s:/data0/clickhouse_data:/data5/clickhouse_data:g\" /data0/software/clickhouse/config/config_node5.xml\nsed -i \"s:/data0/clickhouse_data:/data6/clickhouse_data:g\" /data0/software/clickhouse/config/config_node6.xml\nsed -i \"s:/data0/clickhouse_data:/data7/clickhouse_data:g\" /data0/software/clickhouse/config/config_node7.xml\nsed -i \"s:/data0/clickhouse_data:/data8/clickhouse_data:g\" /data0/software/clickhouse/config/config_node8.xml\nsed -i \"s:/data0/clickhouse_data:/data9/clickhouse_data:g\" /data0/software/clickhouse/config/config_node9.xml\nsed -i \"s:/data0/clickhouse_data:/data10/clickhouse_data:g\" /data0/software/clickhouse/config/config_node10.xml\nsed -i \"s:/data0/clickhouse_data:/data11/clickhouse_data:g\" /data0/software/clickhouse/config/config_node11.xml\n\n# 更改端口\nsed -i \"s:8121:8122:g\" /data0/software/clickhouse/config/config_node1.xml\nsed -i \"s:8121:8123:g\" /data0/software/clickhouse/config/config_node2.xml\nsed -i \"s:8121:8124:g\" /data0/software/clickhouse/config/config_node3.xml\nsed -i \"s:8121:8125:g\" /data0/software/clickhouse/config/config_node4.xml\nsed -i \"s:8121:8126:g\" /data0/software/clickhouse/config/config_node5.xml\nsed -i \"s:8121:8127:g\" /data0/software/clickhouse/config/config_node6.xml\nsed -i \"s:8121:8128:g\" /data0/software/clickhouse/config/config_node7.xml\nsed -i \"s:8121:8129:g\" /data0/software/clickhouse/config/config_node8.xml\nsed -i \"s:8121:8130:g\" /data0/software/clickhouse/config/config_node9.xml\nsed -i \"s:8121:8131:g\" /data0/software/clickhouse/config/config_node10.xml\nsed -i \"s:8121:8132:g\" /data0/software/clickhouse/config/config_node11.xml\n\nsed -i \"s:9001:9002:g\" /data0/software/clickhouse/config/config_node1.xml\nsed -i \"s:9001:9003:g\" /data0/software/clickhouse/config/config_node2.xml\nsed -i \"s:9001:9004:g\" /data0/software/clickhouse/config/config_node3.xml\nsed -i \"s:9001:9005:g\" /data0/software/clickhouse/config/config_node4.xml\nsed -i \"s:9001:9006:g\" /data0/software/clickhouse/config/config_node5.xml\nsed -i \"s:9001:9007:g\" /data0/software/clickhouse/config/config_node6.xml\nsed -i \"s:9001:9008:g\" /data0/software/clickhouse/config/config_node7.xml\nsed -i \"s:9001:9009:g\" /data0/software/clickhouse/config/config_node8.xml\nsed -i \"s:9001:9010:g\" /data0/software/clickhouse/config/config_node9.xml\nsed -i \"s:9001:9011:g\" /data0/software/clickhouse/config/config_node10.xml\nsed -i \"s:9001:9012:g\" /data0/software/clickhouse/config/config_node11.xml\n\nsed -i \"s:9021:9022:g\" /data0/software/clickhouse/config/config_node1.xml\nsed -i \"s:9021:9023:g\" /data0/software/clickhouse/config/config_node2.xml\nsed -i \"s:9021:9024:g\" /data0/software/clickhouse/config/config_node3.xml\nsed -i \"s:9021:9025:g\" /data0/software/clickhouse/config/config_node4.xml\nsed -i \"s:9021:9026:g\" /data0/software/clickhouse/config/config_node5.xml\nsed -i \"s:9021:9027:g\" /data0/software/clickhouse/config/config_node6.xml\nsed -i \"s:9021:9028:g\" /data0/software/clickhouse/config/config_node7.xml\nsed -i \"s:9021:9029:g\" /data0/software/clickhouse/config/config_node8.xml\nsed -i \"s:9021:9030:g\" /data0/software/clickhouse/config/config_node9.xml\nsed -i \"s:9021:9031:g\" /data0/software/clickhouse/config/config_node10.xml\nsed -i \"s:9021:9032:g\" /data0/software/clickhouse/config/config_node11.xml\n\nsed -i \"s:node0:node1:g\" /data0/software/clickhouse/config/config_node1.xml\nsed -i \"s:node0:node2:g\" /data0/software/clickhouse/config/config_node2.xml\nsed -i \"s:node0:node3:g\" /data0/software/clickhouse/config/config_node3.xml\nsed -i \"s:node0:node4:g\" /data0/software/clickhouse/config/config_node4.xml\nsed -i \"s:node0:node5:g\" /data0/software/clickhouse/config/config_node5.xml\nsed -i \"s:node0:node6:g\" /data0/software/clickhouse/config/config_node6.xml\nsed -i \"s:node0:node7:g\" /data0/software/clickhouse/config/config_node7.xml\nsed -i \"s:node0:node8:g\" /data0/software/clickhouse/config/config_node8.xml\nsed -i \"s:node0:node9:g\" /data0/software/clickhouse/config/config_node9.xml\nsed -i \"s:node0:node10:g\" /data0/software/clickhouse/config/config_node10.xml\nsed -i \"s:node0:node11:g\" /data0/software/clickhouse/config/config_node11.xml\n\nsed -i \"s:9361:9362:g\" /data0/software/clickhouse/config/config_node1.xml\nsed -i \"s:9361:9363:g\" /data0/software/clickhouse/config/config_node2.xml\nsed -i \"s:9361:9364:g\" /data0/software/clickhouse/config/config_node3.xml\nsed -i \"s:9361:9365:g\" /data0/software/clickhouse/config/config_node4.xml\nsed -i \"s:9361:9366:g\" /data0/software/clickhouse/config/config_node5.xml\nsed -i \"s:9361:9367:g\" /data0/software/clickhouse/config/config_node6.xml\nsed -i \"s:9361:9368:g\" /data0/software/clickhouse/config/config_node7.xml\nsed -i \"s:9361:9369:g\" /data0/software/clickhouse/config/config_node8.xml\nsed -i \"s:9361:9370:g\" /data0/software/clickhouse/config/config_node9.xml\nsed -i \"s:9361:9371:g\" /data0/software/clickhouse/config/config_node10.xml\nsed -i \"s:9361:9372:g\" /data0/software/clickhouse/config/config_node11.xml\n```\n\n```xml\n<!-- config_node0.xml -->\n<?xml version=\"1.0\"?>\n<yandex>\n    <!--日志-->\n    <logger>\n        <level>trace</level>\n        <log>/data0/clickhouse_data/logs/clickhouse-node0.log</log>\n        <errorlog>/data0/clickhouse_data/logs/clickhouse-data0.err.log</errorlog>\n        <size>1000M</size>\n        <count>10</count>\n    </logger>\n    <!--本地节点信息-->\n    <http_port>8121</http_port>\n    <tcp_port>9001</tcp_port>\n    <interserver_http_port>9021</interserver_http_port>\n    <interserver_http_host>pod3-mmp1-a-13</interserver_http_host>\n    <!--本机域名或IP-->\n    <!--本地配置-->\n    <listen_host>0.0.0.0</listen_host>\n    <max_connections>2048</max_connections>\n    <!-- <receive_timeout>800</receive_timeout>\n             <send_timeout>800</send_timeout> -->\n    <keep_alive_timeout>3</keep_alive_timeout>\n\n<!--     <grpc>\n                 <enable_ssl>false</enable_ssl>\n        <ssl_cert_file>/path/to/ssl_cert_file</ssl_cert_file>\n        <ssl_key_file>/path/to/ssl_key_file</ssl_key_file>\n        <ssl_require_client_auth>false</ssl_require_client_auth>\n        <ssl_ca_cert_file>/path/to/ssl_ca_cert_file</ssl_ca_cert_file>\n        <compression>deflate</compression>\n        <compression_level>medium</compression_level>\n        <max_send_message_size>-1</max_send_message_size>\n        <max_receive_message_size>-1</max_receive_message_size>\n        <verbose_logs>false</verbose_logs>\n    </grpc> -->\n\n\n    <openSSL>\n        <server> \n            <certificateFile>/data0/clickhouse_data/server.crt</certificateFile>\n            <privateKeyFile>/data0/clickhouse_data/server.key</privateKeyFile>\n            <dhParamsFile>/data0/clickhouse_data/dhparam.pem</dhParamsFile>\n            <verificationMode>none</verificationMode>\n            <loadDefaultCAFile>true</loadDefaultCAFile>\n            <cacheSessions>true</cacheSessions>\n            <disableProtocols>sslv2,sslv3</disableProtocols>\n            <preferServerCiphers>true</preferServerCiphers>\n        </server>\n\n        <client> \n            <loadDefaultCAFile>true</loadDefaultCAFile>\n            <cacheSessions>true</cacheSessions>\n            <disableProtocols>sslv2,sslv3</disableProtocols>\n            <preferServerCiphers>true</preferServerCiphers>\n            <invalidCertificateHandler>\n                <name>RejectCertificateHandler</name>\n            </invalidCertificateHandler>\n        </client>\n    </openSSL>\n\n    <max_concurrent_queries>100</max_concurrent_queries>\n    <max_server_memory_usage>0</max_server_memory_usage>\n    <max_thread_pool_size>10000</max_thread_pool_size>\n    <max_server_memory_usage_to_ram_ratio>0.9</max_server_memory_usage_to_ram_ratio>\n    <total_memory_profiler_step>4194304</total_memory_profiler_step>\n    <total_memory_tracker_sample_probability>0</total_memory_tracker_sample_probability>\n    <uncompressed_cache_size>8589934592</uncompressed_cache_size>\n    <mark_cache_size>5368709120</mark_cache_size>\n    <path>/data0/clickhouse_data/</path>\n    <tmp_path>/data0/clickhouse_data/tmp/</tmp_path>\n\n    <!-- <users_config>/data/clickhouse/node1/users.xml</users_config> -->\n    <user_files_path>/data0/clickhouse_data/user_files/</user_files_path>\n    <ldap_servers>\n    </ldap_servers>\n    <user_directories>\n        <users_xml>\n            <path>users.xml</path>\n        </users_xml>\n        <local_directory>\n            <path>/data0/clickhouse_data/access/</path>\n        </local_directory>\n    </user_directories>\n\n    <default_profile>default</default_profile>\n    <default_database>default</default_database>\n    <!-- Perform mlockall after startup to lower first queries latency\n                   and to prevent clickhouse executable from being paged out under high IO load.\n         Enabling this option is recommended but will lead to increased startup time for up to a few seconds.\n    -->\n    <mlock_executable>true</mlock_executable>\n    <remap_executable>false</remap_executable>\n\n    <!--集群相关配置 对应metrika文件的标签名-->\n    <remote_servers incl=\"clickhouse_remote_servers\" />\n    <zookeeper incl=\"zookeeper-servers\" optional=\"true\" />\n    <macros incl=\"macros\" optional=\"true\" />\n    <include_from>/data0/software/clickhouse/config/metrika_node0.xml</include_from>\n\n    <builtin_dictionaries_reload_interval>3600</builtin_dictionaries_reload_interval>\n    <max_session_timeout>3600</max_session_timeout>\n    <default_session_timeout>300</default_session_timeout>\n\n    <query_log>\n        <database>system</database>\n        <table>query_log</table>\n        <partition_by>toMonday(event_date)</partition_by>\n        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n    </query_log>\n\n    <trace_log>\n        <database>system</database>\n        <table>trace_log</table>\n\n        <partition_by>toYYYYMM(event_date)</partition_by>\n        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n    </trace_log>\n\n    <query_thread_log>\n        <database>system</database>\n        <table>query_thread_log</table>\n        <partition_by>toMonday(event_date)</partition_by>\n        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n    </query_thread_log>\n\n    <metric_log>\n        <database>system</database>\n        <table>metric_log</table>\n        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n        <collect_interval_milliseconds>1000</collect_interval_milliseconds>\n    </metric_log>\n\n    <asynchronous_metric_log>\n        <database>system</database>\n        <table>asynchronous_metric_log</table>\n        <flush_interval_milliseconds>60000</flush_interval_milliseconds>\n    </asynchronous_metric_log>\n\n    <opentelemetry_span_log>\n        <engine>\n            engine MergeTree\n            partition by toYYYYMM(finish_date)\n            order by (finish_date, finish_time_us, trace_id)\n        </engine>\n        <database>system</database>\n        <table>opentelemetry_span_log</table>\n        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n    </opentelemetry_span_log>\n\n\n    <crash_log>\n        <database>system</database>\n        <table>crash_log</table>\n\n        <partition_by />\n        <flush_interval_milliseconds>1000</flush_interval_milliseconds>\n    </crash_log>\n\n    <dictionaries_config>*_dictionary.xml</dictionaries_config>\n\n    <distributed_ddl>\n        <!-- Path in ZooKeeper to queue with DDL queries -->\n        <path>/clickhouse/task_queue/ddl</path>\n    </distributed_ddl>\n\n    <graphite_rollup_example>\n        <pattern>\n            <regexp>click_cost</regexp>\n            <function>any</function>\n            <retention>\n                <age>0</age>\n                <precision>3600</precision>\n            </retention>\n            <retention>\n                <age>86400</age>\n                <precision>60</precision>\n            </retention>\n        </pattern>\n        <default>\n            <function>max</function>\n            <retention>\n                <age>0</age>\n                <precision>60</precision>\n            </retention>\n            <retention>\n                <age>3600</age>\n                <precision>300</precision>\n            </retention>\n            <retention>\n                <age>86400</age>\n                <precision>3600</precision>\n            </retention>\n        </default>\n    </graphite_rollup_example>\n\n    <format_schema_path>/data0/clickhouse_data/format_schemas/</format_schema_path>\n\n    <query_masking_rules>\n        <rule>\n            <name>hide encrypt/decrypt arguments</name>\n            <regexp>((?:aes_)?(?:encrypt|decrypt)(?:_mysql)?)\\s*\\(\\s*(?:'(?:\\\\'|.)+'|.*?)\\s*\\)</regexp>\n            <replace>\\1(???)</replace>\n        </rule>\n    </query_masking_rules>\n\n    <!-- <merge_tree>\n                 <parts_to_delay_insert>300</parts_to_delay_insert>\n        <parts_to_throw_insert>600</parts_to_throw_insert>\n        <max_delay_to_insert>2</max_delay_to_insert>\n    </merge_tree> -->\n\n    <merge_tree>\n        <max_suspicious_broken_parts>5</max_suspicious_broken_parts>\n    </merge_tree>\n    <!-- 不限制drop表 -->\n    <max_table_size_to_drop>0</max_table_size_to_drop>\n    <max_partition_size_to_drop>0</max_partition_size_to_drop>\n\n    <send_crash_reports>\n        <enabled>false</enabled>\n        <anonymize>false</anonymize>\n        <endpoint>https://6f33034cfe684dd7a3ab9875e57b1c8d@o388870.ingest.sentry.io/5226277</endpoint>\n    </send_crash_reports>\n\n    <prometheus>\n        <endpoint>/metrics</endpoint>\n        <port>9361</port>\n        <metrics>true</metrics>\n        <events>true</events>\n        <asynchronous_metrics>true</asynchronous_metrics>\n        <status_info>true</status_info>\n    </prometheus>\n\n    <timezone>Asia/Shanghai</timezone>\n</yandex>\n```\n\n\n\n```shell\n# 由于sed -i 修改了日志目录,重新把日志目录改回来\nsed -i \"s:/data1/clickhouse_data/logs:/data0/clickhouse_data/logs:g\" /data0/software/clickhouse/config/config_node1.xml\nsed -i \"s:/data2/clickhouse_data/logs:/data0/clickhouse_data/logs:g\" /data0/software/clickhouse/config/config_node2.xml\nsed -i \"s:/data3/clickhouse_data/logs:/data0/clickhouse_data/logs:g\" /data0/software/clickhouse/config/config_node3.xml\nsed -i \"s:/data4/clickhouse_data/logs:/data0/clickhouse_data/logs:g\" /data0/software/clickhouse/config/config_node4.xml\nsed -i \"s:/data5/clickhouse_data/logs:/data0/clickhouse_data/logs:g\" /data0/software/clickhouse/config/config_node5.xml\nsed -i \"s:/data6/clickhouse_data/logs:/data0/clickhouse_data/logs:g\" /data0/software/clickhouse/config/config_node6.xml\nsed -i \"s:/data7/clickhouse_data/logs:/data0/clickhouse_data/logs:g\" /data0/software/clickhouse/config/config_node7.xml\nsed -i \"s:/data8/clickhouse_data/logs:/data0/clickhouse_data/logs:g\" /data0/software/clickhouse/config/config_node8.xml\nsed -i \"s:/data9/clickhouse_data/logs:/data0/clickhouse_data/logs:g\" /data0/software/clickhouse/config/config_node9.xml\nsed -i \"s:/data10/clickhouse_data/logs:/data0/clickhouse_data/logs:g\" /data0/software/clickhouse/config/config_node10.xml\nsed -i \"s:/data11/clickhouse_data/logs:/data0/clickhouse_data/logs:g\" /data0/software/clickhouse/config/config_node11.xml\n```\n\n\n\n```shell\n# 配置文件改完后 scp到其他节点 由于met文件是本地改好上传的，此处将config文件放入config-dir 然后scp\n\n# WY#cluster@2021\nscp -rp config-dir root@pod3-mmp1-a-14:/data0/software/clickhouse/config/\nscp -rp config-dir root@pod3-mmp1-a-18:/data0/software/clickhouse/config/\nscp -rp config-dir root@pod3-mmp1-a-39:/data0/software/clickhouse/config/\nscp -rp config-dir root@pod3-mmp1-e-26:/data0/software/clickhouse/config/\nscp -rp config-dir root@pod3-mmp1-a-32:/data0/software/clickhouse/config/\nscp -rp config-dir root@pod3-mmp1-a-21:/data0/software/clickhouse/config/\nscp -rp config-dir root@pod3-mmp1-e-6:/data0/software/clickhouse/config/\nscp -rp config-dir root@pod3-mmp1-d-4:/data0/software/clickhouse/config/\nscp -rp config-dir root@pod3-mmp1-e-17:/data0/software/clickhouse/config/\nscp -rp config-dir root@pod3-mmp1-d-12:/data0/software/clickhouse/config/\nscp -rp config-dir root@pod3-mmp1-e-7:/data0/software/clickhouse/config/\nscp -rp config-dir root@pod3-mmp1-d-28:/data0/software/clickhouse/config/\nscp -rp config-dir root@pod3-mmp1-d-29:/data0/software/clickhouse/config/\nscp -rp config-dir root@pod3-mmp1-e-22:/data0/software/clickhouse/config/\n\n\n# 给每个节点更改host\nsed -i \"s:pod3-mmp1-a-13:pod3-mmp1-a-14:g\" /data0/software/clickhouse/config/config-dir/*\nsed -i \"s:pod3-mmp1-a-13:pod3-mmp1-a-18:g\" /data0/software/clickhouse/config/config-dir/*\nsed -i \"s:pod3-mmp1-a-13:pod3-mmp1-a-39:g\" /data0/software/clickhouse/config/config-dir/*\nsed -i \"s:pod3-mmp1-a-13:pod3-mmp1-e-26:g\" /data0/software/clickhouse/config/config-dir/*\nsed -i \"s:pod3-mmp1-a-13:pod3-mmp1-a-32:g\" /data0/software/clickhouse/config/config-dir/*\nsed -i \"s:pod3-mmp1-a-13:pod3-mmp1-a-21:g\" /data0/software/clickhouse/config/config-dir/*\nsed -i \"s:pod3-mmp1-a-13:pod3-mmp1-e-6:g\" /data0/software/clickhouse/config/config-dir/*\nsed -i \"s:pod3-mmp1-a-13:pod3-mmp1-d-4:g\" /data0/software/clickhouse/config/config-dir/*\nsed -i \"s:pod3-mmp1-a-13:pod3-mmp1-e-17:g\" /data0/software/clickhouse/config/config-dir/*\nsed -i \"s:pod3-mmp1-a-13:pod3-mmp1-d-12:g\" /data0/software/clickhouse/config/config-dir/*\nsed -i \"s:pod3-mmp1-a-13:pod3-mmp1-e-7:g\" /data0/software/clickhouse/config/config-dir/*\nsed -i \"s:pod3-mmp1-a-13:pod3-mmp1-d-28:g\" /data0/software/clickhouse/config/config-dir/*\nsed -i \"s:pod3-mmp1-a-13:pod3-mmp1-d-29:g\" /data0/software/clickhouse/config/config-dir/*\nsed -i \"s:pod3-mmp1-a-13:pod3-mmp1-e-22:g\" /data0/software/clickhouse/config/config-dir/*\n```\n\n\n\n\n\n```shell\n# 操作所有窗口\n# 编辑启动脚本\nvim /data0/software/clickhouse/start.sh\n\nnohup /usr/bin/clickhouse-server -C=/data0/software/clickhouse/config/config_node0.xml --pid-file=/data0/software/clickhouse/config/config_node0.pid &\nnohup /usr/bin/clickhouse-server -C=/data0/software/clickhouse/config/config_node1.xml --pid-file=/data0/software/clickhouse/config/config_node1.pid &\nnohup /usr/bin/clickhouse-server -C=/data0/software/clickhouse/config/config_node2.xml --pid-file=/data0/software/clickhouse/config/config_node2.pid &\nnohup /usr/bin/clickhouse-server -C=/data0/software/clickhouse/config/config_node3.xml --pid-file=/data0/software/clickhouse/config/config_node3.pid &\nnohup /usr/bin/clickhouse-server -C=/data0/software/clickhouse/config/config_node4.xml --pid-file=/data0/software/clickhouse/config/config_node4.pid &\nnohup /usr/bin/clickhouse-server -C=/data0/software/clickhouse/config/config_node5.xml --pid-file=/data0/software/clickhouse/config/config_node5.pid &\nnohup /usr/bin/clickhouse-server -C=/data0/software/clickhouse/config/config_node6.xml --pid-file=/data0/software/clickhouse/config/config_node6.pid &\nnohup /usr/bin/clickhouse-server -C=/data0/software/clickhouse/config/config_node7.xml --pid-file=/data0/software/clickhouse/config/config_node7.pid &\nnohup /usr/bin/clickhouse-server -C=/data0/software/clickhouse/config/config_node8.xml --pid-file=/data0/software/clickhouse/config/config_node8.pid &\nnohup /usr/bin/clickhouse-server -C=/data0/software/clickhouse/config/config_node9.xml --pid-file=/data0/software/clickhouse/config/config_node9.pid &\nnohup /usr/bin/clickhouse-server -C=/data0/software/clickhouse/config/config_node10.xml --pid-file=/data0/software/clickhouse/config/config_node10.pid &\nnohup /usr/bin/clickhouse-server -C=/data0/software/clickhouse/config/config_node11.xml --pid-file=/data0/software/clickhouse/config/config_node11.pid &\n\n# 保存退出\nchmod +x /data0/software/clickhouse/start.sh\n\n# 启动\n/data0/software/clickhouse/start.sh\n\n# 检查 应当有看门狗和程序server俩服务 加上ps供25个\nps -ef|grep click\nps -ef|grep click|wc -l\n\n\n# 验证客户端\nclickhouse-client --host pod3-mmp1-a-13 --port 9001\nselect * from system.clusters;\n\n# 创建集群表要指定集群名 集群名为metr*文件中的标签名ch_cluster_all\ncreate database jikewang on cluster perftest_3shards_1replicas;\nCREATE TABLE log_test ON CLUSTER perftest_3shards_1replicas\n(\n    `ts` DateTime,\n    `uid` String,\n    `biz` String\n)\nENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/log_test', '{replica}')\nPARTITION BY toYYYYMMDD(ts)\nORDER BY ts;\n```\n\n\n\n**参考文档:**\n\n[ClickHouse最佳实战之Clickhouse服务配置文件config.xml详解](https://zhuanlan.zhihu.com/p/161444526)\n\n[ClickHouse集群多实例部署](https://blog.csdn.net/ashic/article/details/105901792)\n\n[ClickHouse 多实例环境安装](https://www.jianshu.com/p/22a4687c422a)","tags":["Linux","Clickhouse"]},{"title":"面试","url":"/2022/09/07/cyb-mds/面试/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n#### ES\n\n##### 什么是ES，你对ES的理解，\n\n1. ES是建立在lucene基础上的一个开源搜索和分析引擎\n2. ES本身有一个分布式存储，检索速度快的特性，所以我们经常会用他去实现全文检索这一类的场景，比如说网站搜索，公司内部使用ELK做日志聚集和检索，基本上涉及到TB级的数据使用ES是一个很好地选择\n\n##### ES为什么快\n\n1. ES是基于lucene开发的全文搜索引擎，lucene是擅长管理大量的索引数据的，他会对数据进行分词再保存，提升检索效率\n2. ES采用了倒排索引\n3. ES存储的数据才用了分片机制，多个分片增加处理的并行度。\n4. ES横向扩展性好\n5. ES内部提供的数据汇总和索引生命周期管理的一些功能可以方便我们更加高效的存储和搜索数据\n\n##### 倒排索引\n\n倒排索引也叫反向索引，通俗来讲正向索引是通过key找value，反向索引则是通过value找key.\n\nElasticsearch分别为每个field都建立了一个倒排索引，倒排列表记录了出现过某个单词的文档列表及单词在该文档中的位置，每条记录称为一个倒排项(Posting)。根据倒排列表，可以知道哪些文档包含某个单词,避免全表扫描。\n\n##### Elasticsearch 索引数据多了怎么办,如何调优,部署\n\n* 增加ES节点\n* 增加JVM，且不超过机器内存的1/2\n* 启用lz4压缩\n* 滚动创建索引，每天一个索引\n* 冷热分离\n* 定期force_merge\n\n##### Elasticsearch是如何实现master选举的\n\n1、对所有可以成为master的节点根据nodeId排序，每次选举每个节点都把自己所知道节点排一次序，然后选出第0位节点，暂且认为它是master节点。\n2、如果对某个节点的投票数达到一定的值（master节点数n/2+1）并且该节点自己也选举自己，那这个节点就是master。否则重新选举。\n\n##### 详细描述一下 Elasticsearch 索引文档的过程\n\n\n\n##### Elasticsearch 如何避免裂脑问题\n\n1. 修改集群中每个节点的配置文件（elasticsearch.yml）参数 discovery.zen.minimum_master_nodes，这个参数决定了主节点选择过程中最少需要多少个 master 节点，默认配置是1。\n    一个基本原则是这里需要设置成 N/2+1，N 是集群中节点的数量。\n2. 修改集群中每个节点的配置文件（elasticsearch.yml）参数 discovery.zen.ping.timeout，默认值是3，它确定节点在假定节点发生故障之前将等待集群中其他节点响应的时间。在网络速度较慢的情况下，稍微增加默认值绝对是个好主意。此参数不仅可以满足更高的网络延迟，而且在节点由于过载而响应较慢的情况下也很有用。\n3. 修改集群中每个节点的配置文件（elasticsearch.yml）参数 discovery.zen.ping.unicast.hosts，把集群中可能成为主节点的机器节点都配置到这个参数中。\n\n##### Elasticsearch的三种分页方式\n\n| 分页方式     | 性能 | 优点                                             | 缺点                                                         | 场景                                   |\n| ------------ | ---- | ------------------------------------------------ | ------------------------------------------------------------ | -------------------------------------- |\n| from + size  | 低   | 灵活性好，实现简单                               | 深度分页问题                                                 | 数据量比较小，能容忍深度分页问题       |\n| scroll       | 中   | 解决了深度分页问题                               | 无法反应数据的实时性（快照版本）维护成本高，需要维护一个 scroll_id | 海量数据的导出需要查询海量结果集的数据 |\n| search_after | 高   | 性能最好不存在深度分页问题能够反映数据的实时变更 | 实现复杂，需要有一个全局唯一的字段连续分页的实现会比较复杂，因为每一次查询都需要上次查询的结果 | 海量数据的分页                         |\n\n##### Elasticsearch修改索引结构\n\n还是要结合具体的数据场景来处理。\n\n如果数据源数据都有，且数据量较大，能比较方便的重新入得话建议是采用重入数据的方式。\n\n如果无法重入或者其他原因，基于5.0以后的版本，ES可以使用reindex重构索引。但是reindex比较慢，需要从以下几个方面去优化下：\n\n1. reindex默认使用1000进行批量操作，通过调整batch_size，一般从5~15MB的物理文件大小开始往上增加批次，通过kibana或者iostat等监控速度瓶颈。\n2. ES副本数设置为0\n3. 增加refresh间隔或干脆禁用掉\n4. reindex的底层是scroll实现，借助scroll并行优化方式，提升效率\n\n由于我们使用的是1.7.2版本，对于这种情况我们只能是重入数据。之前我们的产品是采用的预留字段的方式，这也是一个低维护成本且实用的方法。\n\n##### Elasticsearch应该设置多少分片\n\n一个分片是一个lucene索引实例，分片是有代价的，消耗一定的内存、文件句柄、cpu等，es官方推荐的是对于时序性数据，每个分片20~40G之间，并且不超过es的jvm最大堆空间。如果没有特别苛刻的要求，按照官方就行。另外每个查询都是在单个分片上以单线程方式执行的，提升分片数量能提高查询效率，但也不是分片越多，查询越快，如果查询请求很多，任务需要进入队列并按顺序加以处理，并不会比查询较少的大分片快。从查询性能的角度来看，确定分片数量还是要使用有实际意义的数据和查询进行基准测试。\n\n\n\n#### MYSQL\n\n##### MYSQL引擎\n\n1. MyIsam ， 2. InnoDB， 3. Memory， 4. Blackhole， 5. CSV， 6. Performance_Schema， 7. Archive， 8. Federated ， 9 Mrg_Myisam\n\n##### MySQL的InnoDB和MyISAM的区别:\n\n1. 在事务上:myisam不支持事务，innodb支持事务。\n2. myisam使用了表级锁，innodb使用了行级锁\n3. InnoDB支持外键，而MyISAM不支持 \n4. InnoDB不支持全文索引，而MyISAM支持。\n\n\n\n##### MYSQL有几种索引\n\n###### 从数据结构角度\n\n　　1、 B-Tree 索引\n\n​\t\t\t\t\t\t\tB-Tree 索引需要从根节点到枝节点，最后才能访问到页节点这样多次的IO访问\n\n​\t\t\t\t\t\t\tBtree类型在我们查询数据时适合用于范围查找\n\n　　2、 hash索引【**MyISAM和InnoDB不支持**】\n\n​\t\t\t\t\t\t\t检索效率非常高，索引的检索可以一次定位）\n\n​\t\t\t\t\t\t\tHash 索引仅仅能满足\"=\"，\"IN\"和\"<=>\"查询，不能使用范围查询。\n\n​\t\t\t\t\t\t\tHash 索引无法被用来避免数据的排序操作\n\n​\t\t\t\t\t\t\tHash 索引在任何时候都不能避免表扫描。\n\n　　3、 FULLTEXT索引\n\n　　4、 R-Tree索引\n\n###### 从物理存储角度\n\n　　1、 聚集索引\n\n　　2、 非聚集索引\n\n###### 从逻辑角度\n\n　　1、 主键索引:主键索引是一种特殊的唯一索引，不允许有空值\n\n　　2、 普通索引或者单列索引\n\n　　3、 多列索引（复合索引）\n\n　　4、 唯一索引或者非唯一索引（索引列的值必须唯一，但允许有空值。）\n\n　　5、 全文索引\n\n\n\n##### MYSQL优化\n\n1. 硬件和操作系统层面的优化\n2. 架构设计层面的优化\n3. mysql程序配置的优化\n4. sql执行优化\n\n\n\n##### MYSQL优化\n\n1. ① SQL优化\n\n> - 避免 SELECT *，只查询需要的字段。\n> - 小表驱动大表，即小的数据集驱动大的数据集:\n>   当B表的数据集比A表小时，用in优化 exist两表执行顺序是先查B表再查A表查询语句:SELECT * FROM tb_dept WHERE id in (SELECT id FROM tb_dept) ;\n>   当A表的数据集比B表小时，用exist优化in ，两表执行顺序是先查A表，再查B表，查询语句:SELECT * FROM A WHERE EXISTS (SELECT id FROM B WHERE A.id = B.ID) ;\n> - 尽量使用连接代替子查询，因为使用 join 时，MySQL 不会在内存中创建临时表。\n> - 对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。对字段进行 null 值判断，使用!=或<>操作符，使用 or 来连接条件，in 和 not in 【between/exists  代替】\n> - \n\n​\t\t② 优化索引的使用\n\n> - 尽量使用主键查询，而非其他索引，因为主键查询不会触发回表查询。\n> - 不做列运算，把计算都放入各个业务系统实现\n> - 查询语句尽可能简单，大语句拆小语句，减少锁时间\n> - or 查询改写成 union 查询\n> - 不用函数和触发器\n> - 避免 %xx 查询，可以使用:select * from t where reverse(f) like reverse('%abc');\n> - 少用 join 查询\n> - 使用同类型比较，比如 '123' 和 '123'、123 和 123\n> - 尽量避免在 where 子句中使用 != 或者 <> 操作符，查询引用会放弃索引而进行全表扫描\n> - 列表数据使用分页查询，每页数据量不要太大\n> - 避免在索引列上使用 is null 和 is not null\n\n​\t\t③ 表结构设计优化\n\n> - 使用可以存下数据最小的数据类型。\n> - 尽量使用 tinyint、smallint、mediumint 作为整数类型而非 int。\n> - 尽可能使用 not null 定义字段，因为 null 占用 4 字节空间。数字可以默认 0 ，字符串默认 “”\n> - 尽量少用 text 类型，非用不可时最好独立出一张表。\n> - 尽量使用 timestamp，而非 datetime。\n> - 单表不要有太多字段，建议在 20 个字段以内。\n\n2. 配置优化\n\n\n\n3. 部署优化-主从复制（读写分离）\n\n##### Mysql 主从复制\n\nMySQL中复制的优点包括:\n\n- 横向扩展解决方案 - 在多个从站之间分配负载以提高性能。在此环境中，所有写入和更新都必须在主服务器上进行。但是，读取可以在一个或多个从设备上进行。该模型可以提高写入性能（因为主设备专用于更新），同时显着提高了越来越多的从设备的读取速度。\n- 数据安全性 - 因为数据被复制到从站，并且从站可以暂停复制过程，所以可以在从站上运行备份服务而不会破坏相应的主数据。\n- 分析 - 可以在主服务器上创建实时数据，而信息分析可以在从服务器上进行，而不会影响主服务器的性能。\n- 远程数据分发 - 您可以使用复制为远程站点创建数据的本地副本，而无需永久访问主服务器。\n\n总结:主从复制解决了数据库的读写分离，并很好的提升了读的性能\n\n步骤\n\n1. 在主服务器上，您必须启用二进制日志记录并配置唯一的服务器ID。需要重启服务器。\n\n编辑主服务器的配置文件 `my.cnf`，添加如下内容\n\n```bash\n[mysqld]\nlog-bin=/var/log/mysql/mysql-bin\nserver-id=1\n```\n\n2. 创建一个专门用于复制数据的用户\n3. 在`从服务器`上使用刚才的用户进行测试连接\n\n```bash\nshell> mysql -urepl -p'QFedu123!' -hmysql-master1\n```\n\n\n\n##### ORACLE和MYSQL区别\n\n###### 一、宏观上:\n\n1、Oracle是大型的数据库而Mysql是中小型数据库；Mysql是开源的，Oracle是收费的。\n\n2、Oracle支持大并发，大访问量。\n\n###### 二、微观上:\n\n1、对于事务的支持\n\nMysql对于事务只有innodb可以支持；而Oracle对于事物是完全支持的。\n\n2、并发性\n\nMysql以表锁为主，对资源锁定的力度很大，如果一个session对一个表加锁时间过长，会让其他session无法更新此表的数据。【innodb是行级锁】\n\nOracle使用行级锁，对资源锁定的力度要小很多，只是锁定sql需要的资源，并且加锁是在数据库中的数据行上，不依赖于索引。所以oracle对并发性的支持要好很多。\n\n3、数据的持久性\n\nOracle保证提交的事务均可以恢复，因为Oracle把提交的sql操作线写入了日志文件中，保存到磁盘上，如果出现数据库或者主机异常重启，重启Oracle可以靠日志恢复客户提交的数据。\n\nMysql默认提交sql语句，但是如果更新过程中出现db或者主机重启的问题，也可能会丢失数据。\n\n4、提交方式\n\nOracle默认不自动提交，需要手动提交。Mysql默认自动提交。\n\n5、操作上的一些区别\n\nmysql对sql语句有很多非常实用而方便的扩展\n\n主键 \tMysql一般使用自动增长类型，Oracle没有自动增长类型，主键一般使用的序列\n\n单引号的处理\t MYSQL里可以用双引号包起字符串，ORACLE里只可以用单引号包起字符串。\n\n分页的SQL语句的处理\t MYSQL处理翻页的SQL语句比较简单，用LIMIT 开始位置, 记录个数；ORACLE处理分页很麻烦,需要指定rownum,而且rownum只能做<或者<=的条件查询\n\n6、数据复制\n\nMySQL:复制服务器配置简单，但主库出问题时，丛库有可能丢失一定的数据。且需要手工切换丛库到主库。\n\nOracle:既有推或拉式的传统数据复制，也有dataguard的双机或多机容灾机制，主库出现问题是，可以自动切换备库到主库，但配置管理较复杂。\n\n7、性能诊断方面\n\nOracle有各种成熟的性能诊断调优工具，能实现很多自动分析、诊断功能。比如awr、addm、sqltrace、tkproof等 ；MySQL的诊断调优方法较少，主要有慢查询日志。\n\n\n\n##### mysql索引结构详细讲讲（回表）\n\n索引从逻辑角度上来讲分为主键索引和非主键索引，它俩的数据结构都是 B+Tree，唯一的区别是叶子结点中存储的内容不同：\n\n- 主键索引的叶子结点存储的是一行完整的数据。\n- 非主键索引的叶子结点存储的则是主键值。\n\n对于非主键索引，如果一个表有主键id，索引name和普通列age年龄，如果我们查询某个name的所有列，他会去索引里找到这个name对应的主键id值，在通过id去主键索引里找到全量记录，这个行为称为回表。当然不是非主键索引一定会回表，如果不查询age的话就会索引覆盖，不会回表，这也是我们经常不用select * 的原因。\n\n\n\n##### mysql的数据文件和索引文件存在一起吗\n\ninnodb是放在一起的 \n\n\n\n##### mysql底层存储的数据结构\n\nMySQL底层使用的是B+tree存储的\n非叶子节点存储索引和下一个子节点的地址\n叶子结点存储所有的索引和数据\n\n\n\n##### mysql为什么选择B+tree做索引\n\n1. 常规数据库一般都是选择btree和b+tree做索引,相对于btree,b+tree非叶子节点不存储数据,所以它每一层存储的索引数据更多,使得磁盘io次数更少\n2. 在mysql中,范围查询是很常用的,而b+tree的叶子节点使用双向链表关联,所以查询的时候只需要查询两个节点遍历就行,而btree需要获取所有节点.\n3. 在数据检索方面,因为所有数据都存在叶子节点,所以b+tree的io次数更稳定一写\n4. 因为b+tree数据全在叶子节点,全表扫描时只扫叶子节点就行,而btree需要遍历整棵树\n\n\n\n#### ClickHouse\n\n\n\n##### 简单介绍下CK\n\nCK是俄罗斯YANDEX开源的一款是基于 MPP 架构的分布式 ROLAP列式存储数据库,主要用于WEB流量分析.\n\n**他的特性:**\n\n它支持完备的SQL操作\n\n支持数据压缩\n\n使用磁盘存储数据\n\n本质上属于MPP架构,多核并行处理\n\n集群是是分布式多主架构,读请求可以打到任意节点\n\n数据是列式存储,cpu用向量执行SIMD,处理效率高\n\n支持稀疏主索引和辅助数据跳过索引\n\n支持亚秒级,适合在线查询\n\n**他的缺点:**\n\n低版本不支持完整的事物\n\nolap通病,对高速低延迟删改支持不太好\n\n稀疏索引使ck通过key查询单行不是特别高效\n\n\n\n##### ClickHouse有哪些表引擎\n\n适用于高负载的功能最强最通用的**MergeTree**\n\n最小功能的轻量引擎**LOG**\n\n集成引擎:mysql\\kafka\\jdbc等等\n\n特定功能的引擎:JOIN\\SET\\Distributed等等\n\n\n\n##### MergeTree特点\n\n* 在写入一批数据时，数据总会以数据片段的形式写入磁盘，且数据片段不可修改。为了避免片段过多，ClickHouse会通过后台线程，定期合并这些数据片段，属于相同分区的数据片段会被合成一个新的片段。这种数据片段往复合并的特点，也正是合并树名称的由来。\n\n- 存储的数据按照主键排序：允许创建稀疏索引，从而加快数据查询速度\n- 支持分区，可以通过PARTITION BY指定分区字段。\n- 支持数据副本\n- 支持数据采样\n\n\n\n##### ClickHouse为何如此之快\n\n硬件方面,ck会在内存中进行GROUP BY,并且使用HashTable装载数据。\n\n算法方面,ck采用了常量,非常量,正则匹配：\n对于常量，使用 Volnitsky 算法；\n对于非常量， CPU向量化执行 SIMD，暴力优化;\n正则匹配使用 re2 和 hyperscan 算法。\n\nck一直在验证市面上新出的强大的算法,可行就纳入其用\n\n同一个函数特定场景,使用特殊优化,比如一些函数会根据数据量的大小选择不同的算法\n\n最后就是ck因为有yandex的数据,可以进行持续测试,持续改进\n\n\n\n##### ck和mysql的区别\n\n他俩的区别是非常多的，我从我目前想到的一些点来说\n\n1. 首先是存储方面，mysql是行式存储，ck是列式存储，列式存储有天然优势去做统计分析、聚类分析。\n2. mysql是面向OLTP的数据库，强调事务一致性。ck是面向OLAP，侧重于联机数据分析。\n3. ck等数据库更擅长于大数据量的导入导出，且提供了bulkload、copy from、元copy等优化的入库方式，但往往不擅长一份数据的反复修改，有些数据库比如gp，并不删除物理数据而是将这一行数据标记版本号。\n4. ck还有数据压缩，可以使用lz4或者zstd等高效压缩算法。\n5. ck是分布式多主架构的，使读请求可以随机打到任意节点，写请求也不用转发到master，这样它的大数据量的读写性能很高。\n6. ck还有向量引擎，利用 SIMD 指令实现并行计算。\n7. 最后就是索引，ck采用了稀疏索引及跳数索引。同时还有很多 MergeTree，提供海量业务场景支持。\n\n\n\n#### REDIS\n\n\n\n##### REDIS集群原理\n\n所有的redis节点彼此互联，内部使用二进制协议优化传输速度和带宽。\n\n节点的fail是通过集群中超过半数的节点检测失效时才生效。\n\n客户端可以与任何一个节点相连接，然后就可以访问集群中的任何一个节点。对其进行存取和其他操作。\n\n在redis的每一个节点上，都有这么两个东西，一个是插槽（slot），取值范围是:0-16383，还有一个就是cluster，当我们的存取的key到达的时候，redis会根据crc16的算法得出一个结果，然后把结果对 16384 取模，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，通过这个值，去找到对应的插槽所对应的节点，然后直接自动跳转到这个对应的节点上进行存取操作。\n\n\n\n##### Redis 主从复制、哨兵和集群三者区别\n\n1. 主从模式：读写分离，备份，一个Master可以有多个Slaves。\n2. 哨兵sentinel：监控，自动转移，哨兵发现主服务器挂了后，就会从slave中重新选举一个主服务器。\n3. 集群：为了解决单机Redis容量有限的问题，将数据按一定的规则分配到多台机器，内存/QPS不受限于单机，可受益于分布式集群高扩展性。\n\n\n\n##### REDIS数据类型\n\nRedis支持五种数据类型:\n\nString，底层是动态字符串\n\nList，底层是双向列表和压缩列表\n\nHash，底层是压缩列表和Hash表\n\nSet，底层是整数数组和Hash表\n\nSorted Set，底层是压缩列表和跳表\n\n还有一些扩展类型:Bit\\HyperLogLog\\Geo\n\n\n\n##### Redis如何解决Hash冲突的\n\nHash冲突是由于被计算的数据是无限的，而计算后的结果范围是有限的，所以总会存在经过计算后得到的值是一样的。Redis采取的是和jdk1.7的hashmap相同的方案，链式寻址法，这是一种非常常见的方法。就是把存在hash冲突的key以单向链表的方式进行存储。当key特别大的时候，链式查找的时间会相对增加。\n\n\n\n\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594faly1gs0gfrexnqj226z0yjjyo.jpg)\n\n\n\n##### REDIS缓存雪崩\n\n缓存雪崩是指大量的请求无法在Redis中进行处理，这些请求到了数据库层，导致数据库压力激增。\n\n缓存雪崩一般是由两个原因导致的,一方面是缓存中有大量数据同时过期，导致大量请求无法得到处理,可以通过微调过期时间或者服务降级来解决;另一方面是redis实例发生故障宕机了,这个一般是是在业务系统中实现**服务熔断**或**请求限流**机制,但最好是搭建高可靠redis集群来事前预防。\n\n\n\n##### REDIS缓存击穿\n\n缓存击穿，是指针对某个访问非常频繁的热点数据,请求无法在缓存中处理,全部发送到了数据库层,导致数据库压力激增,缓存击穿一般发生在key失效时。\n\n对于缓存击穿,一般解决方式也比较直接,对于热点数据不设置过期时间,这样一来热点数据的请求都可以在缓存中处理,redis的数万级别的高吞吐量可以很好地应对大量的并发请求。\n\n\n\n##### REDIS缓存穿透\n\n缓存穿透是指要访问的某个数据既不在redis中,也不在数据库中,导致访问缓存时发生缓存缺失,访问数据库也得不到数据也就无法缓存.如果应用持续有大量请求访问这种数据,就会给redis和数据库造成巨大压力。\n\n缓存穿透一般发生在业务层误操作删除了缓存和数据库数据或者恶意攻击上。\n\n一般有三种解决方案:一种是发生缓存穿透时,针对查询的数据在redis中缓存一个空值或者和业务人员商定一个缺省值,请求直接在缓存里处理;一种是前端对请求的参数进行检测,过滤一些参数不合理\\非法请求值\\字段不存在等情况的请求;还有一种就是常见的使用布隆过滤器,它由一个初值都是0的bit数组和n个哈希函数组成,可以快速判断某个数据是否存在。我们可以在把数据写入数据库时，使用布隆过滤器做个标记。当缓存缺失后，应用查询数据库时，可以通过查询布隆过滤器快速判断数据是否存在。如果不存在，就不用再去数据库中查询了。\n\n\n\n##### Redis的持久化\n\nRDB 的优势和劣势\n\n①、优势\n\n（1）RDB文件紧凑，全量备份，非常适合用于进行备份和灾难恢复。\n\n（2）生成RDB文件的时候，redis主进程会fork()一个子进程来处理所有保存工作，主进程不需要进行任何磁盘IO操作。\n\n（3）RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。\n\n②、劣势\n\nRDB快照是一次全量备份，存储的是内存数据的二进制序列化形式，存储上非常紧凑。当进行快照持久化时，会开启一个子进程专门负责快照持久化，子进程会拥有父进程的内存数据，父进程修改内存子进程不会反应出来，所以在快照持久化期间修改的数据不会被保存，可能丢失数据。\n\nAOF 的优势和劣势\n\n①、优势\n\n（1）AOF可以更好的保护数据不丢失，一般AOF会每隔1秒，通过一个后台线程执行一次fsync操作，最多丢失1秒钟的数据。\n\n（2）AOF日志文件没有任何磁盘寻址的开销，写入性能非常高，文件不容易破损。\n\n（3）AOF日志文件即使过大的时候，出现后台重写操作，也不会影响客户端的读写。\n\n（4）AOF日志文件的命令通过非常可读的方式进行记录，这个特性非常适合做灾难性的误删除的紧急恢复。比如某人不小心用flushall命令清空了所有数据，只要这个时候后台rewrite还没有发生，那么就可以立即拷贝AOF文件，将最后一条flushall命令给删了，然后再将该AOF文件放回去，就可以通过恢复机制，自动恢复所有数据\n\n②、劣势\n\n（1）对于同一份数据来说，AOF日志文件通常比RDB数据快照文件更大\n\n（2）AOF开启后，支持的写QPS会比RDB支持的写QPS低，因为AOF一般会配置成每秒fsync一次日志文件，当然，每秒一次fsync，性能也还是很高的\n\n\n\n##### Redis实现消息队列\n\n1. 基于List 实现消息队列，在生产者往 List 中写入数据时，List 消息集合并不会主动地通知消费者有新消息写入。所以 Redis 提供了 brpop阻塞式读取，客户端在没有读到队列数据时，自动阻塞，直到有新的数据写入队列，再开始读取新数据。此外，消息队列通过给每一个消息提供全局唯一的 ID 号来解决分辨重复消息的需求。\n2. 基于Zset实现消息队列。\n3. 基于频道（channel）和基于模式（pattern）的发布/订阅。\n4. 基于Stream 实现消息队列。\n\n\n\n##### Redis淘汰策略\n\n淘汰策略是为了解决缓存写满的问题,redis提供了8种淘汰策略\n\n- 不进行数据淘汰\n- volatile-ttl根据过期时间的先后进行删除，越早过期的越先被删除。\n- volatile-random在设置了过期时间的键值对中，进行随机删除。\n- volatile-lru会使用LRU算法筛选设置了过期时间的键值对,更加关注数据的时效性.\n- volatile-lfu会使用LFU算法选择设置了过期时间的键值对,更加关注数据的访问频次.\n- allkeys-random策略，从所有键值对中随机选择并删除数据；\n- allkeys-lru策略，使用LRU算法在所有数据中进行筛选.\n- allkeys-lfu策略，使用LFU算法在所有数据中进行筛选.\n\n\n\n#### KAFKA\n\n##### KAFKA是什么\n\nApache Kafka 是消息引擎系统，也是一个分布式流处理平台.\n\n\n\n##### 为什么要使用KAFKA\n\n最最主要的用途是**\"削峰填谷\"**,所谓的“削峰填谷”就是指缓冲上下游瞬时突发流量，使其更平滑。另一个原因是在于发送方和接收方的松耦合，减少了系统间不必要的交互。\n\n\n\n##### KAFKA为什么写入磁盘快\n\n零拷贝，mmap和顺序写入\n\n先说零拷贝mmap，再补充顺序写入，服务器都是使用机械硬盘做数据盘，它的随机写入的寻址会比较消耗时间，KFAKA使用的顺序写入，顺序读写速度能和内存持平。kafka的消息都是append操作，partition是有序的，节省了磁盘的寻址时间。同时通过批量操作，节省写入次数。partition物理上分为多个segment存储，删除的效率也比较高。\n\n\n\n##### Kafka零拷贝\n\nkafka采用了零拷贝技术，传统的读取文件发送会有四步copy：\n\n1. 将磁盘文件读取到系统内核缓冲区\n2. 将内核缓冲区的数据copy到用户的缓冲区中\n3. 在应用程序中调用‘write()’方法，将用户空间的缓冲区数据copy到内核空间的Socket Buffer中\n4. 把内核模式下的Socket Buffer数据赋值到网卡缓冲区NIC Buffer，由网卡缓冲区再把数据传输到目标服务器上\n\n在这4次copy中，23步操作是浪费的，另外由于用户空间和内核空间的切换，会带来CPU的上下文切换，对CPU的性能也会造成影响。零拷贝通过DMA技术，把文件内容复制到内核空间的Read Buffer，接着把包含文件信息的描述符加载到Socket Buffer中，而不用经过应用程序所在的用户空间。这种操作减少了两次copy，并且减少了cpu的上下文切换，对于效率是有非常大的提高。\n\n这个零拷贝，Linux中是依赖于底层的sendfile()方法去实现的，在java中，FileChannal.transferTo()的底层实现就是sendfile()。\n\n此外还有一个叫mmap的文件映射机制，它的原理是把磁盘文件映射到内存，用户通过修改内存就可以修改磁盘文件，使用这个方式可以获得很大的I/O提升\n\n\n\n##### Kafka创建topic应该给多少个partition\n\n官方推荐限制在100 × broker × 副本数，滴滴给的最佳实践是单节点partition不超1000。\n\n但如果有对接计算引擎比如spark且有并发度的需求的话，这个要分业务需求和并发需求。\n\n从业务角度出发，默认情况下，当用客户端向某个topic灌数据时，如果没有指定消息的key和要写入的partition，那么数据会以round-robin的方式均匀写到topic的每个partition中。比如我的数据包含31个省，我会指定31个partition，写入的时候将省份缩写作为key进行写入，这样数据就会按照这个key进行hash然后跟partition的个数取模，最终进入特定的partition中。这样数据读到计算引擎后，因为分区数据跟计算引擎的分区数据一一对应，对数据进行聚合分组时，因为数据写入kafka时已经按业务分了组，当用同样的并行度来取数据时，此时的数据是天然按照省份分组的，因此避免了宽依赖的产生，而没有宽依赖就不会有shuffle，数据的处理性能大大提升。\n\n从并行度角度出发，可以指定为broker的数量。如果有计算引擎的话，可以指定为计算引擎并行的数量。当然比如spark可以通过reparation和coalesce这两个函数来修改你要想的并行度，但是这样将原本小的partition数改大，必定会导致宽依赖的产生，而宽依赖则一定会产生shuffle\n\n\n\n##### KAFKA丢数据吗\n\n我认为可以三个方面考虑和实现。\n\n首先是producer端，需要确保消息能够到达broker，但是由于网络波动等等导致消息发送失败，针对producer端有这么几种方案。\n\n1. producer默认是异步发送的，这里可以把异步改为同步发送，这样的话producer就能实时的知道发送的结果。\n\n2. 添加异步回调函数来监听消息发送的结果，如果发送失败，可以在回调中重试。\n3. producer本身提供了一个重试参数，retries，如果发送失败，producer会自动重试。\n\n然后是broker端，broker需要确保producer发送的数据是不丢失的，也就是落盘可以了，但是kafka为了提高性能，采用的异步批量刷盘的实现机制，按照一定的消息量和时间去刷盘，而最终刷新到磁盘的这个动作，是由操作系统来调度的，如果刷盘之前系统崩溃了，就会导致消息丢失。kafka并没有提供同步刷盘的一个机制，所以针对这个问题，需要通过partition的副本机制和ack机制来解决。ack提供了几个参数，ack=0，表示producer不需要等待broker响应就认为消息发送成功了，这种情况下会存在消息丢失。ack=1，表示broker中的leader partition收到消息后，不等待follower 的同步，就给producer返回确认，这种情况下，假如leader partition挂了，就会造成消息丢失。第三种ack=-1，表示leader partition收到消息后，并且等待ISR列表中所有follower partition同步完成，再给producer返回一个确认，这样的一个配置是可以保证数据的可靠性的。\n\n最后就是consumer必须能消费到这个消息，我认为只要producer和broker的消息得到保障，那么consumer不太可能出现消息无法消费的问题。除非是consumer没有消费完就已经提交了offset，但是即便是这种情况，我们也可以通过调整offset的值来实现重新消费。\n\n\n\n##### HW,LEO,LW,LSO名词解释\n\n**HW**:俗称高水位，它标识了一个特定的消息偏移量（offset），消费者只能拉取到这个offset之前的消息。\n\n\n\n##### 什么是ISR\n\nISR就是与leader保持同步的follower集合。当ISR中的follower完成数据的同步之后，leader就会给follower发送ack。如果follower长时间未向leader同步数据，则该follower将被踢出ISR，该时间阈值由replica.lag.time.max.ms参数设定。Leader发生故障之后，就会从ISR中选举新的leader。\n\n\n\n##### Kafka 分布式的情况下，如何保证消息的顺序?\n\n大多数业务不关注顺序,如果要保证有序的话,Kafka 分布式的单位是 Partition。\n\n- 同一个 Partition 用一个 write ahead log 组织，所以可以保证 先进先出(FIFO) 的顺序。\n- 不同 Partition 之间不能保证顺序。\n- Kafka 中发送1条消息的时候，可以指定(topic, partition, key) 3个参数。partiton 和 key 是可选的。如果你指定了 partition，那就是所有消息发往同1个 partition，就是有序的。并且在消费端，Kafka 保证，1个 partition 只能被1个 consumer 消费。或者你指定 key（比如 order id），具有同1个 key 的所有消息，会发往同1个 partition。也是有序的。\n\n##### Kafka分区策略\n\n**所谓分区策略是决定生产者将消息发送到哪个分区的算法。**\n\n* Round-robin轮询策略,也就是顺序分配,有非常好的负载均衡表现,总是能保证消息最大限度的平均分配到所有分区,他也是kafka默认的分区策略.\n* randomness随机策略,就是随意的把消息放置在任意的分区上\n* key-odering策略,相同key的消息进入相同的分区中\n\n\n\n##### Kafka 怎么实现**精确一次**(生产者方面)？\n\nKafka 默认是提供的至少一次,如果生产者已经成功提交,但是没有收到broker的确认反馈,他会选择重试,这就导致了消息重复发送.\n\nKafka也可以提供最多一次,也就是把producer的重试机制给关掉,但是这样一来有可能会丢失数据,\n\n于是kafka提供了两个方式实现精确一次.\n\n* 幂等性producer,底层思想就是用空间换时间,在broker端多存储一些字段,来知道消息是否重复.但是他只能实现单分区且单会话上的幂等性.\n* 还有一种是事务型producer,它能够保证将消息原子性地写入到多个分区中,也不怕进程的重启.但是,相对幂等性producer,它的性能更差一些.\n\n\n\n##### Kafka 怎么避免重复消费？\n\nbroker上存储的消息都有一个offset的标记，consumer通过offset维护当前消费的数据，默认5秒去自动提交消费完的数据，所以consumer在消费时，如果程序强制被kill或者宕机之类的，可能会导致offset没有提交，从而导致下次消费重复消费。\n\n还有就是kafka里面有partition balance的一个机制，就是把多个partition均衡的分配给多个消费者，consumer会从分配的partition中消费数据，如果consumer在默认5分钟内没有处理完这批消息，就会触发kafka的rebalance，从而导致offset自动提交失败，在重新rebalance之后，consumer还是会从之前没有提交的offset位置开始去消费，导致重复消费。这种情景下有很多种处理方法：比如\n\n1. 提高消费端的处理性能，比如使用异步的方式处理消息，缩短单个消息的消费时长，或者调整消费超时时间，或者减少一次从broker拉取的数据量\n\n \t2. 可以针对消息生成md5然后保存到redis里面，在处理消息前先判断是否已经消费过，如果存在就不在消费了，这个方法其实就是利用幂等性的思想来实现。\n\n\n\n#### 其他\n\n##### TCP三次握手\n\n**第一次握手**:\n\n建立连接时，客户端发送syn包（syn=j）到服务器，并进入SYN_SENT状态，等待服务器确认；SYN:同步序列编号（Synchronize Sequence Numbers）。\n\n**第二次握手**:\n\n服务器收到syn包，必须确认客户的SYN（ack=j+1），同时自己也发送一个SYN包（syn=k），即SYN+ACK包，此时服务器进入SYN_RECV状态；\n\n**第三次握手**:\n\n客户端收到服务器的SYN+ACK包，向服务器发送确认包ACK(ack=k+1），此包发送完毕，客户端和服务器进入ESTABLISHED（TCP连接成功）状态，完成三次握手。\n\n##### 4次挥手\n\n客户端打算关闭连接，此时会发送一个 TCP 首部 FIN 标志位被置为 1 的报文，也即 FIN 报文，之后客户端进入 FIN_WAIT_1 状态。\n\n服务端收到该报文后，就向客户端发送 ACK 应答报文，接着服务端进入 CLOSED_WAIT 状态。\n\n客户端收到服务端的 ACK 应答报文后，之后进入 FIN_WAIT_2 状态。\n\n等待服务端处理完数据后，也向客户端发送 FIN 报文，之后服务端进入 LAST_ACK 状态。\n\n客户端收到服务端的 FIN 报文后，回一个 ACK 应答报文，之后进入 TIME_WAIT 状态服务器收到了 ACK 应答报文后，就进入了 CLOSE 状态，至此服务端已经完成连接的关闭。\n\n客户端在经过 2MSL 一段时间后，自动进入 CLOSE 状态，至此客户端也完成连接的关闭\n\n\n\n##### 为什么连接的时候是三次握手，关闭的时候却是四次握手？\n\n因为当Server端收到Client端的SYN连接请求报文后，可以直接发送SYN+ACK报文。其中ACK报文是用来应答的，SYN报文是用来同步的。但是关闭连接时，当Server端收到FIN报文时，很可能并不会立即关闭SOCKET，所以只能先回复一个ACK报文，告诉Client端，\"你发的FIN报文我收到了\"。只有等到我Server端所有的报文都发送完了，我才能发送FIN报文，因此不能一起发送。故需要四步握手。\n\n\n\n##### TCP 和 UDP 区别:\n\n**1. 连接**\n\nTCP 是面向连接的传输层协议，传输数据前先要建立连接。UDP 是不需要连接，即刻传输数据。\n\n**2. 服务对象**\n\nTCP 是一对一的两点服务，即一条连接只有两个端点。UDP 支持一对一、一对多、多对多的交互通信\n\n**3. 可靠性**\n\nTCP 是可靠交付数据的，数据可以无差错、不丢失、不重复、按需到达。UDP 是尽最大努力交付，不保证可靠交付数据。\n\n**4. 拥塞控制、流量控制**\n\nTCP 有拥塞控制和流量控制机制，保证数据传输的安全性。UDP 则没有，即使网络非常拥堵了，也不会影响 UDP 的发送速率。\n\n**5. 首部开销**\n\nTCP 首部长度较长，会有一定的开销，首部在没有使用「选项」字段时是 20 个字节，如果使用了「选项」字段则会变长的。UDP 首部只有 8 个字节，并且是固定不变的，开销较小。\n\n\n\n##### TCP 和 UDP 应用场景:\n\n由于 TCP 是面向连接，能保证数据的可靠性交付，因此经常用于:\n\nFTP 文件传输HTTP / HTTPS\n\n由于 UDP 面向无连接，它可以随时发送数据，再加上UDP本身的处理既简单又高效，因此经常用于:\n\n包总量较少的通信，如 DNS 、SNMP 等视频、音频等多媒体通信广播通信\n\n\n\n##### 事务特性\n\n1. 原子性 （atomicity）:强调事务的不可分割.\n2. 一致性 （consistency）:事务的执行的前后数据的完整性保持一致.\n3. 隔离性 （isolation）:一个事务执行的过程中,不应该受到其他事务的干扰\n4. 持久性（durability） :事务一旦结束,数据就持久到数据库\n\n\n\n##### 事务隔离级别\n\n**1、DEFAULT**\n\n默认隔离级别，每种数据库支持的事务隔离级别不一样\n\n**Mysql 默认:可重复读**\n**Oracle 默认:读已提交**\n\n**2、READ_UNCOMMITTED**\n\n读未提交，是最低的事务隔离级别，它允许另外一个事务可以看到这个事务未提交的数据，这个级别的隔离机制无法解决脏读、不可重复读、幻读中的任何一种，因此很少使用\n\n**3、READ_COMMITED**\n\n读已提交，保证一个事物提交后才能被另外一个事务读取，自然能够防止脏读，但是无法限制不可重复读和幻读\n\n**4、REPEATABLE_READ**\n\n可重复度，读取了一条数据，这个事务不结束，别的事务就不可以改这条记录，这样就解决了脏读、不可重复读的问题，但是幻读的问题还是无法解决\n\n**5、SERLALIZABLE**\n\n串行化，这是花费最高代价但最可靠的事务隔离级别，就解决了脏读、不可重复读和幻读的问题了\n\n##### 拦截器和过滤器\n\n　　**①拦截器是基于java的反射机制的，而过滤器是基于函数回调。\n　　②拦截器不依赖与servlet容器，过滤器依赖与servlet容器。\n　　③拦截器只能对action请求起作用，而过滤器则可以对几乎所有的请求起作用。\n　　④拦截器可以访问action上下文、值栈里的对象，而过滤器不能访问。\n　　⑤在action的生命周期中，拦截器可以多次被调用，而过滤器只能在容器初始化时被调用一次。**\n\n　　**⑥拦截器可以获取IOC容器中的各个bean，而过滤器就不行，这点很重要，在拦截器里注入一个service，可以调用业务逻辑。**\n\n\n\n4. \n\n#### JAVA\n\n##### 单例\n\n![image-20200820141522766](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594faly1gi2xp2nqhbj20dc09jgls.jpg)\n\n\n\n##### ArrayList和LinkedList的区别\n\nArrayList，数组是一种线性表结构。它用一组连续的内存空间，来存储相同类型的数据。最大的特点就是随机访问快，但是插入数据和删除数据效率低，因为插入或删除数据时，待插入或删除位置的元素和他后面的所有元素都需要向后或向前搬移。数组扩容的话，需要把旧数组中的所有元素向新数组中搬移。数组的空间是从栈分配的\n\nLinkedList，链表它并不需要一块连续的内存空间，它的元素有两个属性，一个是元素的值，另一个是指针，此指针标记了下一个元素的地址．通过该地址就可以找到下一个数据．所以任意位置插入元素和删除元素时间效率较高．但是由于其不具有随机访问性，如果需要访问某个位置的数据，需要从第一个数开始找起，依次往后遍历，直到找到待查询的位置． 空间不需要提前指定大小，根据需求动态的申请和删除内存空间，扩容方便．链表的空间是从堆中分配的。\n\n\n\n##### 字节流和字符流区别\n\n字节流操作的基本单元为字节；字符流操作的基本单元为Unicode码元。\n字节流默认不使用缓冲区；字符流使用缓冲区。\n字节流在操作的时候本身是不会用到缓冲区的，是与文件本身直接操作的，所以字节流在操作文件时，即使不关闭资源，文件也能输出；字符流在操作的时候是使用到缓冲区的。如果字符流不调用close或flush方法，则不会输出任何内容。\n字节流可用于任何类型的对象，包括二进制对象，而字符流只能处理字符或者字符串； 字节流提供了处理任何类型的IO操作的功能，但它不能直接处理Unicode字符，而字符流就可以。\n\n\n\n##### HashSet和HashMap的区别\n\n| *HashMap*                                   | *HashSet*                                                    |\n| ------------------------------------------- | ------------------------------------------------------------ |\n| HashMap实现了Map接口                        | HashSet实现了Set接口                                         |\n| HashMap储存键值对                           | HashSet仅仅存储对象                                          |\n| 使用put()方法将元素放入map中                | 使用add()方法将元素放入set中                                 |\n| HashMap中使用键对象来计算hashcode值         | HashSet使用成员对象来计算hashcode值，对于两个对象来说hashcode可能相同，所以equals()方法用来判断对象的相等性，如果两个对象不同的话，那么返回false |\n| HashMap比较快，因为是使用唯一的键来获取对象 | HashSet较HashMap来说比较慢                                   |\n\n\n\n##### 介绍一下HashMap\n\nHashMap在1.8之前是数组+链表的结构,put数据时,key通过hash算法和取模获取数组下标,如果下标为空则把数据封装为entry对象放入.对于hash冲突,hashmap采用的是链式寻址法,将key计算结果相同的数据以单向链表的形式存储,在链表比较长的时候检索效率会降低.在扩容方面因为使用的是头插法,可能会造成循环链表的问题.\n\n而到了jdk1.8,HashMap采用了数组+链表+红黑树的结构,对于hash冲突,当链表深度超过8的时候会将链表转为红黑树,此外,扩容方面改为尾插法,解决了循环链表的问题.\n\n##### 说一下HashMap的put方法\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594faly1gryxyyhw12j211h0ayjwj.jpg)\n\n##### HashMap扩容\n\njdk1.7是数组加链表的结构，他的数据节点是一个entry节点。它插入使用的头插法，在扩容的过程中，也就是resize的时候，又调用了transfer方法，把里面的entry进行rehash，在这个过程，可能会造成可能导致的循环链表，在下次get的时候出现一个死循环。\n\njdk1.8的话HashMap默认采用数组+链表的方式存储键值对，entry节点改为了node节点，当链表深度超过8且数组超过64，会执行转换红黑树的操作，以减少搜索时间。否则，就是只是执行 resize() 方法对数组扩容。\n\n##### ConcurrentHashMap 的存储结构是怎样的？\n\nJava8 中的 ConcurrnetHashMap 使用的 Synchronized 锁加 CAS 的机制。结构也由 Java7 中的 Segment 数组 + HashEntry 数组 + 链表 进化成了 Node 数组 + 链表 / 红黑树，Node 是类似于一个 HashEntry 的结构。它的冲突再达到一定大小时会转化成红黑树，在冲突小于一定数量时又退回链表。\n\n\n\n##### 集合是否有序\n\n###### 无序\n\nHASHSET\n\nHASHMAP\n\n###### 有序\n\nLinkedHashSet添加的顺序\n\nTreeSet自然顺序a-z排列\n\nTREEMAP自然顺序a-z排列【底层存储结构是二叉树，二叉树的中序遍历保证了数据的有序性】\n\nLinkedHashMap添加的顺序【底层存储结构是哈希表+链表，链表记录了添加数据的顺序】\n\n\n\n##### 集合线程安全\n\n###### Map\n\nHashtable\tHashtable就是直接在hashmap上加了个锁\n\nConcurrentHashMap\tConcurrenthashmap1.8之前就是分成多个分段锁，JDK 1.8中直接采用CAS + synchronized保证并发更新的安全性，底层采用数组+链表+红黑树的存储结构\n\n###### Set\n\nHashSet、TreeSet、LinkedHashSet.这三个都是线程不安全的。它底层其实就是map，hashmap……\n\nCopyOnWriteArraySet \t就是使用 CopyOnWriteArrayList 的 addIfAbsent 方法来去重的，添加元素的时候判断对象是否已经存在，不存在才添加进集合。\n\n###### List\n\narraylist和linkedlist不安全\n\nvector\t因为它内部主要使用synchronized关键字实现同步\n\nSynchronizedList\t很可惜，它所有方法都是带同步对象锁的，和 Vector 一样，它不是性能最优的\n\nCopyOnWriteArrayList\t即复制再写入，就是在添加元素的时候，先把原 List 列表复制一份，再添加新的元素。添加元素时，先加锁，再进行复制替换操作，最后再释放锁。获取元素并没有加锁，在高并发情况下，大大提升了读取性能\n\n\n\n##### JVM内存模型\n\n![](https://raw.githubusercontent.com/chiyuanbo/pic/master/360b8f453e016cb641208a6a8fb589bc.png)\n\n私有区\n\n* 程序计数器:\n\n  程序计数器会存储当前线程正在执行的 Java 方法的 JVM 指令地址；或者，如果是在执行本地方法，则是undefined\n\n* Java 虚拟机栈:\n\n  每个线程在创建时都会创建一个虚拟机栈，其内部保存一个个的栈帧，对应着一次次的 Java 方法调用。\n\n* 本地方法栈\n\n  它和 Java 虚拟机栈是非常相似的，支持对本地方法的调用，也是每个线程都会创建一个。在 Oracle Hotspot JVM 中，本地方法栈和 Java 虚拟机栈是在同一块儿区域，这完全取决于技术实现的决定，并未在规范中强制。\n\n共享区\n\n* 堆:\n\n  它是 Java 内存管理的核心区域，用来放置 Java 对象实例，几乎所有创建的 Java 对象实例都是被直接分配在堆上。堆也是垃圾收集器重点照顾的区域，所以堆内空间还会被不同的垃圾收集器进行进一步的细分，最有名的就是新生代、老年代的划分。\n\n* 方法区:\n\n  用于存储所谓的元数据，例如类结构信息，以及对应的运行时常量池、字段、方法代码等。\n\n* 运行时常量池:\n\n  方法区的一部分，存放各种常量信息。\n\n##### 谈谈 JVM 内存区域的划分，哪些区域可能发生 OutOfMemoryError？\n\njavadoc 中对OutOfMemoryError的解释是，没有空闲内存，并且垃圾收集器也无法提供更多内存。这里面隐含着一层意思是，在抛出 OutOfMemoryError 之前，通常垃圾收集器会被触发，尽其所能去清理出空间。\n\n堆内存不足是最常见的 OOM 原因之一，抛出的错误信息是“java.lang.OutOfMemoryError:Java heap space”，原因可能千奇百怪，例如，可能存在内存泄漏问题；也很有可能就是堆的大小不合理。\n\n对于 Java 虚拟机栈和本地方法栈。如果不断的进行递归调用，而且没有退出条件，就会导致不断地进行压栈。JVM 实际会抛出 StackOverFlowError；当然，如果 JVM 试图去扩展栈空间的的时候失败，则会抛出 OutOfMemoryError。\n\n对于老版本的 Oracle JDK，因为永久代的大小是有限的，内存溢出会抛出“java.lang.OutOfMemoryError: PermGen space”。但是我们都是用的jdk1.8起步。\n\n随着元数据区的引入，方法区内存已经不再那么窘迫，所以相应的 OOM 有所改观，出现 OOM，异常信息则变成了:“java.lang.OutOfMemoryError: Metaspace”。\n\n##### 如何解决OOM\n\n top找出占用cpu最高的进程，记下id，用printf '0x%x' tid，线程id转换16进制，然后jstack pid|grep tid找到线程堆栈，找到报错信息\n\n开源的脚本，show-busy-java-thread\n\njvm里的工具jvisualvm\n\n##### 垃圾收集器\n\n* Serial GC，它是最古老的垃圾收集器，其收集工作是单线程的，并且在进行垃圾收集过程中，会进入臭名昭著的“Stop-The-World”状态。\n* ParNew GC，实际是 Serial GC 的多线程版本。\n* CMS（Concurrent Mark Sweep） GC，基于标记 - 清除（Mark-Sweep）算法，设计目标是尽量减少停顿时间。但是，CMS 采用的标记 - 清除算法，存在着内存碎片化问题，长时间运行 难免发生 full GC。另外，既然强调了并发，CMS 会占用更多 CPU 资源，并和用户线程争抢。\n* Parallel GC，在早期 JDK 8 等版本中，它是 server 模式 JVM 的默认 GC 选择，也被称作是吞吐量优先的 GC，其特点是新生代和老年代 GC 都是并行进行的。\n* G1 GC Garbage-First，意为垃圾优先，哪一块的垃圾最多就优先清理它。这是一种兼顾吞吐量和停顿时间的 GC 实现，是 Oracle JDK 9 以后的默认 GC 选项。\n\n\n\n##### jvm调优\n\n* 垃圾回收器的调优\n\n  1,首先通过printgcdetail 查看fullgc频率以及时长\n  2,通过dump 查看内存中哪些对象多，这些可能是引起fullgc的原因，看是否能优化\n  3,如果堆大或者是生产环境，可以开起jmc 飞行一段时间，查看这期间的相关数据来订位问题\n\n* 参数调优 堆空间大小，gc选择等等\n\n* 其他的忘了，因为我觉得那是jvm工程师做的事。\n\n  \n\n##### 多线程的实现方式\n\n继承Thread类并重写run()方法\n\n实现Runnable接口\n\n通过Callable和FutureTask创建线程\n\n通过线程池创建线程\n\n定时器（java.util.Timer）\n\njava8lambda表达式的parallelStream多管道\n\nSpring异步方法,启动类加上`@EnableAsync`,其次，方法加上`@Async`注解\n\n\n\n##### Excutor提供了哪几种线程池\n\n​\t\tExecutors 目前提供了 5 种不同的线程池创建配置:\n\n* newCachedThreadPool()，它是一种用来处理大量短时间工作任务的线程池，具有几个鲜明特点:它会试图缓存线程并重用，当无缓存线程可用时，就会创建新的工作线程；如果线程闲置的时间超过 60 秒，则被终止并移出缓存；长时间闲置时，这种线程池，不会消耗什么资源。其内部使用 SynchronousQueue 作为工作队列。\n* newFixedThreadPool(int nThreads)，重用指定数目（nThreads）的线程，其背后使用的是无界的工作队列，任何时候最多有 nThreads 个工作线程是活动的。这意味着，如果任务数量超过了活动队列数目，将在工作队列中等待空闲线程出现；如果有工作线程退出，将会有新的工作线程被创建，以补足指定的数目 nThreads。\n* newSingleThreadExecutor()，它的特点在于工作线程数目被限制为 1，操作一个无界的工作队列，所以它保证了所有任务的都是被顺序执行，最多会有一个任务处于活动状态，并且不允许使用者改动线程池实例，因此可以避免其改变线程数目。\n* newSingleThreadScheduledExecutor() 和 newScheduledThreadPool(int corePoolSize)，创建的是个 ScheduledExecutorService，可以进行定时或周期性的工作调度，区别在于单一工作线程还是多个工作线程。\n* newWorkStealingPool(int parallelism)，这是一个经常被人忽略的线程池，Java 8 才加入这个创建方法，其内部会构建ForkJoinPool，利用Work-Stealing算法，并行地处理任务，不保证处理顺序。\n\n\n\n##### Java线程池七个参数\n\ncorePoolSize 核心线程数\n\nmaximumPoolSize 最大线程数量\n\nkeepAliveTime 空闲线程存活时间\n\nunit 时间单位\n\nworkQueue 工作队列\n\nthreadFactory 线程工厂\n\nhandler 拒绝策略\n\n\n\n##### 线程池如果满了会怎么样？\n\n如果使用的是无界队列 LinkedBlockingQueue，继续添加任务到阻塞队列中等待执行，因为 LinkedBlockingQueue 可以近乎认为是一个无穷大的队列，可以无限存放任务 \n\n如果使用的是有界队列比如 ArrayBlockingQueue ， 任务首先会被添加到 ArrayBlockingQueue 中，ArrayBlockingQueue 满了，会根据最大线程数的值增加线程数量，如果增加了线程数量还是处理不过来， 那么则会使用拒绝策略 RejectedExecutionHandler 处理满了的任务，默认是\nAbortPolicy。\n\n##### 线程池的工作流程\n\n1. 线程池是一种池化技术，线程的实际调度执行是由操作系统做的。线程池刚创建时，里面并没有线程。只有调用execute()方法后，才会创建线程。\n2. 当调用execute()方法添加一个任务时，线程池会做以下判断：\n   1. 当正在运行的线程数量小于核心线程数，那么马上创建线程运行这个任务。\n   2. 如果线程数量大于等于核心线程数，那么会将任务放入队列。\n   3. 如果队列满了，且线程数量小于最大线程数，那么会创建非核心线程立刻运行这个任务。\n   4. 如果队列满了，且线程数量大于等于最大线程数，那么线程池会执行拒绝策略。\n3. 当一个线程完成任务时，他会从队列中取下一个任务来执行。\n4. 当一个线程无事可做，超过keepAliveTime时，线程池会判断，如果当前运行的线程数大于核心线程数，那么这个线程会被停掉。所以线程池的所有任务完成后，他最终会收缩到核心线程数的大小。\n\n##### 拒绝策略\n\n* AbortPolicy:丢弃任务并抛出RejectedExecutionException异常。\n* DiscardPolicy:静默丢弃任务，但是不抛出异常。\n* DiscardOldestPolicy:丢弃队列最前面的任务，然后重新提交被拒绝的任务。\n* CallerRunsPolicy:由调用线程处理该任务\n\n##### 线程的状态如何控制，等待、唤醒……\n\n\n\n##### 并发编程中，如何中断一个正在运行中的线程？\n\n线程是一个系统级的概念，在java里实现的线程最终的执行和调度都是由操作系统实现的。所以理论上中断一个线程只能像linux的kill命令杀死线程的方式一样去强制终止。thread里面提供了一个过时的stop方法可以去强制终止，但是这个方式不安全，可能这个线程的任务还没执行完。如果想安全的停止一个线程，只能在线程里面埋下一个钩子，外部线程通过这个钩子去触发线程的一个中断命令。thread里面提供了interrupt方法，结合在run方法里面使用if(this.isInterrupt())来实现线程的安全中断。这种方法并不是强制中断，而是告诉线程可以停止，是否要中断取决于正在运行的线程，所以它能保证线程运算结果的一个安全性。\n\n\n\n##### Synchronized锁升级的原理\n\njdk1.6之前Synchronized是通过重量级锁的方式来实现线程之间锁的竞争，它依赖与操作系统底层的mutex lock，会涉及到用户态到内核态的一个切换，性能损耗很大。在1.6之后，Synchronized增加了锁升级的一个机制，引入了偏向锁和轻量级锁。偏向锁就是把当前的某个锁偏向于某个线程，这种锁适合同一个线程多次申请一个锁资源，并且没有其他线程竞争的情况。轻量级锁也就是自旋锁，通过多次自旋去重试竞争锁，避免了用户态和内核态的切换损耗。访问Synchronized同步代码块时，首先使用偏向锁竞争锁资源，如果竞争到偏向锁说明加锁成功，直接返回，如果失败就升级为轻量级锁，线程会根据自适应自旋次数去尝试自旋占用锁资源，如果还没竞争到锁就会升级到重量级锁，没有竞争到锁的线程会被阻塞，处于锁等待状态。\n\n\n\n##### String和StringBuffer和StringBuilder\n\nString是不可变对象,StringBuffer和StringBuilder是可以追加的\n\nStringBuffer内部一些方法使用Synchronized修饰,所以他是线程安全的\n\nStringBuilder线程不安全,但相对的性能更高一点\n\n\n\n##### 用过哪些java自带注解\n\n@Override 表示当前方法覆盖了父类的方法\n@Deprecation 表示方法已经过时,方法上有横线，使用时会有警告。\n@SuppviseWarnings 表示关闭一些警告信息(通知java编译器忽略特定的编译警告)\n\n\n\n#### Spring\n\n##### 谈谈对spring的理解\n\n\n\n##### Spring用到了哪些设计模式\n\n1. 工厂设计模式 : Spring 使用工厂模式通过 BeanFactory、ApplicationContext 创建 bean 对象。\n2.  代理设计模式 : Spring AOP 功能的实现。 \n3. 单例设计模式 : Spring 中的 Bean 默认都是单例的。\n4.  模板方法模式 : Spring 中 jdbcTemplate、hibernateTemplate 等以 Template 结尾的 对数据库操作的类，它们就使用到了模板模式。 \n5. 包装器设计模式 : 我们的项目需要连接多个数据库，而且不同的客户在每次访问中根据需 要会去访问不同的数据库。这种模式让我们可以根据客户的需求能够动态切换不同的数据 源。 \n6. 观察者模式: Spring 事件驱动模型就是观察者模式很经典的一个应用。 \n7. 适配器模式 : Spring AOP 的增强或通知(Advice)使用到了适配器模式、spring MVC 中 也是用到了适配器模式适配 Controller。\n\n\n\n##### @Component 和 @Bean 的区别是什么？\n\n1. 作用对象不同：`@Component` 注解作用于类，而 `@Bean` 注解作用于方法\n2. `@Component` 通常是通过路径扫描来自动侦测以及自动装配到 Spring 容器中。`@Bean` 注解通常是我们在标有该注解的方法中定义产生这个 bean，`@Bean` 告诉了 Spring 这是某个类的实例，当我们需要用它的时候还给我。\n3. `@Bean` 注解比 `@Component` 注解的自定义性更强，而且很多地方我们只能通过 `@Bean` 注解来注册 bean。比如当我们引用第三方库中的类需要装配到 Spring 容器时，只能通过 `@Bean` 来实现\n\n\n\n##### spring的作用域\n\nSpring支持5种作用域:\n\n- singleton:单例模式,bean以单实例的形式存在\n- prototype:原型模式,每次调用getBean方法都会返回一个新实例\n- request:对于每次HTTP请求，使用request定义的Bean都将产生一个新实例，该作用域仅适用于WebApplicationContext环境\n- session:同一个http session共享一个bean\n- globalsession:全局会话,和session类似,但是只在基于portlet的web中有效。\n\n\n\n##### Spring IOC Bean的生命周期\n\n单实例的生命周期是容器启动时初始化,容器关闭时销毁\n\n多实例的生命周期是容器启动后调用getBean时初始化,容器关闭时并不执行销毁方法,需要写程序销毁\n\n\n\n##### spring中一条请求100ms，如何1s处理1000条请求\n\n\n\n\n\n##### Spring注入方式都有哪些\n\n1. set 方法注入\n2. 构造器注入\n3. 静态工厂的方法注入\n4. 实例工厂的方法注入\n5. @Autowired自动装配,但是现在不推荐使用了\n\n##### 为什么不推荐使用@Autowired注入\n\n1. 初始化顺序,Autowired是在构造方法之后的,如果构造方法里使用了这个bean,就会出现空指针异常\n2. Autowired是byType方式,注入两个相同类型的bean会失败\n\n\n\n##### @Autowired和@Resource区别\n\n1. 提供方不同,@Autowired 是Spring提供的，@Resource 是[J2EE](https://so.csdn.net/so/search?q=J2EE&spm=1001.2101.3001.7020)提供的。\n2. 装配时默认类型不同,@Autowired只按type装配,@Resource默认是按name装配。\n\n\n\n##### Spring的controller是单例吗\n\ncontroller默认是单例的，所以不能使用非静态的成员变量。如果必须要定义一个非静态成员变量，可以通过注解@Scope(“prototype”)，将其设置为多例模式，或者是使用ThreadLocal变量\n\n\n\n##### Spring自定义注解\n\n###### 流程:\n\n1. 在配置中打开aop编程\n\n2. 编写自己自定义的注解，这里需要使用\n\n   ​\t@Target表示注解作用的位置，一般是用ElemenetType.METHOD\n\n   ​\t@Retention表示注解的生命周期，一般是用RetentionPolicy.RUNTIME\n\n   ​\t@Documented:注解信                                                                                                                                                                                                                                                                                                                            ，使用@Aspect 注解标示该类为切面类以及@Component 注入依赖，我之前做日志记录标注该方法体为后置通知，当目标方法执行成功后执行该方法体  \n\n\n\n\n1、@Target 的 ElemenetType 参数包括:       \n\nElemenetType.CONSTRUCTOR  构造器声明       \n\nElemenetType.FIELD  域声明（包括 enum 实例）      \n\nElemenetType.LOCAL_VARIABLE  局部变量声明      \n\nElemenetType.METHOD  方法声明       \n\nElemenetType.PACKAGE  包声明       \n\nElemenetType.PARAMETER  参数声明       \n\nElemenetType.TYPE  类，接口（包括注解类型）或enum声明\n\n\n\n2、@Retention 表示在什么级别保存该注解信息。可选的 RetentionPolicy 参数包括:   \n\nRetentionPolicy.SOURCE  注解将被编译器丢弃      \n\nRetentionPolicy.CLASS  注解在class文件中可用，但会被VM丢弃      \n\nRetentionPolicy.RUNTIME  VM将在运行期也保留注释，因此可以通过反射机制读取注解的信息。\n\n\n\n##### IOC、AOP的理解以及都有哪些运用\n\nIOC控制反转,依赖注入.在使用spring之前是需要手动new出来的，是我们主动获取的。使用spring之后，是将这个获取的过程交给spring来管理，我们只需要告诉spring你需要什么就行了\n\nAOP就是面向切面编程.AOP适合于那些具有横切逻辑的应用:如性能监测，访问控制，事务管理、缓存、对象池管理以及日志记录\n\n\n\n##### Spring boot 事物\n\n事物注解:\n\n指定回滚\n\n@Transactional(rollbackFor=Exception.class) \n\n指定不回滚\n\n@Transactional(noRollbackFor=Exception.class)\n\n**事物传播行为 propagation(springboot默认值为Propagation.REQUIRED)**\n\n用法\n\n@Transactional(propagation=Propagation.REQUIRED)\n\n\n\n##### spring的七种事物传播行为\n\n1. REQUIRED       使用当前的事务，如果当前没有事务，就新建一个事务。这是默认的也是最常见的选择。\n2. REQUIRES_NEW  新建事务，如果当前存在事务，把当前事务挂起。\n3. SUPPORTS       支持当前事务，如果当前没有事务，就以非事务方式执行。\n4. NOT_SUPPORTED  以非事务方式执行，如果当前存在事务，就把当前事务挂起。\n5. MANDATORY  使用当前的事务，如果当前没有事务，就抛出异常。\n6. NEVER              以非事务方式执行，如果当前存在事务，则抛出异常。\n7. NESTED            如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与REQUIRED类似的操作。\n\n\n\n##### Spring异步编程\n\n1. 新建配置类，使用注解@EnableAsync开启异步支持，\n2. 因为异步默认的使用的线程池不是真的线程池，他不会重用线程，所以一般是自定义线程池ThreadPoolTaskExecutor，并使用@bean注解注入\n3. 在需要异步的方法上使用@Async使用异步调用\n\n\n\n##### Spring 注解之@RestController与@Controller的区别\n\n@RestController是@Controller的衍生注解,他等价于@Controller+@ResponseBody。他俩的共同点都是标识一个类能否接收http请求，但是@RestController是直接返回数据，无法返回指定页面，此时配置的视图解析器不起作用，而@Controller是直接返回指定页面，如果要返回数据需要借助@ResponseBody\n\n\n\n#### Mybatis\n\n\n##### MYBATIS多数据源配置\n\napplication.xml里配置多种数据源，我们应用的时候是通过自定义注解，注意是应用了spring aop来设置，把数据源都设置为注解标签，在service层中需要切换数据源的方法上，写上注解标签，调用相应方法切换数据源\n\n\n\n##### mybatis一级缓存，二级缓存\n\nmybatis的的一级缓存是SqlSession级别的缓存，一级缓存缓存的是对象，当SqlSession提交、关闭以及其他的更新数据库的操作发生后，一级缓存就会清空。\n\n二级缓存是Application/SqlSessionFactory级别的缓存，同一个SqlSessionFactory产生的SqlSession都共享一个二级缓存，二级缓存中存储的是数据，当命中二级缓存时，通过存储的数据构造对象返回。\n\n查询数据的时候，查询的流程是二级缓存>一级缓存>数据库。\n\n\n\n##### Mybatis的工作原理\n\n1. 读取核心配置文件mybatis-config.xml并返回`InputStream`流对象。\n2. 根据`InputStream`流对象解析出`Configuration`对象，`Configuration`对象的组织结构和XML文件的组织结构几乎完全一样，然后创建`SqlSessionFactory`\n3. 根据一系列属性从`SqlSessionFactory`工厂中创建`SqlSession`\n4. 从`SqlSession`中调用`Executor`执行数据库操作以及生成具体SQL指令\n5. 然后对执行结果进行二次封装\n6. 最后提交与事务\n\n\n\n##### 如何写出安全的代码\n\n* 在早期设计阶段，就由安全专家组对新特性进行风险评估。\n* 开发过程中，尤其是 code review 阶段，应用 OpenJDK 自身定制的代码规范。\n* 利用多种静态分析工具如FindBugs、Parfait等，帮助早期发现潜在安全风险，并对相应问题采取零容忍态度，强制要求解决。\n* 甚至 OpenJDK 会默认将任何（编译等）警告，都当作错误对待，并体现在 CI 流程中。在代码 check-in 等关键环节，利用 hook 机制去调用规则检查工具，以保证不合规代码不能进入代码库。\n\n#### Hadoop\n\n##### HDFS写原理\n\n1. 客户端提交写请求到NameNode，NameNode收到后对客户端进行鉴权，权限ok后会将合适的DataNode节点信息返回给客户端\n2. 客户端拿到DataNode信息后，会直接和DataNode进行交互，进行数据写入。由于数据库具有副本replication，在数据写入时是先写入第一个副本，写完后再从第一个副本的节点把数据拷贝到其他节点，依次类推，知道所有副本都写完，才算成功。副本写入采用的是串行，每个副本写的过程中会逐级向上反馈进度，以保证实时知道副本的写入进度。\n3. 所有副本写完后，客户端会受到数据节点返回的成功状态，然后关闭与DateNode的通道，并告诉NameNode写入完成。\n\n\n\n##### HDFS读原理\n\n1. 客户端访问NameNode，查询元数据信息，获得这个文件的数据块位置列表，返回输入流对象。\n2. 就近挑选一台datanode服务器，请求建立输入流 。\n3. DataNode向输入流中中写数据，以packet为单位来校验。\n4. 最后关闭输入流\n\n##### HDFS通信原理\n\n\n\n##### MapTask和ReduceTask工作机制/MapReduce工作原理\n\n**MapTask**\n\n1. Read阶段：Map Task通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value。\n2. Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value。\n3. Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区（调用Partitioner），并写入一个环形内存缓冲区中。\n4. Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。\n5. Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。\n\n**ReduceTask**\n\n1. Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。\n2. Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。\n3. Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。\n4. Reduce阶段：reduce()函数将计算结果写到HDFS上。\n\n\n\n#### HIVE\n\n\n\n##### hive3大执行引擎区别在哪\n\n* mr:\n\n  MR将一个算法抽象成Map和Reduce两个阶段进行处理，如果一个HQL经过转化可能有多个job，那么在这中间文件就有多次落盘，速度较慢。\n\n* tez:\n\n  该引擎核心思想是将Map和Reduce两个操作进一步拆分，即Map被拆分成Input、Processor、Sort、Merge和Output，Reduce被拆分成Input、Processor、Sort、Merge、Output和Shuffle等，这样，这些分解后的元操作可以任意灵活组合，产生新的操作，这些操作经过一些控制程序组装后，可形成一个大的DAG作业。Tez可以将多个有依赖的作业转换为一个作业（这样只需写一次HDFS，且中间节点较少），从而大大提升DAG作业的性能\n\n* spark:\n\n  Spark是一个分布式的内存计算框架，其特点是能处理大规模数据，计算速度快。\n\n  Spark的计算过程保持在内存中，减少了硬盘读写，能够将多个操作进行合并后计算，因此提升了计算速度。同时Spark也提供了更丰富的计算API，例如filter，flatMap，count，distinct等。\n\n  过程间耦合度低，单个过程的失败后可以重新计算，而不会导致整体失败；\n\n##### HIVE sort by 和 order by 的区别\n\n* order by 会对输入做全局排序，因此只有一个reducer\n* sort by只保证每个reducer的输出有序，不保证全局有序\n\n##### 分区表和分桶表各自的优点能介绍一下吗？\n\n- 分区表\n  - 分区使用的是表外字段，需要指定字段类型\n  - 分区通过关键字partitioned by(partition_name string)声明\n  - 分区划分粒度较粗\n  -  优点:将数据按区域划分开，查询时不用扫描无关的数据，加快查询速度\n- 分桶表\n  - 分桶使用的是表内字段，已经知道字段类型，不需要再指定。\n  - 分桶表通过关键字clustered by(column_name) into … buckets声明\n  - 分桶是更细粒度的划分、管理数据，可以对表进行先分区再分桶的划分策略\n  -  优点:数据取样；能够起到优化加速的作用\n\n##### 了解过动态分区吗，它和静态分区的区别是什么？能简单讲下动态分区的底层原理吗？\n\n- 静态分区与动态分区的主要区别在于静态分区是手动指定，而动态分区是通过数据来进行判断\n- 详细来说，静态分区的列是在编译时期，通过用户传递来决定的；动态分区只有在 SQL 执行时才能决定\n- 简单理解就是静态分区是只给固定的值，动态分区是基于查询参数的位置去推断分区的名称，从而建立分区\n\n##### HIVE分桶的逻辑\n\n对分桶字段求哈希值，用哈希值与分桶的数量取余，余几，这个数据就放在那个桶内。\n\n##### HIVE索引\n\nHive的索引其实是一张索引表（Hive的物理表），在表里面存储索引列的值，该值对应的HDFS的文件路径，该值在数据文件中的偏移量。\n\n当Hive通过索引列执行查询时，首先通过一个MR Job去查询索引表，根据索引列的过滤条件，查询出该索引列值对应的HDFS文件目录及偏移量，并且把这些数据输出到HDFS的一个文件中，然后再根据这个文件中去筛选原文件，作为查询Job的输入。\n\n##### HIVE 数据倾斜怎么解决\n\n数据倾斜问题主要有以下几种:\n\n1. 空值引发的数据倾斜:不让null值参与join操作，也就是不让null值有shuffle阶段,或者是给null值随机赋值，这样它们的hash结果就不一样，就会进到不同的reduce中：\n2. 不同数据类型引发的数据倾斜:统一join关联字段数据类型或者转格式\n3. 不可拆分大文件引发的数据倾斜:大多数是数据压缩选择了不可分割的压缩算法,可以选snappy或zstd这种支持分割的算法\n4. 数据膨胀引发的数据倾斜\n5. 表连接时引发的数据倾斜\n\n#####  Hive优化有哪些\n\n1. 数据存储及压缩:\n\n   针对hive中表的存储格式通常有orc和parquet，压缩格式一般使用snappy。相比与textfile格式表，orc占有更少的存储。因为hive底层使用MR计算架构，数据流是hdfs到磁盘再到hdfs，而且会有很多次，所以使用orc数据格式和snappy压缩策略可以降低IO读写，还能降低网络传输量，这样在一定程度上可以节省存储，还能提升hql任务执行效率；\n\n2. 通过调参优化:\n\n   并行执行，调节parallel参数；\n\n   调节jvm参数，重用jvm；\n\n   设置map、reduce的参数；开启strict mode模式；\n\n   关闭推测执行设置。\n\n3. 有效地减小数据集将大表拆分成子表；结合使用外部表和分区表。\n\n4. SQL优化\n\n   1. 大表对大表:尽量减少数据集，可以通过分区表，避免扫描全表或者全字段；\n   2. 大表对小表:设置自动识别小表，将小表放入内存中去执行。\n\n   \n\n##### HIVE的优缺点\n\n\n\n##### HIVE存储数据原理\n\n\n\n#### HBASE\n\n\n\n##### 为什么Hbase支持实时查询\n\n首先数据量很大的时候，HBase会拆分成多个Region分配到多台RegionServer.\n客户端通过meta信息定位到某台RegionServer（也可能是多台）,\n通过Rowkey定位Region，这当中会先经过BlockCache，这边找不到的话，再经过MemStore和Hfile查询，这当中通过布隆过滤器过滤掉一些不需要查询的HFile。\n\n\n\n##### Hbase的两种缓存\n\nHBase在实现中提供了两种缓存结构：**MemStore**和**BlockCache**。\n**MemStore**\n1、其中MemStore称为写缓存\n2、HBase执行写操作首先会将数据顺序写入HLog然后写入MemStore\n3、等满足一定条件后统一将MemStore中数据异步刷盘，这种设计可以极大地提升HBase的写性能。\n4、MemStore对于读性能也至关重要，假如没有MemStore，读取刚写入的数据就需要从文件中通过IO查找，这种代价很昂贵\n\n**BlockCache**\n1、BlockCache称为读缓存\n2、HBase会将一次文件查找的Block块缓存到Cache中，以便后续同一请求或者邻近数据查找请求，可以直接从内存中获取，避免昂贵的IO操作。\n\n\n\n##### HLOG\n\nWAL 意为Write ahead log，类似 mysql 中的 binlog,用来 做灾难恢复时用，Hlog记录数据的所有变更,一旦数据修改，就可以从log中进行恢复。\n\n**每个Region Server维护一个Hlog,而不是每个Region一个**。这样不同region(来自不同table)的日志会混在一起，这样做的目的是不断追加单个文件相对于同时写多个文件而言，可以减少磁盘寻址次数，**因此可以提高对table的写性能**。带来的麻烦是，如果一台region server下线，为了**恢复其上的region，需要将region server上的log进行拆分**，然后分发到其它region server上进行恢复。\n\n\n\n##### Hbase读请求过程\n\nHRegionServer保存着meta表以及表数据，要访问表数据，首先Client先去访问zookeeper，从zookeeper里面获取meta表所在的位置信息，即找到这个meta表在哪个HRegionServer上保存着。\n\n接着Client通过刚才获取到的HRegionServer的IP来访问Meta表所在的HRegionServer，从而读取到Meta，进而获取到Meta表中存放的元数据。\n\nClient通过元数据中存储的信息，访问对应的HRegionServer，然后扫描所在HRegionServer的Memstore和Storefile来查询数据。\n\n最后HRegionServer把查询到的数据响应给Client。\n\n\n\n##### Hbase写请求过程\n\nClient也是先访问zookeeper，找到Meta表，并获取Meta表元数据。\n\n确定当前将要写入的数据所对应的HRegion和HRegionServer服务器。\n\nClient向该HRegionServer服务器发起写入数据请求，然后HRegionServer收到请求并响应。\n\nClient先把数据写入到HLog，以防止数据丢失。\n\n然后将数据写入到Memstore。\n\n如果HLog和Memstore均写入成功，则这条数据写入成功\n\n如果Memstore达到阈值，会把Memstore中的数据flush到Storefile中。\n\n当Storefile越来越多，会触发Compact合并操作，把过多的Storefile合并成一个大的Storefile。\n\n当Storefile越来越大，Region也会越来越大，达到阈值后，会触发Split操作，将Region一分为二。\n\n\n\n##### HIVE和HBASE的区别及使用情景\n\nHive是建立于Hadoop之上的数据仓库，支持sql，将sql转换为mapreduce进行计算，适合做批量处理。\n\nHbase是一种Key/Value的Nosql数据库，它运行在hdfs之上，适合做实时计算。\n\nHive利用分区机制来加快查询，只适合数据插入和查询。\n\nHbase通过存储Key/Value来工作，支持增删改查所有操作。\n\nHive不支持事物，Hbase支持部分事物，Hbase强依赖于ZK，使用ZK管理元数据。\n\n\n\n##### \n\n#### ZK\n\n##### Zookeeper通信原理\n\n\n\n\n\n#### SPARK\n\n\n\n##### spark在Client与在cluster运行的区别\n\n主要区别是driver的运行的机器不同,client模式是运行在提交作业的机器上,而cluster模式是运行在集群的某一台机器上\n\n\n\n##### Spark为什么快，Spark SQL 一定比 Hive 快吗\n\nSpark SQL 比 Hadoop Hive 快，是有一定条件的，而且不是 Spark SQL 的引擎比 Hive 的引擎快，相反，Hive 的 HQL 引擎还比 Spark SQL 的引擎更快。其实，关键还是在于 Spark 本身快。\n\n1. 消除了冗余的 HDFS 读写: Hadoop 每次 shuffle 操作后，必须写到磁盘，而 Spark 在 shuffle 后不一定落盘，可以 cache 到内存中，以便迭代时使用。如果操作复杂，很多的 shufle 操作，那么 Hadoop 的读写 IO 时间会大大增加，也是 Hive 更慢的主要原因了。\n2. 消除了冗余的 MapReduce 阶段: Hadoop 的 shuffle 操作一定连着完整的 MapReduce 操作，冗余繁琐。而 Spark 基于 RDD 提供了丰富的算子操作，且 reduce 操作产生 shuffle 数据，可以缓存在内存中。\n3. JVM 的优化: Hadoop 每次 MapReduce 操作，启动一个 Task 便会启动一次 JVM，基于进程的操作。而 Spark 每次 MapReduce 操作是基于线程的，只在启动 Executor 是启动一次 JVM，内存的 Task 操作是在线程复用的。每次启动 JVM 的时间可能就需要几秒甚至十几秒，那么当 Task 多了，这个时间 Hadoop 不知道比 Spark 慢了多少。\n\n##### 如何避免shuffle\n\nshuffle过程，就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作。\n\n\n\n##### RDD讲一下\n\nRDD 是 Spark 的计算模型。RDD（Resilient Distributed Dataset）叫做弹性的分布式数据集合，是 Spark 中最基本的数据抽象，它代表一个不可变、只读的，被分区的数据集。操作 RDD 就像操作本地集合一样，有很多的方法可以调用，使用方便，而无需关心底层的调度细节。\n\n\n\n#### 算法\n\n##### 二叉树写冒泡【升级红黑树】\n\n\n\n","tags":["面试"]},{"title":"vba合并sheet","url":"/2022/09/07/cyb-mds/vba/合并sheet/","content":"\n==作者：YB-Chi==\n\n```vb\n'功能：把多个excel工作簿的第一个sheet工作表合并到一个excel工作簿的多个sheet工作表，新工作表的名称等于原工作簿的名称\nSub Books2Sheets()\n\n'定义对话框变量\nDim fd As FileDialog\nSet fd = Application.FileDialog(msoFileDialogFilePicker)\n'新建一个工作簿\nDim newwb As Workbook\nSet newwb = Workbooks.Add\nWith fd\nIf .Show = -1 Then\n'定义单个文件变量\nDim vrtSelectedItem As Variant\n'定义循环变量\nDim i As Integer\ni = 1\n'开始文件检索\nFor Each vrtSelectedItem In .SelectedItems\n'打开被合并工作簿\nDim tempwb As Workbook\nSet tempwb = Workbooks.Open(vrtSelectedItem)\n'复制工作表\ntempwb.Worksheets(1).Copy Before:=newwb.Worksheets(i)\n'把新工作簿的工作表名字改成被复制工作簿文件名，这儿应用于xls文件，即Excel97-2003的文件，如果是Excel2007，需要改成xlsx\nnewwb.Worksheets(i).Name = VBA.Replace(tempwb.Name, \".csv\", \"\")\n'关闭被合并工作簿\ntempwb.Close SaveChanges:=False\ni = i + 1\nNext vrtSelectedItem\nEnd If\nEnd With\nSet fd = Nothing\nEnd Sub\n```\n\n\n\n","tags":["Vba"]},{"title":"README","url":"/2022/09/07/cyb-mds/README/","content":"\n笔记及收录"},{"title":"Redis-5-主从","url":"/2022/09/07/cyb-mds/module/Redis/Redis-5-主从/","content":"\n==作者：YB-Chi==\n\n\n\n那我们总说的Redis具有高可靠性，又是什么意思呢？其实，这里有两层含义：一是**数据尽量少丢失**，二是**服务尽量少中断**。AOF和RDB保证了前者，而对于后者，Redis的做法就是**增加副本冗余量**，将一份数据同时保存在多个实例上。即使有一个实例出现了故障，需要过一段时间才能恢复，其他实例也可以对外提供服务，不会影响业务使用。\n\nRedis提供了主从库模式，以保证数据副本的一致，主从库之间采用的是读写分离的方式。\n\n- **读操作**：主库、从库都可以接收；\n- **写操作**：首先到主库执行，然后，主库将写操作同步给从库。","tags":["redis"]},{"title":"Redis-9-GEO","url":"/2022/09/07/cyb-mds/module/Redis/Redis-9-GEO/","content":"\n==作者：YB-Chi==\n\n[toc]\n\nRedis提供了3种扩展数据类型，分别是Bitmap、HyperLogLog和GEO。前两种已经介绍过了，今天讲一讲GEO。\n\n### 面向LBS应用的GEO数据类型\n\n在日常生活中，我们越来越依赖搜索“附近的餐馆”、在打车软件上叫车，这些都离不开基于位置信息服务（Location-Based Service，LBS）的应用。LBS应用访问的数据是和人或物关联的一组经纬度信息，而且要能查询相邻的经纬度范围，GEO就非常适合应用在LBS服务的场景中，我们来看一下它的底层结构。\n\n#TODO 以后再补充  GEO用不到\n\n","tags":["redis"]},{"title":"数据结构-跳表","url":"/2022/09/07/cyb-mds/module/数据结构/数据结构-跳表/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n### 说明\n\nRedis中没有对跳表完全理解,特此补充一篇文章记录对跳表的学习.\n\n### 有序单链表\n\n下图是一个简单的**有序单链表**，单链表的特性就是每个元素存放下一个元素的引用。即：通过第一个元素可以找到第二个元素，通过第二个元素可以找到第三个元素，依次类推，直到找到最后一个元素。想快速找到下图链表中的 10 这个元素，只能从头开始遍历链表，直到找到我们需要找的元素。查找路径：1、3、4、5、7、8、9、10。这样的查找效率很低，平均时间复杂度很高O(n)。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4uhm5a2ifj20vq07gdht.jpg\" alt=\"image\" style=\"zoom:67%;\" />\n\n\n\n### 跳表\n\n我们从链表中每两个元素抽出来，加一级索引，一级索引指向了原始链表。查找时先在索引找 1、4、7、9，遍历到一级索引的 9 时，发现 9 的后继节点是 13，比 10 大，于是不往后找了，而是通过 9 找到原始链表的 9，然后再往后遍历找到了我们要找的 10，遍历结束。加了一级索引后，查找路径：1、4、7、9、10，查找节点需要遍历的元素相对少了，我们不需要对 10 之前的所有数据都遍历，查找的效率提升了。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4uk413qxlj20vq0diadz.jpg\" alt=\"image\" style=\"zoom: 67%;\" />\n\n那如果加二级索引呢？如下图所示，查找路径：1、7、9、10。是不是找 10 的效率更高了？这就是跳表的思想，用“空间换时间”，通过给链表建立索引，提高了查找的效率。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4uk6j1b26j20vq0if79b.jpg\" alt=\"image\" style=\"zoom:67%;\" />\n\n当元素数量较多时，索引提高的效率比较大，近似于二分查找。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4uk4pv1hxj20xc0aaq67.jpg\" alt=\"image\" style=\"zoom:67%;\" />\n\n到这里大家应该已经明白了什么是跳表。跳表是**可以实现二分查找的有序链表**。\n\n\n\n### 跳表的查找时间复杂度\n\n先来求跳表的索引高度。如下图所示，假设每两个结点会抽出一个结点作为上一级索引的结点，原始的链表有n个元素，则一级索引有n/2 个元素、二级索引有 n/4 个元素、k级索引就有 n/2k个元素。最高级索引一般有2个元素，即：最高级索引 h 满足 2 = n/2h，即 h = log2n - 1，最高级索引 h 为索引层的高度加上原始数据一层，跳表的总高度 h = log2n。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4uonla03sj20vq0c7wie.jpg\" alt=\"image\" style=\"zoom: 67%;\" />\n\n每级索引中都是两个结点抽出一个结点作为上一级索引的结点，每一层最多遍历3个结点。跳表的索引高度 h = log2n，且每层索引最多遍历 3 个元素。所以跳表中查找一个元素的时间复杂度为 O(3*logn)，省略常数即：O(logn)。\n\n\n\n### 跳表的空间复杂度\n\n假如原始链表包含 n 个元素，则一级索引元素个数为 n/2、二级索引元素个数为 n/4、三级索引元素个数为 n/8 以此类推。所以，索引节点的总和是：n/2 + n/4 + n/8 + … + 8 + 4 + 2 = n-2，**空间复杂度是 O(n)**。\n\n如下图所示：如果每三个结点抽一个结点做为索引，索引总和数就是 n/3 + n/9 + n/27 + … + 9 + 3 + 1= n/2，减少了一半。所以我们可以通过较少索引数来减少空间复杂度，但是相应的肯定会造成查找效率有一定下降，我们可以根据我们的应用场景来控制这个阈值，看我们更注重时间还是空间。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4up4yjto3j20vq0aijtd.jpg\" alt=\"image\" style=\"zoom:67%;\" />\n\nBut，索引结点往往只需要存储 key 和几个指针，并不需要存储完整的对象，所以当对象比索引结点大很多时，索引占用的额外空间就可以忽略了。举个例子：我们现在需要用跳表来给所有学生建索引，学生有很多属性：学号、姓名、性别、身份证号、年龄、家庭住址、身高、体重等。学生的各种属性只需要在原始链表中存储一份即可，我们只需要用学生的学号（int 类型的数据）建立索引，所以索引相对原始数据而言，占用的空间可以忽略。\n\n\n\n### 插入数据\n\n插入数据看起来也很简单，跳表的原始链表需要保持有序，所以我们会向查找元素一样，找到元素应该插入的位置。如下图所示，要插入数据6，整个过程类似于查找6，整个的查找路径为 1、1、1、4、4、5。查找到第底层原始链表的元素 5 时，发现 5 小于 6 但是后继节点 7 大于 6，所以应该把 6 插入到 5 之后 7 之前。整个时间复杂度为查找元素的时间复杂度 O(logn)。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4uqof2spzj20vq0ngn38.jpg\" alt=\"image\" style=\"zoom:67%;\" />\n\n如下图所示，假如一直往原始列表中添加数据，但是不更新索引，就可能出现两个索引节点之间数据非常多的情况，极端情况，跳表退化为单链表，从而使得查找效率从 O(logn) 退化为 O(n)。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4uqq97m01j20vq0dp40t.jpg\" alt=\"image\" style=\"zoom:67%;\" />\n\n我们该如何去维护这个索引呢？比较容易理解的做法就是完全重建索引，我们每次插入数据后，都把这个跳表的索引删掉全部重建，重建索引的时间复杂度是多少呢？因为索引的空间复杂度是 O(n)，即：索引节点的个数是 O(n) 级别，每次完全重新建一个 O(n) 级别的索引，时间复杂度也是 O(n) 。造成的后果是：为了维护索引，导致每次插入数据的时间复杂度变成了 O(n)。\n\n一个比较好的方法是使用随机函数:当我们往跳表中插入数据的时候，我们可以选择同时将这个数据插入到部分索引层中。如何选择加入哪些索引层呢？我们通过一个随机函数，来决定将这个结点插入到哪几级索引中，比如随机函数生成了值 K，那我们就将这个结点添加到第一级到第 K 级这 K 级索引中。\n\n<img src=\"https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1h4urf2i2r7j20vq0ni44h.jpg\" alt=\"image\" style=\"zoom:67%;\" />\n\n随机函数的选择很有讲究，从概率上来讲，能够保证跳表的索引大小和数据大小平衡性，不至于性能过度退化。\n\n代码参考王争老师的https://github.com/wangzheng0822/algo\n\n\n\n\n\n","tags":["数据结构"]},{"title":"Nginx负载均衡","url":"/2022/09/07/cyb-mds/module/Ngix/Nginx负载均衡/","content":"\n==作者：YB-Chi==","tags":["nginx"]},{"title":"VMWARE克隆虚机更改静态IP","url":"/2022/09/07/cyb-mds/linux/VMWARE克隆虚机更改静态IP/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n==使用版本:centos6.7==\n\n克隆过程这里就不写了\n\n\n克隆过后它会默认开启另一个ip接口\n\n我们要做的就是修改几个配置文件\n\n### 修改主机名\n\n    vi /etc/sysconfig/network\n    \n    更改这里的HOSTNAME\n\n### 修改映射关系\n\n    vi /etc/hosts\n    \n    删掉上边存的不正确的 将自己的添加上 例如:\n    127.0.0.1       cluster_model localhost.localdomain localhost\n    192.168.15.11   cluster_model\n    192.168.15.12   cluster_node1\n    192.168.15.13   cluster_node2\n    \n### 修改ip  及 eth1( 这里是eth0改为eth1)    及 硬件地址\n    \n    ifconfig查看HWADDR\n    \n    重命名\n    mv /etc/sysconfig/network-scripts/ifcfg-eth0 /etc/sysconfig/network-scripts/ifcfg-eth1\n    \n    修改配置\n    vi /etc/sysconfig/network-scripts/ifcfg-eth1\n    \n    DEVICE=eth1\n    加上刚才查看的HWADDR\n    IPADDR=\n\n### 重启服务\n\n    service network restart\n### 测试\n    \n    ping下其他集群ip以及宿主机ip测试下","tags":["Linux"]},{"title":"中标麒麟v5申威cpu服务器部署","url":"/2022/09/07/cyb-mds/linux/中标麒麟v5申威cpu服务器部署/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n### 软硬件型号\n\n| 硬件 | 型号  |\n| ---- | ----- |\n| CPU  | SW6A  |\n| 架构 | sw_64 |\n\n|    软件    |                             版本                             | 预装 | 是否保留 |\n| :--------: | :----------------------------------------------------------: | :--: | :------: |\n|     OS     |             NeoKylin Server release 5.0 (Sunway)             |  Y   |    Y     |\n|    JDK     | java-1.8.0-swjdk-devel-H324P-1.ns7.1.sw_64<br/>java-1.8.0-swjdk-H324P-1.ns7.1.sw_64<br/>java-1.8.0-swjdk-demo-H324P-1.ns7.1.sw_64<br/>java-1.8.0-swjdk-src-H324P-1.ns7.1.sw_64 |  Y   |    Y     |\n|  MARIADB   | mariadb-server-5.5.50-1.ns7.1.sw_64<br/>mariadb-5.5.50-1.ns7.1.sw_64<br/>mariadb-libs-5.5.50-1.ns7.1.sw_64 |  Y   |    N     |\n|   NGINX    | nginx-all-modules-1.12.1-1.ns7.1.noarch<br/>nginx-mod-http-perl-1.12.1-1.ns7.1.sw_64<br/>nginx-filesystem-1.12.1-1.ns7.1.noarch<br/>nginx-mod-stream-1.12.1-1.ns7.1.sw_64<br/>nginx-mod-http-image-filter-1.12.1-1.ns7.1.sw_64<br/>nginx-1.12.1-1.ns7.1.sw_64<br/>nginx-mod-http-xslt-filter-1.12.1-1.ns7.1.sw_64<br/>nginx-mod-mail-1.12.1-1.ns7.1.sw_64<br/>nginx-mod-http-geoip-1.12.1-1.ns7.1.sw_64 |  Y   |    Y     |\n| POSTGRESQL | postgresql-9.2.18-1.ns7.1.sw_64<br/>postgresql-server-9.2.18-1.ns7.1.sw_64<br/>postgresql-contrib-9.2.18-1.ns7.1.sw_64<br/>postgresql-libs-9.2.18-1.ns7.1.sw_64 |  Y   |    Y     |\n|   REDIS    |                  redis-3.2.7-1.ns7.1.sw_64                   |  Y   |    Y     |\n\n### 软件安装\n\n#### Postgresql\n\n```shell\n# 初始化\nservice postgresql initdb\n\n# 修改配置 对外开放连接\nvim /var/lib/pgsql/data/postgresql.conf\nlisten_addresses = '*'\nvim /var/lib/pgsql/data/pg_hba.conf\n# IPv4 local connections:\nhost    all             all             0.0.0.0/0            password\n\n# 启动\nsystemctl start postgresql.service\n\n# 改密码\nsu - postgres\npsql\nALTER USER postgres PASSWORD 'cyb123';\ncreate database pmp\n\n# 重启\n\\q\nexit\nsystemctl restart postgresql.service\n```\n\n#### Nginx\n\n```shell\n# 创建项目文件夹\nmkdir -p /opt/pmp\n# 放置html到/opt/pmp下改名index.html测试\n# 修改nginx配置\nvim /etc/nginx/nginx.conf\n\nserver {\n     listen       8080;\n     server_name  localhost;\n     location / {\n         root         /opt/pmp;\n         index   index.html  index.htm;\n             }\n         } \n         \n# 启动\nsystemctl start nginx.service\n# 测试\n# 访问192.168.4.101:8080\n```\n\n#### REDIS\n\n```shell\nvi /etc/redis.conf\n注释bind 127.0.0.1\nprotected-mode yes\n修改为\nprotected-mode no\nsystemctl start redis\n```\n\n#### WEB\n\n```shell\ngroupadd dev\nuseradd dev -dev\npasswd dev\ndev\ndev\n新建dev连接窗口\nmkdir app\n开启sftp上传jar包\njava -jar pmp-0.0.1-SNAPSHOT.jar\n```\n\n","tags":["Linux","Shell"]},{"title":"常用命令","url":"/2022/09/07/cyb-mds/linux/常用命令/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n#### jdbc连接hive\n\n`beeline -u jdbc:hive2://192.168.6.1:10009 -n yarn`\n\n#### 连接Phoenix\n`sqlline.py localhost可以进入phoenix shell   然后   !tables查看表`\n\n#### ls -l排序\n\n    按大小排序\n    [root@localhost ~]# ll -Sh \n    \n    按时间排序\n    \n    [root@localhost ~]# ll -rt\n\n\n​    \n    ll －t 是降序， ll －t ｜ tac 是升序\n\n#### 查询可执行文件命令\n\n    which eclipse\n    想要在xstar上执行eclipse的话如果有变量直接eclipse就可以了\n\n#### 给SecureCRT安装上传和下载的工具\n\n    yum install lrzsz\n    现在CentOS7使用systemd作为新的init系统，而systemd系统使用“target”来代替“runlevel”，默认有两个主要的target：\n    \n     multi-user.target：相当于runlevel 3[命令行界面]，graphical.target：相当于runlevel 5[图形界面]\n     \n    设置默认的target则使用命令：\n    ln -sf /lib/systemd/system/<target name>.target /etc/systemd/system/default.target\n    \n    我这里要将CentOS7开机默认进入命令行界面，则运行命令：\n    ln -sf /lib/systemd/system/multi-user.target /etc/systemd/system/default.target\n\n#### centos7开机模式\n现在CentOS7使用systemd作为新的init系统，而systemd系统使用“target”来代替“runlevel”，默认有两个主要的target：\t\n\t\n    multi-user.target：相当于runlevel 3[命令行界面]，\t\t\t\n    graphical.target：相当于runlevel 5[图形界面]\n    \n    设置默认的target则使用命令：\n    ln -sf /lib/systemd/system/<target name>.target \t\n    /etc/systemd/system/default.target\n    \n    我这里要将CentOS7开机默认进入命令行界面，则运行命令：\n    ln -sf /lib/systemd/system/multi-user.target \n    /etc/systemd/system/default.target\n\n\n#### 切换用户\n\n    su [user] 和 su - [user]的区别：\n    su [user]切换到其他用户，但是不切换环境变量，su - [user]则是完整的切换到新的用户环境。\n    \n    如：\n    \n    [root@rac1 ~]# pwd  --当前目录\n    /root\n    [root@rac1 ~]# su oracle --使用su [user]\n    [oracle@rac1 root]$ pwd  --当前目录没有改变，还是之前的用户目录\n    /root\n    [oracle@rac1 root]$ su - oracle --使用su - [user]\n    Password:\n    [oracle@rac1 ~]$ pwd   --当前目录变为当前用户的家目录\n    /home/oracle\n    [oracle@rac1 ~]$\n    所以建议大家在切换用户时，尽量用su - [use r]，否则可能会出现环境变量不对的问题。\n#### 无法用yum安装\n\n==提示：file:///mnt/cdrom/repodata/repomd.xml: [Errno 14] Could not open/read file:/ ==    \n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytidtowsyj20j50dmdgn.jpg)\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytieu55wvj20go0h3mxu.jpg)\n#### rz上传不成功 *was skipped \n==权限不够==\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytin3h1itj20iv067glt.jpg)\n\n\n#### 使用tar命令解压.zip文件的时候，遇到如下异常\n\n    linuxidc@Ubuntu:~/Documents$ tar -xzvf wls1033_dev.zip\n    gzip: stdin has more than one entry--rest ignored\n    tar: Child returned status 2\n    tar: Error is not recoverable: exiting now\n\n==tar命令是调用了gunzip命令的，\n对只有一个压缩内容的文件来解压的时候才用tar， 而如果压缩包里有多个文件被压缩了，\ntar命令不能继续工作。可以采用unzip命令去解压。\n先查看是否已安装unzip，没有安装的话下载unzip。\n然后解压缩：==\n\n    linuxidc@ubuntu:~/Documents$  unzip wls1033_dev,zip -d weblogic\n解压缩到当前文件夹下的weblogic文件夹下。\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytiot13t3j20mg0mvjth.jpg)\n#### 设置变量的三种方法\n\n1) 在/etc/profile文件中添加变量【对所有用户生效（永久的）】\n\n用vi在文件/etc/profile文件中增加变量，该变量将会对Linux下所有用户有效，并且是“永久的”。\n\n例如：编辑/etc/profile文件，添加CLASSPATH变量\n\n    vi /etc/profile\n    export CLASSPATH=.:$JAVA_HOME/lib/tools.jar;$JAVA_HOME/lib/dt.jar\n注：修改文件后要想马上生效还要运行# source /etc/profile不然只能在下次重进此用户时生效。\n\n2) 在用户目录下的.bash_profile文件中增加变量【对单一用户生效（永久的）】\n\n用VI在用户目录下的.bash_profile文件中增加变量，改变量仅会对当前用户有效，并且是“永久的”。\n\n例如：编辑li用户目录（/home/li）下的.bashrc\n\n    $ vi /home/li/.bashrc  \n添加如下内容：\n\n    export CLASSPATH=.:$JAVA_HOME/lib/tools.jar;$JAVA_HOME/lib/dt.jar\n注：修改文件后要想马上生效还要运行$ source /home/li/.bashrc不然只能在下次重进此用户时生效。\n\n3) 直接运行export命令定义变量【只对当前shell（BASH）有效（临时的）】\n\n在shell的命令行下直接使用[export变量名=变量值]定义变量，该变量只在当前的shell（BASH）或其子shell（BASH）下是有效的，shell关闭了，变量也就失效了，再打开新shell时就没有这个变量，需要使用的话还需要重新定义。\n\n#### 添加用户 赋予root权\n\n1、添加用户，首先用adduser命令添加一个普通用户，命令如下：\n\n    #adduser tommy\n    //添加一个名为tommy的用户\n    #passwd tommy   //修改密码\n    Changing password for user tommy.\n    New UNIX password:     //在这里输入新密码\n    Retype new UNIX password:  //再次输入新密码\n    passwd: all authentication tokens updated successfully.\n2、赋予root权限\n\n方法一：修改 /etc/sudoers 文件，找到下面一行，把前面的注释（#）去掉\n\n    Allows people in group wheel to run all commands\n    %wheel    ALL=(ALL)    ALL\n然后修改用户，使其属于root组（wheel），命令如下：\n\n    #usermod -g root tommy\n修改完毕，现在可以用tommy帐号登录，然后用命令 su – ，即可获得root权限进行操作。\n\n方法二：修改 /etc/sudoers 文件，找到下面一行，在root下面添加一行，如下所示：\n\n    Allow root to run any commands anywhere\n    root    ALL=(ALL)     ALL\n    tommy   ALL=(ALL)     ALL\n修改完毕，现在可以用tommy帐号登录，然后用命令 sudo – ，即可获得root权限进行操作。\n\n方法三：修改 /etc/passwd 文件，找到如下行，把用户ID修改为 0 ，如下所示：\n\n    tommy:x:0:33:tommy:/data/webroot:/bin/bash\n#### 防火墙\n##### linux6\n1) 重启后生效\n\n    开启： chkconfig iptables on  \n    关闭： chkconfig iptables off\n2) 即时生效，重启后失效service 方式\n\n    开启： service iptables start  \n    关闭： service iptables stop  \n\n##### linux7\nCentOS7这个版本的防火墙默认使用的是firewall，与之前的版本使用iptables不一样。按如下方便配置防火墙：\n````powershell\n    1、 查看防火墙状态\n        firewall-cmd    --state\n        关闭防火墙\n        systemctl  stop   firewalld.service\n        开启防火墙\n        systemctl  start   firewalld.service\n    2、关闭开机启动：systemctl disable firewalld.service\n    \n    3、安装iptables防火墙，执行以下命令安装iptables防火墙：yum install iptables-services  \n        开启iptables防火墙的命令是：\n        systemctl  start  iptables.service\n        重启iptables防火墙的命令是：\n        systemctl  restart  iptables.service\n        关闭iptables防火墙的命令是：\n        systemctl  stop  iptables.service\n        查看iptables防火墙状态的命令是：\n        systemctl  status  iptables.service\n````\n#### 新增用户\n\n```powershell\n新增用户\n[root@BDS-DATA2 kafka]# useradd -g root -m yuanbo\n[root@BDS-DATA2 kafka]# passwd yuanbo\n更改用户 yuanbo 的密码 。\n新的 密码：yuanbo\n无效的密码： 它基于字典单词\n无效的密码： 过于简单\n重新输入新的 密码：yuanbo\npasswd： 所有的身份验证令牌已经成功更新。\n\n修改sudo权限\n[root@BDS-CM config]# vim /etc/sudoers\n在root    ALL=(ALL)       ALL下添加一行\nyuanbo  ALL=(ALL)       ALL\n```\n\n#### 新增硬盘\n````powershell\nfdisk  /dev/sdb\nn\np\nl\n\n\nwq\n\n然后 再做文件系统   mkfs.ext3 /dev/sdb1\n  \n\n然后 vim  /etc/fstab \n\n看着上面的格式加一行 \n/dev/sdb1        /dataN        ext3      defaults    0       0  \n````\n#### 关于分区\n\n```powershell\n[root@BDS data3]# fdisk /dev/sdb1\n\nDevice contains neither a valid DOS partition table, nor Sun, SGI or OSF disklabel\n\nBuilding a new DOS disklabel with disk identifier 0x0b32c3f6.\n\nChanges will remain in memory only, until you decide to write them.\n\nAfter that, of course, the previous content won't be recoverable.\n\nWarning: invalid flag 0x0000 of partition table 4 will be corrected by w(rite)\n\nWARNING: DOS-compatible mode is deprecated. It's strongly recommended to\n\t\tswitch off the mode (command 'c') and change display units to\n     \tsectors (command 'u').\n     \t\nCommand (m for help): m\n\nCommand action\n       #切换一个可启动的标志\n   a   toggle a bootable flag\n       #编辑bsd磁碟标签\n   b   edit bsd disklabel\n       #切换dos兼容性标志\n   c   toggle the dos compatibility flag\n       #删除一个分区\n   d   delete a partition\n       #已知的分区类型列表\n   l   list known partition types\n       #打印这个菜单\n   m   print this menu\n       #添加一个新的分区\n   n   add a new partition\n       #创建一个新的空DOS分区表\n   o   create a new empty DOS partition table\n       #打印分区表\n   p   print the partition table\n       #退出不保存更改\n   q   quit without saving changes\n       #创建一个新的空的Sun disklabel\n   s   create a new empty Sun disklabel\n       #更改分区的系统id\n   t   change a partition's system id\n       #改变显示/输入单元\n   u   change display/entry units\n       #验证分区表\n   v   verify the partition table\n       #将表写入磁盘并退出\n   w   write table to disk and exit\n       #额外的功能(专家)\n   x   extra functionality (experts only)\n   \n   \n```\n\n#### 查看某一端口的占用情况： lsof -i:端口号\n\n#### centos6.7设置开机启动命令界面\n开机后进入图形化界面还是进入命令行取决于inittab文件中的配置。该文件位于etc目录下。\n````powershell\n#vim /etc/inittab\n\nid:5:initdefault:(默认的 run level 等级为 5,即图形 界面)\n将 5 修改为 3 即可。\n````\n保存文件后重启系统你就可以看见是启动的文本界面了。\n\n#### 查看系统内核版本\n\n```powershell\ncat /proc/version\n```\n\n#### 查看系统版本\n\n```powershell\ncat /etc/redhat-release\n```\n\n#### 查看sftp和ftp连接数\n````shell\n# 虽然是打印tcp，但是基本都是sftp和ftp连接\nnetstat -n | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}'\n````\n\n","tags":["Linux","Shell"]},{"title":"挂载离线YUM源","url":"/2022/09/07/cyb-mds/linux/挂载离线YUM源/","content":"\n==作者：YB-Chi==\n\n\n\n[toc]\n\n```\n# 上传iso，授权，挂载\nmkdir -p /mnt/iso/\nmkdir -p /media/iso\nchmod 755 /media/iso/NeoKylin-Desktop.iso\nmount -o loop -t iso9660 /media/iso/NeoKylin-Desktop.iso /mnt/iso/\n\nyum clean all\nyum makecache\n\n```\n\n","tags":["Linux","Shell"]},{"title":"Kafka-1-分区策略","url":"/2022/09/07/cyb-mds/module/Kafka/Kafka-1-分区策略/","content":"\n==作者：YB-Chi==\n\n\n\n\n\n**摘选自:极客时间-Kafka核心技术与实战**","tags":["redis"]},{"title":"Linux虚拟机 配置YUM源","url":"/2022/09/07/cyb-mds/linux/Linux虚拟机 配置YUM源/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n\n#### 使用挂载光驱作为YUM源\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytjrh6zvnj20kh0igdn5.jpg)\n\n#### 挂载光驱\n\n    mkdir /mnt/cdrom        \n    mount -t iso9660 -o ro /dev/cdrom /mnt/cdrom/ \n\n如果报错\n\n    mount: no medium found on /dev/sr1\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytjs4lc7jj20il03xq3g.jpg)\n\n![image](https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fytjslnewlj20iy082dgs.jpg)\n\n#### 修改本机的YUM源，将源只想光驱\n\n    cd /etc/yum.repos.d/            \n    rename .repo .repo.bak *         #为了测试方便，避免其他YUM源的干扰\n    \n    cp CentOS-Media.repo.bak CentOS-Local.repo\n    \n    vi CentOS-Local.repo\n    修改如下配置：\n    baseurl=file:///mnt/cdrom        #挂载光驱的位置\n    gpgcheck=1                              \n    #gpg验证是否开启的选项，1是开启，0是不开启，一般情况可以关掉。\n    enabled=1                                  #是否启用：0：不启用  1：启用\n#### 配置好后\n\n    yum clean all\n    yum list查看列表","tags":["Linux"]},{"title":"Linux查看硬件信息","url":"/2022/09/07/cyb-mds/linux/Linux查看硬件信息/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n#### CPU\n``` shell\n# 总核数 = 物理CPU个数 X 每颗物理CPU的核数 \n# 总逻辑CPU数 = 物理CPU个数 X 每颗物理CPU的核数 X 超线程数\n\n# 查看物理CPU个数\ncat /proc/cpuinfo| grep \"physical id\"| sort| uniq| wc -l\n\n# 查看每个物理CPU中core的个数(即核数)\ncat /proc/cpuinfo| grep \"cpu cores\"| uniq\n\n# 查看逻辑CPU的个数\ncat /proc/cpuinfo| grep \"processor\"| wc -l\n\n# 查看CPU信息（型号）\ncat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c\n```\n\n#### MEM\n``` shell\n# 每个内存插槽上的详细信息\ndmidecode | grep -A16 \"Memory Device$\"\n# 总内存\nfree -h\n```\n\n#### DISK\n``` shell\nlsblk\n\nfdisk -l\n\n# io信息\niostat -x 10\n```\n\n#### Network\n``` shell\nifconfig\n\n# 网关\nnetstat -rn\nip route show\n\n# 路由\nroute -n\n\n# DNS\ndig |grep SERVER\n```\n\n\n#### MegaCli\n##### 安装\n```shell\n# raid信息依赖MegaCli去查看\n\n# 下载MegaCli包\nhttps://docs.broadcom.com/docs-and-downloads/raid-controllers/raid-controllers-common-files/8-07-10_MegaCLI_Linux.zip\n\n# 解压后传到服务器上\nrpm -ivh MegaCli-8.07.10-1.noarch.rpm\n\n# 默认安装到opt下，做个软链\nln -s /opt/MegaRAID/MegaCli/MegaCli64 /usr/bin/\n\nMegaCli64 -v\n```\n\n##### 使用\n```shell\n\n#  查raid卡信息\nMegaCli64 -AdpAllInfo -aALL\n\n# 查看raid信息及raid组成磁盘信息\nMegaCli64 -LdPdInfo -aALL\n\nAdapter #0\n\nNumber of Virtual Disks: 2\nVirtual Drive: 0 (Target Id: 0)\nName                :system\n# Primary为raid等级\nRAID Level          : Primary-1, Secondary-0, RAID Level Qualifier-0\nSize                : 837.258 GB\nSector Size         : 512\nIs VD emulated      : No\nMirror Data         : 837.258 GB\nState               : Optimal\nStrip Size          : 256 KB\n# 该raid组磁盘数\nNumber Of Drives    : 2\nSpan Depth          : 1\nDefault Cache Policy: WriteBack, ReadAhead, Direct, No Write Cache if Bad BBU\nCurrent Cache Policy: WriteThrough, ReadAhead, Direct, No Write Cache if Bad BBU\nDefault Access Policy: Read/Write\nCurrent Access Policy: Read/Write\nDisk Cache Policy   : Disk's Default\nEncryption Type     : None\nPI type: No PI\n\nIs VD Cached: No\nNumber of Spans: 1\nSpan: 0 - Number of PDs: 2\n\nPD: 0 Information\nEnclosure Device ID: 252\n# 盘位\nSlot Number: 0\nDrive's position: DiskGroup: 0, Span: 0, Arm: 0\nEnclosure position: N/A\nDevice Id: 11\nWWN: 5000C50099965DBC\nSequence Number: 2\nMedia Error Count: 0\nOther Error Count: 0\nPredictive Failure Count: 0\nLast Predictive Failure Event Seq Number: 0\nPD Type: SAS\n\nRaw Size: 838.362 GB [0x68cb9e30 Sectors]\nNon Coerced Size: 837.862 GB [0x68bb9e30 Sectors]\nCoerced Size: 837.258 GB [0x68a84800 Sectors]\nSector Size:  512\nLogical Sector Size:  512\nPhysical Sector Size:  512\nFirmware state: Online, Spun Up\nCommissioned Spare : No\nEmergency Spare : No\nDevice Firmware Level: N003\nShield Counter: 0\nSuccessful diagnostics completion on :  N/A\nSAS Address(0): 0x5000c50099965dbd\nSAS Address(1): 0x0\nConnected Port Number: 0(path0) \nInquiry Data: SEAGATE ST900MM0168     N003S403CAWV            \nFDE Capable: Not Capable\nFDE Enable: Disable\nSecured: Unsecured\nLocked: Unlocked\nNeeds EKM Attention: No\nForeign State: None \nDevice Speed: 12.0Gb/s \nLink Speed: 12.0Gb/s \nMedia Type: Hard Disk Device\nDrive:  Not Certified\nDrive Temperature :32C (89.60 F)\nPI Eligibility:  No \nDrive is formatted for PI information:  No\nPI: No PI\nPort-0 :\nPort status: Active\nPort's Linkspeed: 12.0Gb/s \nPort-1 :\nPort status: Active\nPort's Linkspeed: 12.0Gb/s \nDrive has flagged a S.M.A.R.T alert : No\n\n\n\n\nPD: 1 Information\nEnclosure Device ID: 252\nSlot Number: 1\nDrive's position: DiskGroup: 0, Span: 0, Arm: 1\nEnclosure position: N/A\nDevice Id: 9\nWWN: 5000C50099978624\nSequence Number: 2\nMedia Error Count: 0\nOther Error Count: 0\nPredictive Failure Count: 0\nLast Predictive Failure Event Seq Number: 0\nPD Type: SAS\n\nRaw Size: 838.362 GB [0x68cb9e30 Sectors]\nNon Coerced Size: 837.862 GB [0x68bb9e30 Sectors]\nCoerced Size: 837.258 GB [0x68a84800 Sectors]\nSector Size:  512\nLogical Sector Size:  512\nPhysical Sector Size:  512\nFirmware state: Online, Spun Up\nCommissioned Spare : No\nEmergency Spare : No\nDevice Firmware Level: N003\nShield Counter: 0\nSuccessful diagnostics completion on :  N/A\nSAS Address(0): 0x5000c50099978625\nSAS Address(1): 0x0\nConnected Port Number: 1(path0) \nInquiry Data: SEAGATE ST900MM0168     N003S403C74P            \nFDE Capable: Not Capable\nFDE Enable: Disable\nSecured: Unsecured\nLocked: Unlocked\nNeeds EKM Attention: No\nForeign State: None \nDevice Speed: 12.0Gb/s \nLink Speed: 12.0Gb/s \nMedia Type: Hard Disk Device\nDrive:  Not Certified\nDrive Temperature :35C (95.00 F)\nPI Eligibility:  No \nDrive is formatted for PI information:  No\nPI: No PI\nPort-0 :\nPort status: Active\nPort's Linkspeed: 12.0Gb/s \nPort-1 :\nPort status: Active\nPort's Linkspeed: 12.0Gb/s \nDrive has flagged a S.M.A.R.T alert : No\n\n\n\nVirtual Drive: 1 (Target Id: 1)\nName                :data\nRAID Level          : Primary-0, Secondary-0, RAID Level Qualifier-0\nSize                : 837.258 GB\nSector Size         : 512\nIs VD emulated      : No\nParity Size         : 0\nState               : Optimal\nStrip Size          : 256 KB\nNumber Of Drives    : 1\nSpan Depth          : 1\nDefault Cache Policy: WriteBack, ReadAhead, Direct, No Write Cache if Bad BBU\nCurrent Cache Policy: WriteThrough, ReadAhead, Direct, No Write Cache if Bad BBU\nDefault Access Policy: Read/Write\nCurrent Access Policy: Read/Write\nDisk Cache Policy   : Disk's Default\nEncryption Type     : None\nPI type: No PI\n\nIs VD Cached: No\nNumber of Spans: 1\nSpan: 0 - Number of PDs: 1\n\nPD: 0 Information\nEnclosure Device ID: 252\nSlot Number: 2\nDrive's position: DiskGroup: 1, Span: 0, Arm: 0\nEnclosure position: N/A\nDevice Id: 10\nWWN: 5000C50099972CC0\nSequence Number: 2\nMedia Error Count: 0\nOther Error Count: 0\nPredictive Failure Count: 0\nLast Predictive Failure Event Seq Number: 0\nPD Type: SAS\n\nRaw Size: 838.362 GB [0x68cb9e30 Sectors]\nNon Coerced Size: 837.862 GB [0x68bb9e30 Sectors]\nCoerced Size: 837.258 GB [0x68a84800 Sectors]\nSector Size:  512\nLogical Sector Size:  512\nPhysical Sector Size:  512\nFirmware state: Online, Spun Up\nCommissioned Spare : No\nEmergency Spare : No\nDevice Firmware Level: N003\nShield Counter: 0\nSuccessful diagnostics completion on :  N/A\nSAS Address(0): 0x5000c50099972cc1\nSAS Address(1): 0x0\nConnected Port Number: 2(path0) \nInquiry Data: SEAGATE ST900MM0168     N003S403CX3Q            \nFDE Capable: Not Capable\nFDE Enable: Disable\nSecured: Unsecured\nLocked: Unlocked\nNeeds EKM Attention: No\nForeign State: None \nDevice Speed: 12.0Gb/s \nLink Speed: 12.0Gb/s \nMedia Type: Hard Disk Device\nDrive:  Not Certified\nDrive Temperature :30C (86.00 F)\nPI Eligibility:  No \nDrive is formatted for PI information:  No\nPI: No PI\nPort-0 :\nPort status: Active\nPort's Linkspeed: 12.0Gb/s \nPort-1 :\nPort status: Active\nPort's Linkspeed: 12.0Gb/s \nDrive has flagged a S.M.A.R.T alert : No\n\n\n\n\nExit Code: 0x00\n\n\n```\n","tags":["Linux"]},{"title":"常用命令","url":"/2022/09/07/cyb-mds/linux/ftp常用操作/","content":"\n==作者：YB-Chi==\n\n\n\n```shell\n# login\nftp nmsftpuser@100.92.255.214 21111\n# login\nftp ftpuser@10.226.13.7 21 Changeme_321\n# down\nwget ftp://192.168.1.32:21/HA/WX/NS/OMC2/CM* --ftp-user=bjftp --ftp-password=pwd -r\n# down 文件夹\nwget ftp://ip:21/CM/20201021/* --ftp-user=ftirpuser --ftp-password=pwd -r ./20201021\n# 列出文件列表\ncurl -k -u user:pwd sftp://ip:port/path/\n# 下载文件\ncurl -u user:pwd sftp://ip:port/path/file\n```\n\n","tags":["Linux","Shell"]},{"title":"linux里挂载（mount）和取消挂载（umount）命令的使用","url":"/2022/09/07/cyb-mds/linux/linux里挂载（mount）和取消挂载（umount）命令的使用/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n### 一、简单用法\n\n```powershell\n$ mount /dev/hda2 /home\n```\n\n第一个叁数是与包括文件系统的磁盘或分区相关的设备文件。\n第二个叁数是要mount到的目录。\n\n```powershell\n$ umount /dev/hda2\n$ umount /usr\n```\n\n参数可以是设备文件或安装点。\n\n### 二、mount详细介绍\n\n如果想在运行的Linux下访问其它文件系统中的资源的话，就要用mount命令来实现。\n\n **1.    mount的基本用法是？**\n\n格式：**mount [-参数] [设备名称] [挂载点]**\n\n其中常用的参数有：\n\n```\n-a 安装在/etc/fstab文件中类出的所有文件系统。\n\n-f 伪装mount，作出检查设备和目录的样子，但并不真正挂载文件系统。\n\n-n 不把安装记录在/etc/mtab文件中。\n\n-r 讲文件系统安装为只读。\n\n-v 详细显示安装信息。\n\n-w 将文件系统安装为可写，为命令默认情况。\n\n-t 指定设备的文件系统类型，常见的有：\n\next2  linux目前常用的文件系统\n\nmsdos   MS-DOS的fat，就是fat16\n\nvfat   windows98常用的fat32\n\nnfs   网络文件系统\n\niso9660   CD-ROM光盘标准文件系统\n\nntfs   windows NT/2000/XP的文件系统\n\nauto 自动检测文件系统\n\n-o 指定挂载文件系统时的选项，有些也可写到在/etc/fstab中。常用的有：\n\ndefaults 使用所有选项的默认值（auto、nouser、rw、suid）\n\nauto/noauto 允许/不允许以 –a选项进行安装\n\ndev/nodev 对/不对文件系统上的特殊设备进行解释\n\nexec/noexec 允许/不允许执行二进制代码\n\nsuid/nosuid 确认/不确认suid和sgid位\n\nuser /nouser 允许/不允许一般用户挂载\n\ncodepage=XXX 代码页\n\niocharset=XXX 字符集\n\nro 以只读方式挂载\n\nrw 以读写方式挂载\n\nremount 重新安装已经安装了的文件系统\n\nloop 挂载回旋设备\n\n```\n\n需要注意的是，挂载点必须是一个已经存在的目录，这个目录可以不为空，但挂载后这个目录下以前的内容将不可用，umount以后会恢复正常。使用多个-o参数的时候，-o只用一次，参数之间用半角逗号隔开：\n`# mount –o remount,rw /`\n\n例如要挂载windows下文件系统为FAT32的D盘，一般而言在Linux下这个分区对应/dev/hda5，根据具体的分区情况会有不同，这里就以hda5来举例说明：\n\n```\n# mkdir /mnt/hda5   //创建hda5的目录作为挂载点，位置和目录名可自定义//\n\n# mount -t vfat /dev/hda5 /mnt/hda5\n```\n\n一般而言，Linux会自动探测分区的文件系统，除非让你指定时，否则-t vfat 可以省掉。\n`# mount /dev/hda5 /mnt/hda5`\n\n这样就可以进入/mnt/hda5目录去访问分区中的资源了。\n\n**2.    为什么mount上分区后显示不了中文文件为问号/乱码？**\n\n显 示问号表明你的系统中没有可识别使用的中文字体，请先安装中文字体。确保你的系统已经可以很好的显示中文。显示为乱码一般是mount默认使用的文件系统 编码和文件系统中文件的实际编码不一致造成的。要想正常显示中文文件，mount时需要用到 -o 参数里的codepage和iocharset选项。codepage指定文件系统的代码页，简体中文中文代码是936；iocharset指定字符集，简体中文一般用cp936或gb2312。\n\n`# mount –o iocharset=gb2312 codepage=936 /dev/hda5 /mnt/hda5`\n\n一般来说 mount –o iocharset=cp936 /dev/hda5 /mnt/hda5 就可以解决问题了。\n\n如果这样做了以后还有问题，请尝试UTF-8编码：\nCODE:\n`# mount –o iocharset=utf8 /dev/hda5 /mnt/hda5`\n\n \n\n**3.    为什么mount上去以后分区普通用户不可写？**\n\nmount时加上 –oumask=000 即可：\nCODE:\n`# mount –o umask=000, iocharset=cp936 /dev/hda5 /mnt/hda5`\n\n \n\n**4.    为什么mount上去后的分区中的文件都变成短文件名了？**\n\n这是文件系统挂错的原因，将FAT32挂载成FAT16时就会出现这种情况，先umount，然后用–t vfat 重新挂载即可解决问题。\nCODE:\n`# mount –t vat /dev/hda5 /mnt/hda5`\n\n \n\n**5.    为什么不能mount ntfs分区？**\n\n这是内核不支持NTFS文件系统的原因，请重新编译内核或者安装内核的NTFS文件系统支持包，以使得内核有NTFS文件系统的支持。\n\n \n\n**6.    如何挂载U盘和mp3？**\n\n如果计算机没有其它SCSI设备和usb外设的情况下，插入的U盘的设备路径是 /dev/sda1，用命令：\n\n\\# mkdir /mnt/u\n\n\\# mount /dev/sda1 /mnt/u\n\n挂载即可。\n\n**7.    可以直接使用iso文件吗？**\n\n可以，就是mount的这一选项使得Linux下有免费虚拟光驱的说法，具体用法是：\n\n\\# mkdir /mnt/iso\n\n\\# mount –o loop linux.iso /mnt/iso\n\n当然，挂载以后挂载点/mnt/iso也是只读的。 \n\n \n\n**8.    我怎么不可以mount iso文件？**\n\n一般而言，大多数的发行版使用的内核均已将loop设备的支持编译进去了，但是也有没有的情况，所以请确保系统所使用的内核支持loop设备。\n\n第二种情况是iso文件被放置到了NTFS或其它只读文件系统中了。挂载loop 设备必须要求挂载到一个可写的分区中，目前Linux内核对NTFS文件系统的写支持非常有限，请将iso文件复制到其它可写文件系统中后再挂载。\n\n \n\n**9.  如何挂载光驱和软驱**\n\n一般来说CDROM的设备文件是/dev/hdc，软驱的设备名是/dev/fd0\nCODE:\n\\# mkdir /mnt/cdrom\n\n\\# mount /dev/hdc /mnt/cdrom //挂载光驱 //\n\n\\# mkdir /mnt/floppy \n\n\\# mount /dev/fd0 /mnt/floppy //挂载软驱 //\n\n \n\n**10.   为何挂载的CD-ROM不能显示中文文件？**\n\n使用 –o iocharset=cp936 选项一般能解决问题，否则使用utf-8编码。\n`# mount –o iocharset=cp936 /dev/hdc /mnt/cdrom`\n\n \n\n**11.   如何开机自动挂载分区？**\n\n每次挂载都要输入那么长的命令的确是繁琐了些，只要将分区信息写到/etc/fstab文件中即可实现系统启动的自动挂载，例如对于/dev/hda5的自动挂载添加如下的行即可：\n`/dev/hda5 /mnt/hda5 vfat defaults,iocharset=cp936, rw 0 0`\n\n \n\n**12.   如何挂载samba 分区？**\n`# mkdir /mnt/share`\n\n`# mount -t smbfs -ousername=root,password=abc,codepage=936,iocharset=gb2312//192.168.1.100/share   /mnt/share`\n\n如果中文显示不正常请尝试UTF-8编码。当然可以写到fstab中实现自动挂载。\n\n \n\n**13.   mount--bind是什么意思？**\n\nmount --bind 是将一个目录中的内容挂载到另一个目录上，用法是\nCODE:\n`# mount --bind olddir newdir`\n\n这个命令使得自己搭建的FTP要共享某个目录的时候变得特别方便。如果要取消mount用命令：\n`# mount --move olddir newdir 即可。`\n\n如果mount --bind 也想写入fstab中的话格式如下：\n`olddir newdir none bind 0 0`\n\n### 三、 umount基本用法\n\n比如 /dev/hda5 已经挂载在/mnt/hda5上,用一下三条命令均可卸载挂载的文件系统\n\n`# umount /dev/hda5`\n\n`# umount /mnt/hda5`\n\n`# umount /dev/hda5 /mnt/hda5`\n\n16.   为什么umount的时候老显示 device busy？\n\n这是因为有程序正在访问这个设备，最简单的办法就是让访问该设备的程序退出以后再umount。可能有时候用户搞不清除究竟是什么程序在访问设备，如果用户不急着umount，则可以用:\n\n`#umount -l /mnt/hda5`\n\n来卸载设备。选项 –l 并不是马上umount，而是在该目录空闲后再umount。还可以先用命令ps aux 来查看占用设备的程序PID，然后用命令kill来杀死占用设备的进程，这样就umount的非常放心了。","tags":["Linux","Shell"]},{"title":"POI","url":"/2022/09/07/cyb-mds/java/POI/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n#### 用户API (HSSF and XSSF)\n\n```java\n// 创建一个新文件\t\nFileOutputStream out = new FileOutputStream(\"workbook.xls\");\n// 创建一个新的工作簿\nWorkbook wb = new HSSFWorkbook();\n// 创建一个新工作表\nSheet s = wb.createSheet();\n// 声明一个行对象引用\nRow r = null;\n// 声明一个单元格对象引用\nCell c = null;\n// 创建3个单元格样式\nCellStyle cs = wb.createCellStyle();\nCellStyle cs2 = wb.createCellStyle();\nCellStyle cs3 = wb.createCellStyle();\nDataFormat df = wb.createDataFormat();\n// 创建2个字体对象\nFont f = wb.createFont();\nFont f2 = wb.createFont();\n\n//设置字体1到12点类型\nf.setFontHeightInPoints((short) 12);\n//使它成为蓝色\nf.setColor( (short)0xc );\n// 使其加粗\n//arial是默认字体\nf.setBoldweight(Font.BOLDWEIGHT_BOLD);\n\n//设置字体2到10点类型\nf2.setFontHeightInPoints((short) 10);\n//使它成为红色\nf2.setColor( (short)Font.COLOR_RED );\n//使其粗体\nf2.setBoldweight(Font.BOLDWEIGHT_BOLD);\n\nf2.setStrikeout( true );\n\n//设置单元格风格\ncs.setFont(f);\n//设置单元格格式\ncs.setDataFormat(df.getFormat(\"#,##0.0\"));\n\n//设置一个细边框\ncs2.setBorderBottom(cs2.BORDER_THIN);\n//填充w fg填充颜色\ncs2.setFillPattern((short) CellStyle.SOLID_FOREGROUND);\n//将单元格格式设置为文本，请参阅DataFormat以获取完整列表\ncs2.setDataFormat(HSSFDataFormat.getBuiltinFormat(\"text\"));\n\n// 设置字体\ncs2.setFont(f2);\n\n// 在Unicode中设置工作表名称\nwb.setSheetName(0, \"\\u0422\\u0435\\u0441\\u0442\\u043E\\u0432\\u0430\\u044F \" + \n                   \"\\u0421\\u0442\\u0440\\u0430\\u043D\\u0438\\u0447\\u043A\\u0430\" );\n// in case of plain ascii\n// wb.setSheetName(0, \"HSSF Test\");\n// create a sheet with 30 rows (0-29)\nint rownum;\nfor (rownum = (short) 0; rownum < 30; rownum++)\n{\n    // create a row\n    r = s.createRow(rownum);\n    // on every other row\n    if ((rownum % 2) == 0)\n    {\n        // make the row height bigger  (in twips - 1/20 of a point)\n        r.setHeight((short) 0x249);\n    }\n\n    //r.setRowNum(( short ) rownum);\n    // create 10 cells (0-9) (the += 2 becomes apparent later\n    for (short cellnum = (short) 0; cellnum < 10; cellnum += 2)\n    {\n        // create a numeric cell\n        c = r.createCell(cellnum);\n        // do some goofy math to demonstrate decimals\n        c.setCellValue(rownum * 10000 + cellnum\n                + (((double) rownum / 1000)\n                + ((double) cellnum / 10000)));\n\n        String cellValue;\n\n        // create a string cell (see why += 2 in the\n        c = r.createCell((short) (cellnum + 1));\n        \n        // on every other row\n        if ((rownum % 2) == 0)\n        {\n            // set this cell to the first cell style we defined\n            c.setCellStyle(cs);\n            // set the cell's string value to \"Test\"\n            c.setCellValue( \"Test\" );\n        }\n        else\n        {\n            c.setCellStyle(cs2);\n            // set the cell's string value to \"\\u0422\\u0435\\u0441\\u0442\"\n            c.setCellValue( \"\\u0422\\u0435\\u0441\\u0442\" );\n        }\n\n\n        // make this column a bit wider\n        s.setColumnWidth((short) (cellnum + 1), (short) ((50 * 8) / ((double) 1 / 20)));\n    }\n}\n\n//draw a thick black border on the row at the bottom using BLANKS\n// advance 2 rows\nrownum++;\nrownum++;\n\nr = s.createRow(rownum);\n\n// define the third style to be the default\n// except with a thick black border at the bottom\ncs3.setBorderBottom(cs3.BORDER_THICK);\n\n//create 50 cells\nfor (short cellnum = (short) 0; cellnum < 50; cellnum++)\n{\n    //create a blank type cell (no value)\n    c = r.createCell(cellnum);\n    // set it to the thick black border style\n    c.setCellStyle(cs3);\n}\n\n//end draw thick black border\n\n\n// demonstrate adding/naming and deleting a sheet\n// create a sheet, set its title then delete it\ns = wb.createSheet();\nwb.setSheetName(1, \"DeletedSheet\");\nwb.removeSheetAt(1);\n//end deleted sheet\n\n// write the workbook to the output stream\n// close our file (don't blow out our file handles\nwb.write(out);\nout.close();\n```","tags":["Linux","Shell"]},{"title":"解决maven依赖","url":"/2022/09/07/cyb-mds/java/Maven/解决maven依赖/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n依赖过滤\n\n（1）单依赖过滤\n\n    同依赖过滤直接处理：可以过滤一个或者多个，如果过滤多个要写多个<exclusion>。这个也解决不了我的问题，或者说解决太麻烦，我那里知道hbase要依赖那些包，记不住。\n\nJava代码  \n\n````xml\n    <dependency>      \n         <groupId>org.apache.hbase</groupId>  \n         <artifactId>hbase</artifactId>  \n         <version>0.94.17</version>   \n         <exclusions>    \n               <exclusion>        \n                    <groupId>commons-logging</groupId>            \n                    <artifactId>commons-logging</artifactId>    \n               </exclusion>    \n         </exclusions>    \n    </dependency>   \n````\n（2）多依赖过滤\n\n     把所以依赖都过滤了。手起刀落~啊，世界都安静了。\nJava代码  \n\n````xml\n    <dependency>  \n        <groupId>org.apache.hbase</groupId>  \n        <artifactId>hbase</artifactId>  \n        <version>0.94.17</version>  \n        <exclusions>  \n            <exclusion>  \n                <groupId>*</groupId>  \n                <artifactId>*</artifactId>  \n            </exclusion>  \n        </exclusions>  \n    </dependency>  \n````","tags":["Java","Maven"]},{"title":"Log4j","url":"/2022/09/07/cyb-mds/java/log/LOG4J/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n### 个人配置\n\n````java\n### 设置###\nlog4j.rootLogger = info,stdout,D\n\n### 输出信息到控制抬 ###\nlog4j.appender.stdout = org.apache.log4j.ConsoleAppender\nlog4j.appender.stdout.Target = System.out\nlog4j.appender.stdout.layout = org.apache.log4j.PatternLayout\nlog4j.appender.stdout.layout.ConversionPattern = [%-5p] %d{yyyy-MM-dd HH:mm:ss,SSS} method:%l%n%m%n\n\n### 输出DEBUG 级别以上的日志到=E://logs/error.log ###\nlog4j.appender.D = org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.D.Append = true\nlog4j.appender.D.Threshold = DEBUG\nlog4j.appender.D.layout = org.apache.log4j.PatternLayout\nlog4j.appender.D.layout.ConversionPattern = %-d{yyyy-MM-dd HH:mm:ss}  [ %t:%r ] - [ %p ]  %m%n\n\n### 输出ERROR 级别以上的日志到=E://logs/error.log ###\n````\n\n### 使用说明\n\n日志记录器(Logger)是日志处理的核心组件。log4j具有5种正常级别(Level)。日志记录器(Logger)的可用级别Level (不包括自定义级别 Level)， 以下内容就是摘自log4j API (http://jakarta.apache.org/log4j/docs/api/index.html):\n\n````java\nstatic Level WARN\nWARN level表明会出现潜在错误的情形。\nstatic Level ERROR\nERROR level指出虽然发生错误事件，但仍然不影响系统的继续运行。\nstatic Level FATAL\nFATAL level指出每个严重的错误事件将会导致应用程序的退出。\n另外，还有两个可用的特别的日志记录级别: (以下描述来自log4j APIhttp://jakarta.apache.org/log4j/docs/api/index.html):\nstatic Level ALL\nALL Level是最低等级的，用于打开所有日志记录。\nstatic Level OFF\nOFF Level是最高等级的，用于关闭所有日志记录。\n````\n\n日志记录器（Logger）的行为是分等级的。如下表所示：\n分为OFF、FATAL、ERROR、WARN、INFO、DEBUG、ALL或者您定义的级别。Log4j建议只使用四个级别，优先级从高到低分别是 ERROR、WARN、INFO、DEBUG。通过在这里定义的级别，您可以控制到应用程序中相应级别的日志信息的开关。比如在这里定义了INFO级别，则应用程序中所有DEBUG级别的日志信息将不被打印出来，也是说大于等于的级别的日志才输出。\n \n日志记录的级别有继承性，子类会记录父类的所有的日志级别。\n\n````java\nlogger日志设置：\n1、加包：log4j-1.2.16.jar  一般还会加入 commons-logging-1.1.1.jar\n2、在CLASSPATH 下建立log4j.properties\n\nlog4j.appender.stdout=org.apache.log4j.ConsoleAppender  \nlog4j.appender.stdout.Target=System.out  \nlog4j.appender.stdout.layout=org.apache.log4j.PatternLayout  \nlog4j.appender.stdout.layout.ConversionPattern=%d %5p %c{1}:%L - %m%n  \n\nlog4j.appender.file=org.apache.log4j.RollingFileAppender  \nlog4j.appender.file.File=${catalina.home}/logs/ddoMsg.log  \n#log4j.appender.file.File=D:/SmgpAppService/logs/smgpApp.log  \nlog4j.appender.file.MaxFileSize=1024KB  \nlog4j.appender.file.MaxBackupIndex=100  \nlog4j.appender.file.layout=org.apache.log4j.PatternLayout  \nlog4j.appender.file.layout.ConversionPattern= %d{yyyy-MM-dd HH:mm:ss} %5p %c %t: - %m%n  \n\n#INFO WARN ERROR DEBUG  \nlog4j.rootLogger=WARN, file, stdout  \n#log4j.rootLogger=INFO,stdout  \norg.apache.commons.logging.Log=org.apache.commons.logging.impl.SimpleLog  \n#org.apache.commons.logging.simplelog.log.org.apache.commons.digester.Digester=debug  \n#org.apache.commons.logging.simplelog.log.org.apache.commons.digester.ObjectCreateRule=debug  \n#org.apache.commons.logging.simplelog.log.org.apache.commons.digester.Digester.sax=info  \n\nlog4j.logger.com.jason.ddoMsg=debug  \n````\n\n在要输出的日志的类中\n定义：\n\n````java\nprivate static final org.apache.log4j.Logger logger = Logger.getLogger(类名.class);\n在类输位置：logger.info(XXX);\n````\n\nlogger 配置说明：\n\n````java\n1、 log4j.rootLogger=INFO, stdout , R\n\n此句为将等级为INFO的日志信息输出到stdout和R这两个目的地，stdout和R的定义在下面的代码，可以任意起名。等级可分为OFF、FATAL、ERROR、WARN、INFO、DEBUG、ALL，如果配置OFF则不打出任何信息，如果配置为INFO这样只显示INFO, WARN, ERROR的log信息，而DEBUG信息不会被显示，具体讲解可参照第三部分定义配置文件中的logger。\n\n2、log4j.appender.stdout=org.apache.log4j.ConsoleAppender\n\n此句为定义名为stdout的输出端是哪种类型，可以是\n\norg.apache.log4j.ConsoleAppender（控制台），\n\norg.apache.log4j.FileAppender（文件），\n\norg.apache.log4j.DailyRollingFileAppender（每天产生一个日志文件），\n\norg.apache.log4j.RollingFileAppender（文件大小到达指定尺寸的时候产生一个新的文件）\n\norg.apache.log4j.WriterAppender（将日志信息以流格式发送到任意指定的地方）\n\n3、log4j.appender.stdout.layout=org.apache.log4j.PatternLayout\n\n此句为定义名为stdout的输出端的layout是哪种类型，可以是\n\norg.apache.log4j.HTMLLayout（以HTML表格形式布局），\n\norg.apache.log4j.PatternLayout（可以灵活地指定布局模式），\n\norg.apache.log4j.SimpleLayout（包含日志信息的级别和信息字符串），\n\norg.apache.log4j.TTCCLayout（包含日志产生的时间、线程、类别等等信息）\n\n4、log4j.appender.stdout.layout.ConversionPattern= [QC] %p [%t] %C.%M(%L) | %m%n\n\n如果使用pattern布局就要指定的打印信息的具体格式ConversionPattern，打印参数如下：\n\n%m 输出代码中指定的消息\n\n%p 输出优先级，即DEBUG，INFO，WARN，ERROR，FATAL\n\n%r 输出自应用启动到输出该log信息耗费的毫秒数\n\n%c 输出所属的类目，通常就是所在类的全名\n\n%t 输出产生该日志事件的线程名\n\n%n 输出一个回车换行符，Windows平台为“rn”，Unix平台为“n”\n\n%d 输出日志时间点的日期或时间，默认格式为ISO8601，也可以在其后指定格式，比如：%d{yyyy MMM dd HH:mm:ss,SSS}，输出类似：2002年10月18日 22：10：28，921\n\n%l 输出日志事件的发生位置，包括类目名、发生的线程，以及在代码中的行数。\n\n[QC]是log信息的开头，可以为任意字符，一般为项目简称。\n\n输出的信息\n\n[TS] DEBUG [main] AbstractBeanFactory.getBean(189) | Returning cached instance of singleton bean 'MyAutoProxy'\n\n\n\n5、 log4j.appender.R=org.apache.log4j.DailyRollingFileAppender\n\n此句与第3行一样。定义名为R的输出端的类型为每天产生一个日志文件。\n\n6、log4j.appender.R.File=D:\\\\Tomcat 5.5\\\\logs\\\\qc.log\n\n此句为定义名为R的输出端的文件名为D:\\\\Tomcat 5.5\\\\logs\\\\qc.log可以自行修改。\n\n7、 log4j.appender.R.layout=org.apache.log4j.PatternLayout\n\n与第4行相同。\n\n8、 log4j.appender.R.layout.ConversionPattern=%d-[TS] %p %t %c - %m%n\n\n与第5行相同。\n\n9、 log4j.logger.com. neusoft =DEBUG\n\n指定com.neusoft包下的所有类的等级为DEBUG。\n\n可以把com.neusoft改为自己项目所用的包名。\n\n10、  log4j.logger.com.opensymphony.oscache=ERROR\n\n11、 log4j.logger.NET.sf.navigator=ERROR\n\n这两句是把这两个包下出现的错误的等级设为ERROR，如果项目中没有配置EHCache，则不需要这两句。\n\n12、log4j.logger.org.apache.commons=ERROR\n\n13、 log4j.logger.org.apache.struts=WARN\n\n这两句是struts的包。\n\n14、  log4j.logger.org.displaytag=ERROR\n\n这句是displaytag的包。（QC问题列表页面所用）\n\n15、 log4j.logger.org.springframework=DEBUG\n\n此句为spring的包。\n\n16、 log4j.logger.org.hibernate.ps.PreparedStatementCache=WARN\n\n17、log4j.logger.org.hibernate=DEBUG\n\n此两句是hibernate的包。\n\n以上这些包的设置可根据项目的实际情况而自行定制。\n````","tags":["Java","Log"]},{"title":"Log4j2使用","url":"/2022/09/07/cyb-mds/java/log/Log4j2/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n\n\n#### springboot方式\n\n```xml\n<!--pom.xml引入-->\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-log4j2</artifactId>\n</dependency>\n```\n\n\n\n#### 配置详解\n\n```yaml\nConfiguration:\n  status: warn\n  name: YAMLConfig\n  log-impl: org.apache.ibatis.logging.stdout.StdOutImpl\n  \n  properties: #自定义一些常量，之后使用${变量名}引用\n    property:\n      # 格式化输出：%date表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度 %msg：日志消息，%n是换行符\n      # %d{HH:mm:ss.SSS}——日志输出时间\n      # %thread——输出日志的进程名字，这在Web应用以及异步任务处理中很有用\n      # %-5level——日志级别，并且使用5个字符靠左对齐\n      # %logger- ——日志输出者的名字\n      # %msg——日志消息\n      # %n——平台的换行符\n      - name: LOG_FILE_SIZE #名字\n        value: 500MB #值\n      - name: FILE_PATH\n        value: ${sys:logPath} \n        \n  #appenders:定义输出内容,输出格式,输出方式,日志保存策略等,常用其下三种标签[console,File,RollingFile]。appenders有哪些分类，见下图\n  appenders: \n    #console :控制台输出的配置\n    Console: \n      name: CONSOLE\n      target: SYSTEM_OUT\n      #PatternLayout :输出日志的格式,LOG4J2定义了输出代码,详见第二部分\n      PatternLayout:\n        charset: UTF-8\n        Pattern: '[%d{yyyy-MM-dd HH:mm:ss:SSS}] [%p] - %l - %m%n'\n      #过滤器，后面详细介绍几类过滤器\n      ThresholdFilter:\n        level: info  #过滤器等级\n        onMatch: ACCEPT  #匹配  ACCEPT:接收，DENY:直接拒绝，NEUTRAL:中立，放过，有后面过滤器处理\n        onMismatch: DENY  #不匹配\n    SMTP: #邮件发送日志\n      name: Mail\n      subject: \"ERROR等级日志\" #邮件标题\n      to: 1823795959@qq.com #发给1,发给2,发给3,....\n      cc: 1823795959@qq.com #抄送给谁，多个就逗号分开\n      from: 1823795959@qq.com #发送的邮件\n      smtpHost: smtp.qq.com #发送邮箱服务器\n      smtpHost: 25 #邮箱端口\n      smtpProtocol: smtp #协议\n      smtpUsername: zhanbei #发送名称\n      smtpPassword: 123456 #发送者的密码，开启SMTP时候的那一串密码\n      smtpDebug: false #是否开启发送邮箱调试模式\n      bufferSize: 1024 #缓存区大小\n      filter:  #过滤器，稍后详解\n    RollingRandomAccessFile: #也有RollingFile，日志滚动策略配置，RollingRandomAccessFile标识有缓存的日志滚动。\n      -\n        name: ROLLING_FILE_INFO #策略名称\n        fileName: ${FILE_PATH}info.log  # 日志文件\n        filePattern: ${FILE_PATH}info-%d{yyyy-MM-dd}_%i.log.gz  #指定当发生Rolling时,文件的转移和重命名规则\n        PatternLayout: #配置日志打印格式化内容\n          charset: UTF-8 #编码\n          Pattern: '%highlight{%date{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n}{INFO=bright white}'\n          #格式，具体%logger,%level表示的是啥，有啥，见官方文档\n        ThresholdFilter: #配置过滤器，过滤器类型见后面\n          level: info #等级\n          onMatch: ACCEPT #匹配后，决定\n          onMismatch: DENY #不匹配后策略\n        Policies:  #Policies :日志滚动策略\n          TimeBasedTriggeringPolicy: #时间滚动策略\n            interval: 1 #时间间隔，1s\n          SizeBasedTriggeringPolicy: \n            size: ${LOG_FILE_SIZE} #文件大小滚动\n        DefaultRolloverStrategy:  #默认滚动策略，同个文件夹中允许最多日志文件\n          max: 30  # DefaultRolloverStrategy属性如不设置，则默认为最多同一文件夹下7个文件，这里设置30个\n      -\n        name: ROLLING_FILE_ERROR\n        fileName: ${FILE_PATH}error.log\n        filePattern: ${FILE_PATH}error-%d{yyyy-MM-dd}_%i.log.gz\n        PatternLayout:\n          charset: UTF-8\n          Pattern: '%highlight{%d{yyyy-MM-dd HH:mm:ss.SSS} %-5level %class{36} %L %M - %msg%xEx%rEx%n}{INFO=bright white}'\n        ThresholdFilter:\n          level: error\n          onMatch: ACCEPT\n          onMismatch: DENY\n        Policies:\n          TimeBasedTriggeringPolicy:\n            interval: 1\n          SizeBasedTriggeringPolicy:\n            size: ${LOG_FILE_SIZE}\n        DefaultRolloverStrategy:\n          max: 30\n      -\n        name: ROLLING_P_FILE\n        fileName: ${FILE_PATH}p.log\n        filePattern: ${FILE_PATH}pay-%d{yyyy-MM-dd}_%i.log.gz\n        PatternLayout:\n          charset: UTF-8\n          Pattern: '%highlight{%date{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level - %msg%n}{INFO=bright white}'\n        ThresholdFilter:\n          level: info\n          onMatch: ACCEPT\n          onMismatch: DENY\n        Policies:\n          TimeBasedTriggeringPolicy:\n            interval: 1\n          SizeBasedTriggeringPolicy:\n            size: ${LOG_FILE_SIZE}\n        DefaultRolloverStrategy:\n          max: 30\n      -  #用户登录日志\n        name: ROLLING_ACCESS_LOG_FILE\n        fileName: ${FILE_PATH}access.log\n        filePattern: ${FILE_PATH}access-%d{yyyy-MM-dd}_%i.log.gz\n        PatternLayout:\n          charset: UTF-8\n          Pattern: '%highlight{%date{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{10} %class{36} %L %M - %msg%n}{INFO=bright white}'\n        ThresholdFilter:\n          level: info\n          onMatch: ACCEPT\n          onMismatch: DENY\n        Policies:\n          TimeBasedTriggeringPolicy:\n            interval: 1\n          SizeBasedTriggeringPolicy:\n            size: ${LOG_FILE_SIZE}\n        DefaultRolloverStrategy:\n          max: 30\n      -  #第三方调用日志\n        name: ROLLING_THIRDPART_LOG_FILE\n        fileName: ${FILE_PATH}thirdpart.log\n        filePattern: ${FILE_PATH}thirdpart-%d{yyyy-MM-dd}_%i.log.gz\n        PatternLayout:\n          charset: UTF-8\n          Pattern: '%highlight{%date{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{10} %class{36} %L %M - %msg%n}{INFO=bright white}'\n        ThresholdFilter:\n          level: info\n          onMatch: ACCEPT\n          onMismatch: DENY\n        Policies:\n          TimeBasedTriggeringPolicy:\n            interval: 1\n          SizeBasedTriggeringPolicy:\n            size: ${LOG_FILE_SIZE}\n        DefaultRolloverStrategy:\n          max: 30\n\n  #    <SMTP name=\"Mail\" subject=\"****SaaS系统正式版异常信息\" to=\"message@message.info\" from=\"message@lengjing.info\" smtpUsername=\"message@message.info\" smtpPassword=\"LENG****1234\" smtpHost=\"mail.lengjing.info\" smtpDebug=\"false\" smtpPort=\"25\" bufferSize=\"10\">\n  #    <PatternLayout pattern=\"[%-5p]:%d{YYYY-MM-dd HH:mm:ss} [%t] %c{1}:%L - %msg%n\" />\n  #    </SMTP>\n  Loggers:  #定义logger\n    AsyncLogger:  #定义异步logger，Logger:是同步的。异步会提高代码性能，单独建立一个进程，进行日志打印和管理。这个线程由disruptor管理。disruptor要单独pom引入\n      - name: org.apache.kafka #名称，可以使报名，类路径，直接可以作用在该类上或者某一包下所有的类。如果自定义名称如：name:wangdacui,在springboot中@slf4h(topic=\"wangdacui\")，就可以用了。\n        additivity: false  \n        #是否重复，默认true，设置false，这表示在root定义日志记录器，不再重复打印。\n        #所有自定义的logger都是集成root里面的，所以默认会在root打印后再在自定义的logger中打印，\n        #additivity:false 标识和root日志记录器断绝关系。\n        level: ERROR #定义日志记录器记录日志等级\n        #Logger节点用来单独指定日志的形式，name为包路径,比如要为org.springframework包下所有日志指定为INFO级别等\n      - name: cn.itsource\n        additivity: false\n      - name: AccessLog\n        level: info\n        additivity: false\n        AppenderRef:\n          ref: ROLLING_ACCESS_LOG_FILE\n      - name: ThirdPartLog\n        level: info\n        additivity: false\n        AppenderRef:\n          ref: ROLLING_THIRDPART_LOG_FILE\n    Root: #设置根节点日志记录器，和自定义日志记录器模式一样\n      level: info\n      AppenderRef:\n        - ref: ROLLING_FILE_INFO\n        - ref: ROLLING_FILE_ERROR\n```\n\n","tags":["Java","Log"]},{"title":"Springboot Security使用","url":"/2022/09/07/cyb-mds/java/spring/springboot security使用/","content":"\n==作者：YB-Chi==\n\n\n\n","tags":["java","code","spring"]},{"title":"JVM性能优化， Part 3  ―― 垃圾回收","url":"/2022/09/07/cyb-mds/java/jvm/JVM性能优化， Part 3  ―― 垃圾回收/","content":"\n==作者：Eva Andreasson,译者：曹旭东==\n\n[toc]\n\nJava平台的垃圾回收机制大大提高的开发人员的生产力，但实现糟糕的垃圾回收器却会大大消耗应用程序的资源。本文作为JVM性能优化系列的第3篇，Eva Andeasson将为Java初学者介绍Java平台的内存模型和GC机制。她将解释为什么碎片化（不是GC）是Java应用程序出现性能问题的主要原因，以及为什么当前主要通过分代垃圾回收和压缩，而不是其他最具创意的方法，来解决Java应用程序中碎片化的问题。\n\n垃圾回收（GC）是旨在释放不可达Java对象所占用的内存的过程，是Java virtual machine（JVM）中动态内存管理系统的核心组成部分。在一个典型的垃圾回收周期中，所有仍被引用的对象，即可达对象，会被保留。没有被引用的Java对象所占用的内存会被释放并回收，以便分配给新创建的对象。\n\n为了更好的理解垃圾回收与各种不同的GC算法，你首先需要了解一些关于Java平台内存模型的内容。\n\n#### **垃圾回收与Java平台内存模型**\n\n当你在启动Java应用程序时指定了启动参数_-Xmx_（例如，java -Xmx2g MyApp），则相应大小的内存会被分配给Java进程。这块内存即所谓的*Java堆*（或简称为*堆*）。这块专用的内存地址空间用于存储Java应用程序（有时是JVM）所创建的对象。随着Java应用程序的运行，会不断的创建新对象并为之分配内存，Java堆（即地址空间）会逐渐被填满。\n\n最后，Java堆会被填满，这就是说想要申请内存的线程无法获得一块足够大的连续空闲空间来存放新创建的对象。此时，JVM判断需要启动垃圾回收器来回收内存了。当Java程序调用System.gc()方法时，也有可能会触发垃圾回收器以执行垃圾回收的工作。使用System.gc()方法并不能保证垃圾回收工作肯定会被执行。在执行垃圾回收前，垃圾回收机制首先会检查当前是否是一个“恰当的时机”，而“恰当的时机”指所有的应用程序活动线程都处于安全点（safe point），以便启动垃圾回收。简单举例，为对象分配内存时，或正在优化CPU指令（参见本系列的[前一篇文章](http://www.javaworld.com/javaworld/jw-09-2012/120905-jvm-performance-optimization-compilers.html)）时，就不是“恰当的时机”，因为你可能会丢失上下文信息，从而得到混乱的结果。\n\n垃圾回收不应该回收当前有活动引用指向的对象所占用的内存；因为这样做将违反[JVM规范](http://docs.oracle.com/javase/specs/jvms/se7/html/index.html)。在JVM规范中，并没有强制要求垃圾回收器立即回收已死对象（dead object）。已死对象最终会在后续的垃圾回收周期中被释放掉。目前，已经有多种垃圾回收的实现，它们都包含两个沟通的假设。对垃圾回收来说，真正的挑战在于标识出所有活动对象（即仍有引用指向的对象），回收所有不可达对象所占用的内存，并尽可能不对正在运行的应用程序产生影响。因此，垃圾回收器运行的两个目标：\n\n1.  快速释放不可达对象所占用的内存，防止应用程序出现OOM错误。\n2.  回收内存时，对应用程序的性能（指延迟和吞吐量）的影响要紧性能小。\n\n#### **两类垃圾回收**\n\n在本系列的[第一篇文章](https://github.com/chiyuanbo/cyb-mds/blob/master/java/jvm/JVM%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%20Part%201%20%E2%80%95%E2%80%95%20JVM%E7%AE%80%E4%BB%8B.md)中，我提到了2种主要的垃圾回收方式，引用计数（reference counting）和引用追踪（tracing collector。译者注，在第一篇中，给出的名字是“reference tracing”，这里仍沿用之前的名字）。这里，我将深入这两种垃圾回收方式，并介绍用于生产环境的实现了引用追踪的垃圾回收方式的相关算法。\n\n\n##### **引用计数垃圾回收器**\n\n引用计数垃圾回收器会对指向每个Java对象的引用数进行跟踪。一旦发现指向某个对象的引用数为0，则立即回收该对象所占用的内存。引用计数垃圾回收的主要优点就在于可以立即访问被回收的内存。垃圾回收器维护未被引用的内存并不需要消耗很大的资源，但是保持并不断更新引用计数却代价不菲。\n\n使用引用计数方式执行垃圾回收的主要困难在于保持引用计数的准确性，而另一个众所周知的问题在于解决循环引用结构所带来的麻烦。如果两个对象互相引用，并且没有其他存活东西引用它们，那么这两个对象所占用的内存将永远不会被释放，两个对象都会因引用计数不为0而永远存活下去。要解决循环引用带来的问题需要，而这会使算法复杂度增加，从而影响应用程序的运行性能。\n\n##### **引用跟踪垃圾回收器**\n\n引用跟踪垃圾回收器基于这样一种假设，所有存活对象都可以通过迭代地跟踪从已知存活对象集中对象发出的引用及引用的引用来找到。可以通过对寄存器、全局域、以及触发垃圾回收时栈帧的分析来确定初始存活对象的集合（称为“根对象”，或简称为“根”）。在确定了初始存活对象集后，引用跟踪垃圾回收器会跟踪从这些对象中发出的引用，并将找到的对象标记为“活的（live）”。标记所有找到的对象意味着已知存活对象的集合会随时间而增长。这个过程会一直持续到所有被引用的对象（因此是“存活的”对象）都被标记。当引用跟踪垃圾回收器找到所有存活的对象后，就会开始回收未被标记的对象。\n\n不同于引用计数垃圾回收器，引用跟踪垃圾回收器可以解决循环引用的问题。由于标记阶段的存在，大多数引用跟踪垃圾回收器无法立即释放“已死”对象所占用的内存。\n\n引用跟踪垃圾回收器广泛用于动态语言的内存管理；到目前为止，在Java编程语言的视线中也是应用最广的，并且在多年的商业生产环境中，已经证明其实用性。在本文余下的内容中，我将从一些相关的实现算法开始，介绍引用跟踪垃圾回收器，\n\n#### **引用跟踪垃圾回收器算法**\n\n拷贝和*标记-清理*垃圾回收算法并非新近发明，但仍然是当今实现引用跟踪垃圾回收器最常用的两种算法。\n\n##### **拷贝垃圾回收器**\n\n传统的拷贝垃圾回收器会使用一个“from”区和一个“to”区，它们是堆中两个不同的地址空间。在执行垃圾回收时，from区中存活对象会被拷贝到to区。当from区中所有的存活对象都被拷贝到to后，垃圾回收器会回收整个from区。当再次分配内存时，会首先从to区中的空闲地址开始分配。\n\n在该算法的早期实现中，from区和to区会在垃圾回收周期后进行交换，即当to区被填满后，将再次启动垃圾回收，这是to区会“变成”from区。如图Figure 1所示。\nFigure 1\\. A traditional copying garbage collection sequence\n\n在该算法的近期实现中，可以将堆中任意地址空间指定为from区和to区，这样就不再需要交换from区和to区，堆中任意地址空间都可以成为from区或to区。\n\n拷贝垃圾回收器的一个优点是存活对象的位置会被to区中重新分配，紧凑存放，可以完全消除碎片化。碎片化是其他垃圾回收算法所要面临的一大问题，这点会在后续讨论。\n\n##### **拷贝垃圾回收的缺陷**\n\n通常来说，拷贝垃圾回收器是“stop-the-world”式的，即在垃圾回收周期内，应用程序是被挂起的，无法工作。在“stop-the-world”式的实现中，所需要拷贝的区域越大，对应用程序的性能所造成的影响也越大。对于那些非常注重响应时间的应用程序来说，这是难以接受的。使用拷贝垃圾回收时，你还需要考虑一下最坏情况，即当from区中所有的对象都是存活对象的时候。因此，你不得不给存活对象预留出足够的空间，也就是说to区必须足够大，大到可以将from区中所有的对象都放进去。正是由于这个缺陷，拷贝垃圾回收算法在内存使用效率上略有不足。\n\n##### **标记-清理垃圾回收器**\n\n大多数部署在企业生产环境的商业JVM都使用了标记-清理（或标记）垃圾回收器，这种垃圾回收器并不会想拷贝垃圾回收器那样对应用程序的性能有那么大的影响。其中最著名的几款是CMS、G1、GenPar和DeterministicGC（参见[相关资源](https://github.com/caoxudong/translation/blob/master/java/jvm/JVM_performance_optimization_Part_3_Garbage_collection.md#resources)）。\n标记-清理垃圾回收器会跟踪引用，并使用标记位将每个找到的对象标记位“live”。通常来说，每个标记位都关联着一个地址或堆上的一个地址集合。例如，标记位可能是对象头（object header）中一位，一个位向量，或是一个位图。\n\n当所有的存活对象都被标记位“live”后，将会开始*清理*阶段。一般来说，垃圾回收器的清理阶段包含了通过再次遍历堆（不仅仅是标记位live的对象集合，而是整个堆）来定位内存地址空间中未被标记的区域，并将其回收。然后，垃圾回收器会将这些被回收的区域保存到空闲列表（free list）中。在垃圾回收器中可以同时存在多个空闲列表——通常会按照保存的内存块的大小进行划分。某些JVM（例如JRockit实时系统， JRockit Real Time System）在实现垃圾回收器时会给予应用程序分析数据和对象大小统计数据来动态调整空闲列表所保存的区域块的大小范围。\n\n当清理阶段结束后，应用程序就可以再次启动了。给新创建的对象分配内存时会从空闲列表中查找，而空闲列表中内存块的大小需要匹配于新创建的对象大小、某个线程中平均对象大小，或应用程序所设置的TLAB的大小。从空闲列表中为新创建的对象找到大小合适的内存区域块有助于优化内存的使用，减少内存中的碎片。\n\n\n##### **标记-清理垃圾回收器的缺陷**\n\n标记阶段的时长取决于堆中存活对象的总量，而清理阶段的时长则依赖于堆的大小。由于在*标记*阶段和*清理*阶段完成前，你无事可做，因此对于那些具有较大的堆和较多存活对象的应用程序来说，使用此算法需要想办法解决暂停时间（pause-time）较长这个问题。\n对于那些内存消耗较大的应用程序来说，你可以使用一些GC调优选项来满足其在某些场景下的特殊需求。很多时候，调优至少可以将标记-清理阶段给应用程序或性能要求（SLA，SLA指定了应用程序需要达到的响应时间的要求，即延迟）所带来的风险推后。当负载和应用程序发生改变后，需要重新调优，因为某次调优只对特定的工作负载和内存分配速率有效。\n\n#### **标记-清理算法的实现**\n\n目前，标记-清理垃圾回收算法至少已有2种商业实现，并且都已在生产环境中被证明有效。其一是并行垃圾回收，另一个是并发（或多数时间并发）垃圾回收。\n\n##### **并行垃圾回收器**\n\n并行垃圾回收指的是垃圾回收是多线程并行完成的。大多数商业实现的并行垃圾回收器都是stop-the-world式的垃圾回收器，即在整个垃圾回收周期结束前，所有应用程序线程都会被挂起。挂起所有应用程序线程使垃圾回收器可以以并行的方式，更有效的完成标记和清理工作。并行使得效率大大提高，通常可以在像[SPECjbb](http://www.spec.org/jbb2005/)这样的吞吐量基准测试中跑出高分。如果你的应用程序好似有限考虑吞吐量的，那么并行垃圾回收是你最好的选择。\n\n对于大多数并行垃圾回收器来说，尤其是考虑到应用于生产环境中，最大的问题是，像拷贝垃圾回收算法一样，在垃圾回收周期内应用程序无法工作。使用stop-the-world式的并行垃圾回收会对优先考虑响应时间的应用程序产生较大影响，尤其是当你有大量的引用需要跟踪，而此时恰好又有大量的、具有复杂结构的对象存活于堆中的时候，情况将更加糟糕。（记住，标记-清理垃圾回收器回收内存的时间取决于跟踪存活对象中所有引用的时间与遍历整个堆的时间之和。）以并行方式执行垃圾回收所导致的应用程序暂停会一直持续到整个垃圾回收周期结束。\n\n##### **并发垃圾回收器**\n\n并发垃圾回收器更适用于那些对响应时间比较敏感的应用程序。并发指的是一些（或大多数）垃圾回收工作可以与应用程序线程同时运行。由于并非所有的资源都由垃圾回收器使用，因此这里所面临的问题如何决定何时开始执行垃圾回收，可以保证垃圾回收顺利完成。这里需要足够的时间来跟踪存活对象即的引用，并在应用程序出现OOM错误前回收内存。如果垃圾回收器无法及时完成，则应用程序就会抛出OOM错误。此外，一直做垃圾回收也不好，会不必要的消耗应用程序资源，从而影响应用程序吞吐量。要想在动态环境中保持这种平衡就需要一些技巧，因此设计了启发式方法来决定何时开始垃圾回收，何时执行不同的垃圾回收优化任务，以及一次执行多少垃圾回收优化任务等。\n\n并发垃圾回收器所面临的另一个挑战是如何决定何时执行一个需要完整堆快照的操作时安全的，例如，你需要知道是何时标记所有存活对象的，这样才能转而进入清理阶段。在大多数并行垃圾回收器采用的stop-the-world方式中，*阶段转换（phase-switching）*并不需要什么技巧，因为世界已静止（堆上对象暂时不会发生变化）。但是，在并发垃圾回收中，转换阶段时可能并不是安全的。例如，如果应用程序修改了一块垃圾回收器已经标记过的区域，可能会涉及到一些新的或未被标记的引用，而这些引用使其指向的对象成为存活状态。在某些并发垃圾回收的实现中，这种情况有可能会使应用程序陷入长时间运行重标记（re-mark）的循环，因此当应用程序需要分配内存时无法得到足够做的空闲内存。\n\n到目前为止的讨论中，已经介绍了各种垃圾回收器和垃圾回收算法，他们各自适用于不同的场景，满足不同应用程序的需求。各种垃圾回收方式不仅在算法上有所区别，在具体实现上也不尽相同。所以，在命令行中指定垃圾回收器之前，最好能了解应用程序的需求及其自身特点。在下一节中，将介绍Java平台内存模型中的陷阱，在这里，陷阱指的是在动态生产环境中，Java程序员常常做出的一些中使性能更糟，而非更好的假设。\n\n#### **为什么调优无法取代垃圾回收**\n\n大多数Java程序员都知道，如果有不少方法可以最大化Java程序的性能。而当今众多的JVM实现，垃圾回收器实现，以及多到令人头晕的调优选项都可能会让开发人员将大量的时间消耗在无穷无尽的性能调优上。这种情况催生了这样一种结论，“GC是糟糕的，努力调优以降低GC的频率或时长才是王道”。但是，真这么做是有风险的。\n\n考虑一下针对指定的应用程序需求做调优意味着什么。大多数调优参数，如内存分配速率，对象大小，响应时间，以及对象死亡速度等，都是针对特定的情况而来设定的，例如测试环境下的工作负载。例如。调优结果可能有以下两种：\n\n1.  测试时正常，上线就失败。\n2.  一旦应用程序本身，或工作负载发生改变，就需要全部重调。\n\n调优是需要不断往复的。使用并发垃圾回收器需要做很多调优工作，尤其是在生产环境中。为满足应用程序的需求，你需要不断挑战可能要面对的最差情况。这样做的结果就是，最终形成的配置非常刻板，而且在这个过程中也浪费了大量的资源。这种调优方式（试图通过调优来消除GC）是一种堂吉诃德式的探索——以根本不存在的理由去挑战一个假想敌。而事实是，你针对某个特定的负载而垃圾回收器做的调优越多，你距离Java运行时的动态特性就越远。毕竟，有多少应用程序的工作负载能保持不变呢？你所预估的工作负载的可靠性又有多高呢？\n\n那么，如果不从调优入手又该怎么办呢？有什么其他的办法可以防止应用程序出现OOM错误，并降低响应时间呢？这里，首先要做的是明确影响Java应用程序性能的真正因素。\n\n##### **碎片化**\n\n影响Java应用程序性能的罪魁祸首并不是垃圾回收器本身，而是碎片化，以及垃圾回收器如何处理碎片。碎片是Java堆中空闲空间，但由于连续空间不够大而无法容纳将要创建的对象。正如我在本系列[第2篇](https://github.com/chiyuanbo/cyb-mds/blob/master/java/jvm/JVM%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%20Part%202%20%E2%80%95%E2%80%95%20%E7%BC%96%E8%AF%91%E5%99%A8.md)中提到的，碎片可能是TLAB中的剩余空间，也可能是（这种情况比较多）被释放掉的具有较长生命周期的小对象所占用的空间。\n\n随着应用程序的运行，这种无法使用的碎片会遍布于整个堆空间。在某些情况下，这种状态会因静态调优选项（如提升速率和空闲列表等）更糟糕，以至于无法满足应用程序的原定需求。这些剩下的空间（也就是碎片）无法被应用程序有效利用起来。如果你对此放任自流，就会导致不断垃圾回收，垃圾回收器会不断的释放内存以便创建新对象时使用。在最差情况下，甚至垃圾回收也无法腾出足够的内存空间（因为碎片太多），JVM会强制抛出OOM（out of memory）错误当然，你也可以重启应用程序来消除碎片，这样可以使Java堆焕然一新，于是就又可以为对象分配内存了。但是，重新启动会导致服务器停机，另外，一段时间之后，堆将再次充满碎片，你也不得不再次重启。\n\nOOM错误（OutOfMemoryErrors）会挂起进程，日志中显示的垃圾回收器很忙，是垃圾回收器努力释放内存的标志，也说明了堆中碎片非常多。一些开发人员通过重新调优垃圾回收器来解决碎片化的问题，但我觉着在解决碎片问题成为垃圾回收的使命之前应该用一些更有新意的方法来解决这个问题。本文后面的内容将聚焦于能有效解决碎片化问题的方法：分代黛式垃圾回收和压缩。\n\n#### **分代式垃圾回收**\n\n这个理论你可以已经听说过，即在生产环境中，大部分对象的生命周期都很短。分代式垃圾回收就源于这个理论。在分代式垃圾回收中，堆被分为两个不同的空间（或成为“代”），每个空间存放具有不同年龄的对象，在这里，年龄是指该对象所经历的垃圾回收的次数（也就是该对象挺过了多少次垃圾回收而没有死掉）。\n\n当新创建的对象所处的空间，即*年轻代*，被对象填满后，该空间中仍然存活的对象会被移动到老年代。（译者注，以HotSpot为例，这里应该是挺过若干次GC而不死的，才会被搬到老年代，而一些比较大的对象会直接放到老年代。）大多数的实现都将堆会分为两代，年轻代和老年代。通常来说，分代式垃圾回收器都是单向拷贝的，即从年轻代向老年代拷贝，这点在早先曾讨论过。近几年出现的年轻代垃圾回收器已经可以实现并行垃圾回收，当然也可以实现一些其他的垃圾回收算法实现对年轻代和老年代的垃圾回收。如果你使用拷贝垃圾回收器（可能具有并行收集功能）对年轻代进行垃圾回收，那垃圾回收是stop-the-world式的（参见前面的解释）。\n\n##### **分代式垃圾回收的缺陷**\n\n在分代式垃圾回收中，老年代执行垃圾回收的平率较低，而年轻代中较高，垃圾回收的时间较短，侵入性也较低。但在某些情况下，年轻代的存在会是老年代的垃圾回收更加频繁。典型的例子是，相比于Java堆的大小，年轻代被设置的太大，而应用程序中对象的生命周期又很长（又或者给年轻代对象提升速率设了一个“不正确”的值）。在这种情况下，老年代因太小而放不下所有的存活对象，因此垃圾回收器就会忙于释放内存以便存放从年轻代提升上来的对象。但一般来说，使用分代式垃圾回收器可以使用应用程序的性能和系统延迟保持在一个合适的水平。\n\n使用分代式垃圾回收器的一个额外效果是部分解决了碎片化的问题，或者说，发生最差情况的时间被推迟了。可能造成碎片的小对象被分配于年轻代，也在年轻代被释放掉。老年代中的对象分布会相对紧凑一些，因为这些对象在从年轻代中提升上来的时候会被会紧凑存放。但随着应用程序的运行，如果运行时间够长的话，老年代也会充满碎片的。这时就需要对年轻代和老年代执行一次或多次stop-the-world式的全垃圾回收，导致JVM抛出_OOM错误_或者表明提升失败的错误。但年轻代的存在使这种情况的出现被推迟了，对某些应用程序来说，这就就足够了。（在某些情况下，这种糟糕情况会被推迟到应用程序完全不关心GC的时候。）对大多数应用程序来说，对于大多数使用年轻代作为缓冲的应用程序来说，年轻代的存在可以降低出现stop-the-world式垃圾回收频率，减少抛出OOM错误的次数。\n\n##### **调优分代式垃圾回收**\n\n正如上面提到的，由于使用了分代式垃圾回收，你需要针对每个新版本的应用程序和不同的工作负载来调整年轻代大小和对象提升速度。我无法完整评估出固定运行时的代价：由于针对某个指定工作负载而设置了一系列优化参数，垃圾回收器应对动态变化的能力降低了，而变化是不可避免的。\n\n对于调整年轻代大小来说，最重要的规则是要确保年轻代的大小不应该使因执行stop-the-world式垃圾回收而导致的暂停过长。（假设年轻代中使用的并行垃圾回收器。）还要记住的是，你要在堆中为老年代留出足够的空间来存放那些生命周期较长的对象。下面还有一些在调优分代式垃圾回收器时需要考虑的因素：\n\n1.  大多数年轻代垃圾回收都是stop-the-world式的，年轻代越大，相应的暂停时间越长。所以，对于那些受GC暂停影响较大的应用程序来说，应该仔细斟酌年轻代的大小。\n\n2.  你可以综合考虑不同代的垃圾回收算法。可以在年轻代使用并行垃圾回收，而在老年代使用并行垃圾回收。\n\n3.  当提升失败频繁发生时，这通常说明老年代中的碎片较多。提升失败指的是老年代中没有足够大的空间来存放年轻代中的存活对象。当出现提示失败时，你可以微调对象提升速率（即调整对象提升时年龄），或者确保老年代垃圾回收算法会将对象进行压缩（将在下一节讨论），并以一种适合当前应用程序工作负载的方式调整压缩。你也可以增大堆和各个代的大小，但这会使老年代垃圾回收的暂停时间延长——记住，碎片化是不可避免的。\n\n4.  分代式垃圾回收最适用于那些具有大量短生命周期对象的应用程序，这些对象的生命周期短到活不过一次垃圾回收周期。在这种场景中，分代式垃圾回收可有效的减缓碎片化的趋势，主要是将碎片化随带来的影响推出到将来，而那时可能应用程序对此毫不关心。\n\n#### **压缩**\n\n尽管分代式垃圾回收推出了碎片化和OOM错误出现的时机，但压缩仍然是唯一真正解决碎片化的方法。*压缩*是将对象移动到一起，以便释放掉大块连续内存空间的GC策略。因此，压缩可以生成足够大的空间来存放新创建的对象。\n\n移动对象并修改相关引用是一个stop-the-world式的操作，这会对应用程序的性能造成影响。（只有一种情况是个例外，将在本系列的下一篇文章中讨论。）存活对象越多，垃圾回收造成的暂停也越长。假如堆中的空间所剩无几，而且碎片化又比较严重（这通常是由于应用程序运行的时间很长了），那么对一块存活对象多的区域进行压缩可能会耗费数秒的时间。而如果因出现OOM而导致应用程序无法运行，因此而对整个堆进行压缩时，所消耗的时间可达数十秒。\n\n压缩导致的暂停时间的长短取决于需要移动的存活对象所占用的内存有多大以及有多少引用需要更新。当堆比较大时，从统计上讲，存活对象和需要更新的引用都会很多。从已观察到的数据看，每压缩1到2GB存活数据的需要约1秒钟。所以，对于4GB的堆来说，很可能会有至少25%的存活数据，从而导致约1秒钟的暂停。\n\n##### **压缩与应用程序内存墙**\n\n应用程序内存墙涉及到在GC暂停时间对应用程序的影响大到无法达到满足预定需求之前所能设置的的堆的最大值。目前，大部分Java应用程序在碰到内存墙时，每个JVM实例的堆大小介于4GB到20GB之间，具体数值依赖于具体的环境和应用程序本身。这也是大多数企业及应用程序会部署多个小堆JVM而不是部署少数大堆（50到60GB）JVM的原因之一。在这里，我们需要思考一下：现代企业中有多少Java应用程序的设计与部署架构受制于JVM中的压缩？在这种情况下，我们接受多个小实例的部署方案，以增加管理维护时间为代价，绕开为处理充满碎片的堆而执行stop-the-world式垃圾回收所带来的问题。考虑到现今的硬件性能和企业级Java应用程序中对内存越来越多的访问要求，这种方案是在非常奇怪。为什么仅仅只能给每个JVM实例设置这么小的堆？并发压缩是一种可选方法，它可以降低内存墙带来的影响，这将是本系列中下一篇文章的主题。\n\n从已观察到的数据看，每压缩1到2GB存活数据的需要约1秒钟。所以，对于4GB的堆来说，很可能会有至少25%的存活数据，从而导致约1秒钟的暂停。\n\n#### **总结：回顾**\n\n本文对垃圾回收做了总体介绍，目的是为了使你能了解垃圾回收的相关概念和基本知识。希望本文能激发你继续深入阅读相关文章的兴趣。这里所介绍的大部分内容，它们。在下一篇文章中，我将介绍一些较新颖的概念，并发压缩，目前只有Azul公司的Zing JVM实现了这一技术。并发压缩是对GC技术的综合运用，这些技术试图重新构建Java内存模型，考虑当今内存容量与处理能力的不断提升，这一点尤为重要。\n\n现在，回顾一下本文中所介绍的关于垃圾回收的一些内容：\n\n1.  不同的垃圾回收算法的方式是为满足不同的应用程序需求而设计。目前在商业环境中，应用最为广泛的是引用跟踪垃圾回收器。\n\n2.  并行垃圾回收器会并行使用可用资源执行垃圾回收任务。这种策略的常用实现是stop-the-world式垃圾回收器，使用所有可用系统资源快速完成垃圾回收任务。因此，并行垃圾回收可以提供较高的吞吐量，但在垃圾回收的过程中，所有应用程序线程都会被挂起，对延迟有较大影响。\n\n3.  并发垃圾回收器可以与应用程序并发工作。使用并发垃圾回收器时要注意的是，确保在应用程序发生OOM错误之前完成垃圾回收。\n\n4.  分代式垃圾回收可以推迟碎片化的出现，但并不能消除碎片化。它将堆分为两块空间，一块用于存放“年轻对象”，另一块用于存放从年轻代中存活下来的存活对象。对于那些使用了很多具有较短生命周期活不过几次垃圾回收周期的Java应用程序来说，使用分代式垃圾回收是非常合适的。\n\n5.  压缩是可以完全解决碎片化的唯一方法。大多数垃圾回收器在压缩的时候是都stop-the-world式的。应用程序运行的时间越长，对象间的引就用越复杂，对象大小的异质性也越高。相应的，完成压缩所需要的时间也越长。如果堆的大小较大的话也会对压缩所占产生的暂停有影响，因为较大的堆就会有更多的活动数据和更多的引用需要处理。\n\n6.  调优可以推迟OOM错误的出现，但过度调优是无意义的。在通过试错方式初始调优前，一定要明确生产环境负载的动态性，以及应用程序中的对象类型和对象间的引用情况。在动态负载下，过于刻板的配置很容会失效。在设置非动态调优选项前一定要清楚这样做后果。\n\n#### **JVM 性能优化系列**\n\n第一篇 《[JVM性能优化， Part 1 ―― JVM简介](https://chiyuanbo.github.io/2018/04/28/JVM%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%20Part%201%20%E2%80%95%E2%80%95%20JVM%E7%AE%80%E4%BB%8B/) 》\n\n第二篇《[JVM性能优化， Part 2 ―― 编译器](https://chiyuanbo.github.io/2018/04/28/JVM%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%20Part%202%20%E2%80%95%E2%80%95%20%E7%BC%96%E8%AF%91%E5%99%A8/)》\n\n第三篇[《JVM性能优化， Part 3 —— 垃圾回收》](https://chiyuanbo.github.io/2018/04/28/JVM%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%20Part%203%20%20%E2%80%95%E2%80%95%20%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/)\n\n第四篇[《JVM性能优化， Part 4 —— C4 垃圾回收》](https://chiyuanbo.github.io/2018/04/28/JVM%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%20Part%204%20%E2%80%95%E2%80%95%20C4%20%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/)\n\n第五篇[《JVM性能优化， Part 5 —— Java的伸缩性》](https://chiyuanbo.github.io/2018/04/28/JVM%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%20Part%205%20%20%E2%80%95%E2%80%95%20Java%E7%9A%84%E4%BC%B8%E7%BC%A9%E6%80%A7/)","tags":["Java","JVM"]},{"title":"深入剖析volatile关键字","url":"/2022/09/07/cyb-mds/java/J.U.C/深入剖析volatile关键字/","content":"\n==作者：YB-Chi==\n\n* [一.内存模型的相关概念](#一内存模型的相关概念)\n* [二.并发编程中的三个概念](#二并发编程中的三个概念)\n\t* [1.原子性](#1原子性)\n\t* [2.可见性](#2可见性)\n\t* [3.有序性](#3有序性)\n* [三.Java内存模型](#三java内存模型)\n\t* [1.原子性](#1原子性)\n\t* [2.可见性](#2可见性)\n\t* [3.有序性](#3有序性)\n* [四.深入剖析volatile关键字](#四深入剖析volatile关键字)\n\t* [1.volatile关键字的两层语义](#1volatile关键字的两层语义)\n\t* [2.volatile保证原子性吗？](#2volatile保证原子性吗)\n\t* [3.volatile能保证有序性吗？](#3volatile能保证有序性吗)\n* [五.使用volatile关键字的场景](#五使用volatile关键字的场景)\n\t* [1.状态标记量](#1状态标记量)\n\t* [2.double check](#2double-check)\n\n\n\n&emsp;&emsp;volatile这个关键字可能很多朋友都听说过，或许也都用过。在Java 5之前，它是一个备受争议的关键字，因为在程序中使用它往往会导致出人意料的结果。在Java 5之后，volatile关键字才得以重获生机。\n\n&emsp;&emsp;volatile关键字虽然从字面上理解起来比较简单，但是要用好不是一件容易的事情。由于volatile关键字是与Java的内存模型有关的，因此在讲述volatile关键之前，我们先来了解一下与内存模型相关的概念和知识，然后分析了volatile关键字的实现原理，最后给出了几个使用volatile关键字的场景。\n\n### 一.内存模型的相关概念\n\n&emsp;&emsp;大家都知道，计算机在执行程序时，每条指令都是在CPU中执行的，而执行指令过程中，势必涉及到数据的读取和写入。由于程序运行过程中的临时数据是存放在主存（物理内存）当中的，这时就存在一个问题，由于CPU执行速度很快，而从内存读取数据和向内存写入数据的过程跟CPU执行指令的速度比起来要慢的多，因此如果任何时候对数据的操作都要通过和内存的交互来进行，会大大降低指令执行的速度。因此在CPU里面就有了高速缓存。\n\n&emsp;&emsp;也就是，当程序在运行过程中，会将运算需要的数据从主存复制一份到CPU的高速缓存当中，那么CPU进行计算时就可以直接从它的高速缓存读取数据和向其中写入数据，当运算结束之后，再将高速缓存中的数据刷新到主存当中。举个简单的例子，比如下面的这段代码：\n\n    i = i + 1;  \n&emsp;&emsp;当线程执行这个语句时，会先从主存当中读取i的值，然后复制一份到高速缓存当中，然后CPU执行指令对i进行加1操作，然后将数据写入高速缓存，最后将高速缓存中i最新的值刷新到主存当中。\n\n&emsp;&emsp;这个代码在单线程中运行是没有任何问题的，但是在多线程中运行就会有问题了。在多核CPU中，每条线程可能运行于不同的CPU中，因此每个线程运行时有自己的高速缓存（对单核CPU来说，其实也会出现这种问题，只不过是以线程调度的形式来分别执行的）。本文我们以多核CPU为例。\n\n&emsp;&emsp;比如同时有2个线程执行这段代码，假如初始时i的值为0，那么我们希望两个线程执行完之后i的值变为2。但是事实会是这样吗？\n\n&emsp;&emsp;可能存在下面一种情况：初始时，两个线程分别读取i的值存入各自所在的CPU的高速缓存当中，然后线程1进行加1操作，然后把i的最新值1写入到内存。此时线程2的高速缓存当中i的值还是0，进行加1操作之后，i的值为1，然后线程2把i的值写入内存。\n\n&emsp;&emsp;最终结果i的值是1，而不是2。这就是著名的缓存一致性问题。通常称这种被多个线程访问的变量为共享变量。\n　　\n&emsp;&emsp;也就是说，如果一个变量在多个CPU中都存在缓存（一般在多线程编程时才会出现），那么就可能存在缓存不一致的问题。\n\n&emsp;&emsp;为了解决缓存不一致性问题，通常来说有以下2种解决方法：\n\n&emsp;&emsp;1）通过在总线加LOCK#锁的方式\n\n&emsp;&emsp;2）通过缓存一致性协议\n\n&emsp;&emsp;这2种方式都是硬件层面上提供的方式。\n\n&emsp;&emsp;在早期的CPU当中，是通过在总线上加LOCK#锁的形式来解决缓存不一致的问题。因为CPU和其他部件进行通信都是通过总线来进行的，如果对总线加LOCK#锁的话，也就是说阻塞了其他CPU对其他部件访问（如内存），从而使得只能有一个CPU能使用这个变量的内存。比如上面例子中 如果一个线程在执行 i = i +1，如果在执行这段代码的过程中，在总线上发出了LCOK#锁的信号，那么只有等待这段代码完全执行完毕之后，其他CPU才能从变量i所在的内存读取变量，然后进行相应的操作。这样就解决了缓存不一致的问题。\n\n&emsp;&emsp;但是上面的方式会有一个问题，由于在锁住总线期间，其他CPU无法访问内存，导致效率低下。\n&emsp;&emsp;所以就出现了缓存一致性协议。最出名的就是Intel 的MESI协议，MESI协议保证了每个缓存中使用的共享变量的副本是一致的。它核心的思想是：当CPU写数据时，如果发现操作的变量是共享变量，即在其他CPU中也存在该变量的副本，会发出信号通知其他CPU将该变量的缓存行置为无效状态，因此当其他CPU需要读取这个变量时，发现自己缓存中缓存该变量的缓存行是无效的，那么它就会从内存重新读取。\n![](https://raw.githubusercontent.com/chiyuanbo/pic/master/kfka.jpg)\n\n### 二.并发编程中的三个概念\n      在并发编程中，我们通常会遇到以下三个问题：原子性问题，可见性问题，有序性问题。我们先看具体看一下这三个概念：\n#### 1.原子性\n&emsp;&emsp;原子性：即一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。\n\n&emsp;&emsp;一个很经典的例子就是银行账户转账问题：\n\n&emsp;&emsp;比如从账户A向账户B转1000元，那么必然包括2个操作：从账户A减去1000元，往账户B加上1000元。\n\n&emsp;&emsp;试想一下，如果这2个操作不具备原子性，会造成什么样的后果。假如从账户A减去1000元之后，操作突然中止。然后又从B取出了500元，取出500元之后，再执行 往账户B加上1000元 的操作。这样就会导致账户A虽然减去了1000元，但是账户B没有收到这个转过来的1000元。\n\n&emsp;&emsp;所以这2个操作必须要具备原子性才能保证不出现一些意外的问题。\n\n&emsp;&emsp;同样地反映到并发编程中会出现什么结果呢？\n\n&emsp;&emsp;举个最简单的例子，大家想一下假如为一个32位的变量赋值过程不具备原子性的话，会发生什么后果？\n\n    i = 9;  \n假若一个线程执行到这个语句时，我暂且假设为一个32位的变量赋值包括两个过程：为低16位赋值，为高16位赋值。\n\n那么就可能发生一种情况：当将低16位数值写入之后，突然被中断，而此时又有一个线程去读取i的值，那么读取到的就是错误的数据。           \n\n#### 2.可见性\n\n可见性是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。\n\n举个简单的例子，看下面这段代码：\n\n````java\n//线程1执行的代码  \nint i = 0;  \ni = 10;  \n\n//线程2执行的代码  \nj = i;  \n````\n\n&emsp;&emsp;假若执行线程1的是CPU1，执行线程2的是CPU2。由上面的分析可知，当线程1执行 i =10这句时，会先把i的初始值加载到CPU1的高速缓存中，然后赋值为10，那么在CPU1的高速缓存当中i的值变为10了，却没有立即写入到主存当中。\n\n&emsp;&emsp;此时线程2执行 j =i，它会先去主存读取i的值并加载到CPU2的缓存当中，注意此时内存当中i的值还是0，那么就会使得j的值为0，而不是10.\n\n&emsp;&emsp;这就是可见性问题，线程1对变量i修改了之后，线程2没有立即看到线程1修改的值。\n#### 3.有序性\n&emsp;&emsp;有序性：即程序执行的顺序按照代码的先后顺序执行。举个简单的例子，看下面这段代码：\n\n````java\nint i = 0;                \nboolean flag = false;  \ni = 1;                //语句1    \nflag = true;          //语句2  \n````\n\n&emsp;&emsp;上面代码定义了一个int型变量，定义了一个boolean类型变量，然后分别对两个变量进行赋值操作。从代码顺序上看，语句1是在语句2前面的，那么JVM在真正执行这段代码的时候会保证语句1一定会在语句2前面执行吗？不一定，为什么呢？这里可能会发生指令重排序（Instruction Reorder）。\n\n&emsp;&emsp;下面解释一下什么是指令重排序，一般来说，处理器为了提高程序运行效率，可能会对输入代码进行优化，它不保证程序中各个语句的执行先后顺序同代码中的顺序一致，但是它会保证程序最终执行结果和代码顺序执行的结果是一致的。\n\n&emsp;&emsp;比如上面的代码中，语句1和语句2谁先执行对最终的程序结果并没有影响，那么就有可能在执行过程中，语句2先执行而语句1后执行。\n\n&emsp;&emsp;但是要注意，虽然处理器会对指令进行重排序，但是它会保证程序最终结果会和代码顺序执行结果相同，那么它靠什么保证的呢？再看下面一个例子：\n\n````java\nint a = 10;    //语句1  \nint r = 2;    //语句2  \na = a + 3;    //语句3  \nr = a*a;     //语句4  \n````\n\n这段代码有4个语句，那么可能的一个执行顺序是：\n\n![](https://raw.githubusercontent.com/chiyuanbo/pic/master/shunxu.jpg)\n那么可不可能是这个执行顺序呢： 语句2   语句1    语句4   语句3\n\n&emsp;&emsp;不可能，因为处理器在进行重排序时是会考虑指令之间的数据依赖性，如果一个指令Instruction 2必须用到Instruction 1的结果，那么处理器会保证Instruction 1会在Instruction 2之前执行。\n\n&emsp;&emsp;虽然重排序不会影响单个线程内程序执行的结果，但是多线程呢？下面看一个例子：\n\n````java\n//线程1:  \ncontext = loadContext();   //语句1  \ninited = true;             //语句2  \n\n//线程2:  \nwhile(!inited ){  \n  sleep()  \n}  \ndoSomethingwithconfig(context);  \n````\n\n&emsp;&emsp;上面代码中，由于语句1和语句2没有数据依赖性，因此可能会被重排序。假如发生了重排序，在线程1执行过程中先执行语句2，而此时线程2会以为初始化工作已经完成，那么就会跳出while循环，去执行doSomethingwithconfig(context)方法，而此时context并没有被初始化，就会导致程序出错。\n\n&emsp;&emsp;从上面可以看出，指令重排序不会影响单个线程的执行，但是会影响到线程并发执行的正确性。\n\n&emsp;&emsp;也就是说，要想并发程序正确地执行，必须要保证原子性、可见性以及有序性。只要有一个没有被保证，就有可能会导致程序运行不正确。\n\n### 三.Java内存模型\n\n&emsp;&emsp;在前面谈到了一些关于内存模型以及并发编程中可能会出现的一些问题。下面我们来看一下Java内存模型，研究一下Java内存模型为我们提供了哪些保证以及在java中提供了哪些方法和机制来让我们在进行多线程编程时能够保证程序执行的正确性。\n\n&emsp;&emsp;在Java虚拟机规范中试图定义一种Java内存模型（Java Memory Model，JMM）来屏蔽各个硬件平台和操作系统的内存访问差异，以实现让Java程序在各种平台下都能达到一致的内存访问效果。那么Java内存模型规定了哪些东西呢，它定义了程序中变量的访问规则，往大一点说是定义了程序执行的次序。注意，为了获得较好的执行性能，Java内存模型并没有限制执行引擎使用处理器的寄存器或者高速缓存来提升指令执行速度，也没有限制编译器对指令进行重排序。也就是说，在java内存模型中，也会存在缓存一致性问题和指令重排序的问题。\n\n&emsp;&emsp;Java内存模型规定所有的变量都是存在主存当中（类似于前面说的物理内存），每个线程都有自己的工作内存（类似于前面的高速缓存）。线程对变量的所有操作都必须在工作内存中进行，而不能直接对主存进行操作。并且每个线程不能访问其他线程的工作内存。\n\n&emsp;&emsp;举个简单的例子：在java中，执行下面这个语句：\n\n    i  = 10;  \n\n&emsp;&emsp;执行线程必须先在自己的工作线程中对变量i所在的缓存行进行赋值操作，然后再写入主存当中。而不是直接将数值10写入主存当中。\n\n&emsp;&emsp;那么Java语言 本身对 原子性、可见性以及有序性提供了哪些保证呢？\n#### 1.原子性\n\n&emsp;&emsp;在Java中，对基本数据类型的变量的读取和赋值操作是原子性操作，即这些操作是不可被中断的，要么执行，要么不执行。\n\n&emsp;&emsp;上面一句话虽然看起来简单，但是理解起来并不是那么容易。看下面一个例子i：\n　　请分析以下哪些操作是原子性操作：\n\n````java\n    x = 10;         //语句1  \n    y = x;         //语句2  \n    x++;           //语句3  \n    x = x + 1;     //语句4  \n````\n\n&emsp;&emsp;咋一看，有些朋友可能会说上面的4个语句中的操作都是原子性操作。其实只有语句1是原子性操作，其他三个语句都不是原子性操作。\n\n&emsp;&emsp;语句1是直接将数值10赋值给x，也就是说线程执行这个语句的会直接将数值10写入到工作内存中。\n\n&emsp;&emsp;语句2实际上包含2个操作，它先要去读取x的值，再将x的值写入工作内存，虽然读取x的值以及 将x的值写入工作内存 这2个操作都是原子性操作，但是合起来就不是原子性操作了。\n\n&emsp;&emsp;同样的，x++和 x = x+1包括3个操作：读取x的值，进行加1操作，写入新的值。\n\n&emsp;&emsp;所以上面4个语句只有语句1的操作具备原子性。\n\n&emsp;&emsp;也就是说，只有简单的读取、赋值（而且必须是将数字赋值给某个变量，变量之间的相互赋值不是原子操作）才是原子操作。\n\n&emsp;&emsp;不过这里有一点需要注意：在32位平台下，对64位数据的读取和赋值是需要通过两个操作来完成的，不能保证其原子性。但是好像在最新的JDK中，JVM已经保证对64位数据的读取和赋值也是原子性操作了。\n\n&emsp;&emsp;从上面可以看出，Java内存模型只保证了基本读取和赋值是原子性操作，如果要实现更大范围操作的原子性，可以通过synchronized和Lock来实现。由于synchronized和Lock能够保证任一时刻只有一个线程执行该代码块，那么自然就不存在原子性问题了，从而保证了原子性。\n#### 2.可见性\n\n&emsp;&emsp;对于可见性，Java提供了volatile关键字来保证可见性。\n\n&emsp;&emsp;当一个共享变量被volatile修饰时，它会保证修改的值会立即被更新到主存，当有其他线程需要读取时，它会去内存中读取新值。\n\n&emsp;&emsp;而普通的共享变量不能保证可见性，因为普通共享变量被修改之后，什么时候被写入主存是不确定的，当其他线程去读取时，此时内存中可能还是原来的旧值，因此无法保证可见性。\n\n&emsp;&emsp;另外，通过synchronized和Lock也能够保证可见性，synchronized和Lock能保证同一时刻只有一个线程获取锁然后执行同步代码，并且在释放锁之前会将对变量的修改刷新到主存当中。因此可以保证可见性。\n#### 3.有序性\n\n&emsp;&emsp;在Java内存模型中，允许编译器和处理器对指令进行重排序，但是重排序过程不会影响到单线程程序的执行，却会影响到多线程并发执行的正确性。\n\n&emsp;&emsp;在Java里面，可以通过volatile关键字来保证一定的“有序性”（具体原理在下一节讲述）。另外可以通过synchronized和Lock来保证有序性，很显然，synchronized和Lock保证每个时刻是有一个线程执行同步代码，相当于是让线程顺序执行同步代码，自然就保证了有序性。\n\n&emsp;&emsp;另外，Java内存模型具备一些先天的“有序性”，即不需要通过任何手段就能够得到保证的有序性，这个通常也称为 happens-before 原则。如果两个操作的执行次序无法从happens-before原则推导出来，那么它们就不能保证它们的有序性，虚拟机可以随意地对它们进行重排序。\n\n&emsp;&emsp;下面就来具体介绍下happens-before原则（先行发生原则）：\n\n - 程序次序规则：一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作\n  - 锁定规则：一个unLock操作先行发生于后面对同一个锁额lock操作\n  - volatile变量规则：对一个变量的写操作先行发生于后面对这个变量的读操作\n  - 传递规则：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C\n  - 线程启动规则：Thread对象的start()方法先行发生于此线程的每个一个动作\n  - 线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生\n  - 线程终结规则：线程中所有的操作都先行发生于线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行\n  - 对象终结规则：一个对象的初始化完成先行发生于他的finalize()方法的开始\n\n&emsp;&emsp;这8条原则摘自《深入理解Java虚拟机》。\n\n&emsp;&emsp;这8条规则中，前4条规则是比较重要的，后4条规则都是显而易见的。\n\n&emsp;&emsp;下面我们来解释一下前4条规则：\n\n&emsp;&emsp;对于程序次序规则来说，我的理解就是一段程序代码的执行在单个线程中看起来是有序的。注意，虽然这条规则中提到“书写在前面的操作先行发生于书写在后面的操作”，这个应该是程序看起来执行的顺序是按照代码顺序执行的，因为虚拟机可能会对程序代码进行指令重排序。虽然进行重排序，但是最终执行的结果是与程序顺序执行的结果一致的，它只会对不存在数据依赖性的指令进行重排序。因此，在单个线程中，程序执行看起来是有序执行的，这一点要注意理解。事实上，这个规则是用来保证程序在单线程中执行结果的正确性，但无法保证程序在多线程中执行的正确性。\n\n&emsp;&emsp;第二条规则也比较容易理解，也就是说无论在单线程中还是多线程中，同一个锁如果出于被锁定的状态，那么必须先对锁进行了释放操作，后面才能继续进行lock操作。\n\n&emsp;&emsp;第三条规则是一条比较重要的规则，也是后文将要重点讲述的内容。直观地解释就是，如果一个线程先去写一个变量，然后一个线程去进行读取，那么写入操作肯定会先行发生于读操作。\n\n&emsp;&emsp;第四条规则实际上就是体现happens-before原则具备传递性。\n\n### 四.深入剖析volatile关键字\n&emsp;&emsp;在前面讲述了很多东西，其实都是为讲述volatile关键字作铺垫，那么接下来我们就进入主题。\n\n#### 1.volatile关键字的两层语义\n\n&emsp;&emsp;一旦一个共享变量（类的成员变量、类的静态成员变量）被volatile修饰之后，那么就具备了两层语义：\n\n    1）保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的。\n    2）禁止进行指令重排序。\n\n&emsp;&emsp;先看一段代码，假如线程1先执行，线程2后执行：\n\n````java\n//线程1  \nboolean stop = false;  \nwhile(!stop){  \n\tdoSomething();  \n}  \n\n//线程2  \nstop = true;  \n````\n\n&emsp;&emsp;这段代码是很典型的一段代码，很多人在中断线程时可能都会采用这种标记办法。但是事实上，这段代码会完全运行正确么？即一定会将线程中断么？不一定，也许在大多数时候，这个代码能够把线程中断，但是也有可能会导致无法中断线程（虽然这个可能性很小，但是只要一旦发生这种情况就会造成死循环了）。\n\n&emsp;&emsp;下面解释一下这段代码为何有可能导致无法中断线程。在前面已经解释过，每个线程在运行过程中都有自己的工作内存，那么线程1在运行的时候，会将stop变量的值拷贝一份放在自己的工作内存当中。\n\n&emsp;&emsp;那么当线程2更改了stop变量的值之后，但是还没来得及写入主存当中，线程2转去做其他事情了，那么线程1由于不知道线程2对stop变量的更改，因此还会一直循环下去。\n\n&emsp;&emsp;但是用volatile修饰之后就变得不一样了：\n\n&emsp;&emsp;第一：使用volatile关键字会强制将修改的值立即写入主存；\n\n&emsp;&emsp;第二：使用volatile关键字的话，当线程2进行修改时，会导致线程1的工作内存中缓存变量stop的缓存行无效（反映到硬件层的话，就是CPU的L1或者L2缓存中对应的缓存行无效）；\n\n&emsp;&emsp;第三：由于线程1的工作内存中缓存变量stop的缓存行无效，所以线程1再次读取变量stop的值时会去主存读取。\n\n&emsp;&emsp;那么在线程2修改stop值时（当然这里包括2个操作，修改线程2工作内存中的值，然后将修改后的值写入内存），会使得线程1的工作内存中缓存变量stop的缓存行无效，然后线程1读取时，发现自己的缓存行无效，它会等待缓存行对应的主存地址被更新之后，然后去对应的主存读取最新的值。\n\n&emsp;&emsp;那么线程1读取到的就是最新的正确的值。\n#### 2.volatile保证原子性吗？\n&emsp;&emsp;从上面知道volatile关键字保证了操作的可见性，但是volatile能保证对变量的操作是原子性吗？\n　　\n&emsp;&emsp;下面看一个例子：\n````java\npublic class Test {  \n\tpublic volatile int inc = 0;  \n\n\tpublic void increase() {  \n\t\tinc++;  \n\t}  \n\n\tpublic static void main(String[] args) {  \n\t\tfinal Test test = new Test();  \n\t\tfor(int i=0;i<10;i++){  \n\t\t\tnew Thread(){  \n\t\t\t\tpublic void run() {  \n\t\t\t\t\tfor(int j=0;j<1000;j++)  \n\t\t\t\t\t\ttest.increase();  \n\t\t\t\t};  \n\t\t\t}.start();  \n\t\t}  \n\n\t\twhile(Thread.activeCount()>1)  //保证前面的线程都执行完  \n\t\t\tThread.yield();  \n\t\tSystem.out.println(test.inc);  \n\t}  \n}  \n````\n\n&emsp;&emsp;大家想一下这段程序的输出结果是多少？也许有些朋友认为是10000。但是事实上运行它会发现每次运行结果都不一致，都是一个小于10000的数字。\n\n&emsp;&emsp;可能有的朋友就会有疑问，不对啊，上面是对变量inc进行自增操作，由于volatile保证了可见性，那么在每个线程中对inc自增完之后，在其他线程中都能看到修改后的值啊，所以有10个线程分别进行了1000次操作，那么最终inc的值应该是1000*10=10000。\n\n&emsp;&emsp;这里面就有一个误区了，volatile关键字能保证可见性没有错，但是上面的程序错在没能保证原子性。可见性只能保证每次读取的是最新的值，但是volatile没办法保证对变量的操作的原子性。\n\n&emsp;&emsp;在前面已经提到过，自增操作是不具备原子性的，它包括读取变量的原始值、进行加1操作、写入工作内存。那么就是说自增操作的三个子操作可能会分割开执行，就有可能导致下面这种情况出现：\n\n&emsp;&emsp;假如某个时刻变量inc的值为10，\n\n&emsp;&emsp;线程1对变量进行自增操作，线程1先读取了变量inc的原始值，然后线程1被阻塞了；\n\n&emsp;&emsp;然后线程2对变量进行自增操作，线程2也去读取变量inc的原始值，由于线程1只是对变量inc进行读取操作，而没有对变量进行修改操作，所以不会导致线程2的工作内存中缓存变量inc的缓存行无效，所以线程2会直接去主存读取inc的值，发现inc的值时10，然后进行加1操作，并把11写入工作内存，最后写入主存。\n\n&emsp;&emsp;然后线程1接着进行加1操作，由于已经读取了inc的值，注意此时在线程1的工作内存中inc的值仍然为10，所以线程1对inc进行加1操作后inc的值为11，然后将11写入工作内存，最后写入主存。\n\n&emsp;&emsp;那么两个线程分别进行了一次自增操作后，inc只增加了1。\n\n&emsp;&emsp;解释到这里，可能有朋友会有疑问，不对啊，前面不是保证一个变量在修改volatile变量时，会让缓存行无效吗？然后其他线程去读就会读到新的值，对，这个没错。这个就是上面的happens-before规则中的volatile变量规则，但是要注意，线程1对变量进行读取操作之后，被阻塞了的话，并没有对inc值进行修改。然后虽然volatile能保证线程2对变量inc的值读取是从内存中读取的，但是线程1没有进行修改，所以线程2根本就不会看到修改的值。\n\n&emsp;&emsp;根源就在这里，自增操作不是原子性操作，而且volatile也无法保证对变量的任何操作都是原子性的。\n\n&emsp;&emsp;把上面的代码改成以下任何一种都可以达到效果：\n\n&emsp;&emsp;采用synchronized：\n\n````java\npublic class Test {  \n\tpublic  int inc = 0;  \n\n\tpublic synchronized void increase() {  \n\t\tinc++;  \n\t}  \n\n\tpublic static void main(String[] args) {  \n\t\tfinal Test test = new Test();  \n\t\tfor(int i=0;i<10;i++){  \n\t\t\tnew Thread(){  \n\t\t\t\tpublic void run() {  \n\t\t\t\t\tfor(int j=0;j<1000;j++)  \n\t\t\t\t\t\ttest.increase();  \n\t\t\t\t};  \n\t\t\t}.start();  \n\t\t}  \n\n\t\twhile(Thread.activeCount()>1)  //保证前面的线程都执行完  \n\t\t\tThread.yield();  \n\t\tSystem.out.println(test.inc);  \n\t}  \n} \n````\n&emsp;&emsp;采用Lock：\n\n````java\npublic class Test {  \n\tpublic  int inc = 0;  \n\tLock lock = new ReentrantLock();  \n\n\tpublic  void increase() {  \n\t\tlock.lock();  \n\t\ttry {  \n\t\t\tinc++;  \n\t\t} finally{  \n\t\t\tlock.unlock();  \n\t\t}  \n\t}  \n\n\tpublic static void main(String[] args) {  \n\t\tfinal Test test = new Test();  \n\t\tfor(int i=0;i<10;i++){  \n\t\t\tnew Thread(){  \n\t\t\t\tpublic void run() {  \n\t\t\t\t\tfor(int j=0;j<1000;j++)  \n\t\t\t\t\t\ttest.increase();  \n\t\t\t\t};  \n\t\t\t}.start();  \n\t\t}  \n\n\t\twhile(Thread.activeCount()>1)  //保证前面的线程都执行完  \n\t\t\tThread.yield();  \n\t\tSystem.out.println(test.inc);  \n\t}  \n}  \n````\n&emsp;&emsp;采用AtomicInteger：\n\n````java\npublic class Test {  \n\tpublic  AtomicInteger inc = new AtomicInteger();  \n\n\tpublic  void increase() {  \n\t\tinc.getAndIncrement();  \n\t}  \n\n\tpublic static void main(String[] args) {  \n\t\tfinal Test test = new Test();  \n\t\tfor(int i=0;i<10;i++){  \n\t\t\tnew Thread(){  \n\t\t\t\tpublic void run() {  \n\t\t\t\t\tfor(int j=0;j<1000;j++)  \n\t\t\t\t\t\ttest.increase();  \n\t\t\t\t};  \n\t\t\t}.start();  \n\t\t}  \n\n\t\twhile(Thread.activeCount()>1)  //保证前面的线程都执行完  \n\t\t\tThread.yield();  \n\t\tSystem.out.println(test.inc);  \n\t}  \n}  \n````\n\n&emsp;&emsp;在java 1.5的java.util.concurrent.atomic包下提供了一些原子操作类，即对基本数据类型的 自增（加1操作），自减（减1操作）、以及加法操作（加一个数），减法操作（减一个数）进行了封装，保证这些操作是原子性操作。atomic是利用CAS来实现原子性操作的（Compare And Swap），CAS实际上是利用处理器提供的CMPXCHG指令实现的，而处理器执行CMPXCHG指令是一个原子性操作。\n#### 3.volatile能保证有序性吗？\n\n&emsp;&emsp;在前面提到volatile关键字能禁止指令重排序，所以volatile能在一定程度上保证有序性。\n\n&emsp;&emsp;volatile关键字禁止指令重排序有两层意思：\n\n    1）当程序执行到volatile变量的读操作或者写操作时，在其前面的操作的更改肯定全部已经进行，且结果已经对后面的操作可见；在其后面的操作肯定还没有进行；\n    2）在进行指令优化时，不能将在对volatile变量访问的语句放在其后面执行，也不能把volatile变量后面的语句放到其前面执行。\n\n&emsp;&emsp;可能上面说的比较绕，举个简单的例子：\n\n````java\n//x、y为非volatile变量  \n//flag为volatile变量  \n\nx = 2;        //语句1  \ny = 0;        //语句2  \nflag = true;  //语句3  \nx = 4;         //语句4  \ny = -1;       //语句5  \n````\n\n&emsp;&emsp;由于flag变量为volatile变量，那么在进行指令重排序的过程的时候，不会将语句3放到语句1、语句2前面，也不会讲语句3放到语句4、语句5后面。但是要注意语句1和语句2的顺序、语句4和语句5的顺序是不作任何保证的。\n\n&emsp;&emsp;并且volatile关键字能保证，执行到语句3时，语句1和语句2必定是执行完毕了且语句1和语句2的执行结果对语句3、语句4、语句5是可见的。\n\n&emsp;&emsp;那么我们回到前面举的一个例子：\n\n````java\n//线程1:  \ncontext = loadContext();   //语句1  \ninited = true;             //语句2  \n\n//线程2:  \nwhile(!inited ){  \n  sleep()  \n}  \ndoSomethingwithconfig(context);  \n````\n\n&emsp;&emsp;前面举这个例子的时候，提到有可能语句2会在语句1之前执行，那么就可能导致context还没被初始化，而线程2中就使用未初始化的context去进行操作，导致程序出错。\n\n&emsp;&emsp;这里如果用volatile关键字对inited变量进行修饰，就不会出现这种问题了，因为当执行到语句2时，必定能保证context已经初始化完毕。\n4.volatile的原理和实现机制\n\n&emsp;&emsp;前面讲述了源于volatile关键字的一些使用，下面我们来探讨一下volatile到底如何保证可见性和禁止指令重排序的。\n\n&emsp;&emsp;下面这段话摘自《深入理解Java虚拟机》：\n\n&emsp;&emsp;“观察加入volatile关键字和没有加入volatile关键字时所生成的汇编代码发现，加入volatile关键字时，会多出一个lock前缀指令”\n\n&emsp;&emsp;lock前缀指令实际上相当于一个内存屏障（也成内存栅栏），内存屏障会提供3个功能：\n\n    1）它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成；\n    2）它会强制将对缓存的修改操作立即写入主存；\n    3）如果是写操作，它会导致其他CPU中对应的缓存行无效。\n### 五.使用volatile关键字的场景\n&emsp;&emsp;synchronized关键字是防止多个线程同时执行一段代码，那么就会很影响程序执行效率，而volatile关键字在某些情况下性能要优于synchronized，但是要注意volatile关键字是无法替代synchronized关键字的，因为volatile关键字无法保证操作的原子性。通常来说，使用volatile必须具备以下2个条件：\n\n    1）对变量的写操作不依赖于当前值\n    2）该变量没有包含在具有其他变量的不变式中\n&emsp;&emsp;实际上，这些条件表明，可以被写入 volatile 变量的这些有效值独立于任何程序的状态，包括变量的当前状态。\n\n&emsp;&emsp;事实上，我的理解就是上面的2个条件需要保证操作是原子性操作，才能保证使用volatile关键字的程序在并发时能够正确执行。\n\n&emsp;&emsp;下面列举几个Java中使用volatile的几个场景。\n\n#### 1.状态标记量\n\n````java\nvolatile boolean flag = false;  \n\nwhile(!flag){  \n\tdoSomething();  \n}  \n\npublic void setFlag() {  \n\tflag = true;  \n}  \n\nvolatile boolean inited = false;  \n//线程1:  \ncontext = loadContext();    \ninited = true;              \n\n//线程2:  \nwhile(!inited ){  \n\tsleep()  \n}  \ndoSomethingwithconfig(context);  \n````\n\n#### 2.double check\n\n````java\nclass Singleton{  \n\tprivate volatile static Singleton instance = null;  \n\n\tprivate Singleton() {  \n\n\t}  \n\n\tpublic static Singleton getInstance() {  \n\t\tif(instance==null) {  \n\t\t\tsynchronized (Singleton.class) {  \n\t\t\t\tif(instance==null)  \n\t\t\t\t\tinstance = new Singleton();  \n\t\t\t}  \n\t\t}  \n\t\treturn instance;  \n\t}  \n}  \n````\n\n&emsp;&emsp;于为何;需要这么写请参考：\n\n&emsp;&emsp;《Java 中的双重检查（Double-Check）》http://blog.csdn.net/dl88250/article/details/5439024和http://www.iteye.com/topic/652440\n\n&emsp;&emsp;参考资料：\n　　《深入理解Java虚拟机》","tags":["Java","J.U.C"]},{"title":"jdk1.7和1.8共存及切换","url":"/2022/09/07/cyb-mds/java/jdk/jdk1.7和1.8共存及切换/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n### 所需软件\njdk 1.7和1.8 64位\n\n### 安装步奏\n1.7版本是普通的安装，但是1.8会在c盘创建几个文件，==以及修改注册表==。\n\n### 修改步奏\n\n首先删掉1.8自动生成的环境变量，即：\nC:\\ProgramData\\Oracle\\Java\\javapath;\n并将此目录下的三个快捷方式删掉\n然后删除1.8生成的几个文件 即 C:\\Windows\\System32下的java.exe，javaw.exe,javaws.exe\n然后进入dos运行java -version\n报错了   has value'1.8'，but '1.7'……\n\n运行regedit打开注册表**HKEY_LOCAL_MACHINE\\SOFTWARE\\JavaSoft\\Java Runtime Environment**   \n点击它\n然后点击右侧的CurrentVersion  将值1.8修改为1.7 然后确定  \n重新java -verison   成功","tags":["JDK"]},{"title":"JVM性能优化， Part 1 ―― JVM简介","url":"/2022/09/07/cyb-mds/java/jvm/JVM性能优化， Part 1 ―― JVM简介/","content":"\n==作者：Eva Andreasson,译者：曹旭东==\n\n[toc]\n\n**Java的性能与“一次编写，到处运行”的挑战**\n\n有不少人认为，Java平台本身就挺慢。其主要观点简单来说就是，Java性能低已经有些年头了 ―― 最早可以追溯到Java第一次用于企业级应用程序开发的时候。但这早就是老黄历了。事实是，如果你对不同的开发平台上运行简单的、静态的、确定性任务的运行结果做比较，你就会发现使用经过机器级优化（machine-optimized）代码的平台比任何使用虚拟环境进行运算的都要强，JVM也不例外。但是，在过去的10年中，Java的性能有了大幅提升。市场上不断增长的需求催生了垃圾回收算法的出现和编译技术的革新，在不断探索与优化的过程中，JVM茁壮成长。在这个系列文章中，我将介绍其中的一些内容。\n\nJVM技术中最迷人的地方也正是其最具挑战性的地方：“一次编写，到处运行”。JVM并不对具体的用例、应用程序或用户负载进行优化，而是在应用程序运行过程中不断收集运行时信息，并以此为根据动态的进行优化。这种动态的运行时特性带来了很多动态问题。在设计优化方案时，以JVM为工作平台的程序无法依靠静态编译和可预测的内存分配速率（predictable allocation rates）对应用程序做性能评估，至少在对生产环境进行性能评估时是不行的。\n\n机器级优化过的代码有时可以达到更好的性能，但它是以牺牲可移植性为代价的，在企业级应用程序中，动态负载和快速迭代更新是更加重要的。大多数企业会愿意牺牲一点机器级优化代码带来的性能，以此换取Java平台的诸多优势：\n\n*   编码简单，易于实现（意味着可以更快的推向市场）\n*   有很多非常有才的程序员\n*   使用Java API和标准库实现快速开发\n*   可移植性 ―― 无需为每个平台都编写一套代码\n\n**从源代码到字节码**\n\n作为一名Java程序员，你可以已经对编码、编译和运行这一套流程比较熟悉了。假如说，现在你写了一个程序代码MyApp.java，准备编译运行。为了运行这个程序，首先，你需要使用JDK内建的Java语言编译器，javac，对这个文件进行编译，它可以将Java源代码编译为字节码。javac将根据Java程序的源代码生成对应的可执行字节码，并将其保存为同名类文件：MyApp.class。在经过编译阶段后，你就可以在命令行中使用java命令或其他启动脚本载入可执行的类文件来运行程序，并且可以为程序添加启动参数。之后，类会被载入到运行时（这里指的是正在运行的JVM），程序开始运行。\n\n上面所描述的就是在运行Java应用程序时的表面过程，但现在，我们要深入挖掘一下，在调用Java命令时，到底发生了什么？JVM到底是什么？大多数程序员是通过不断的调优，即使用相应的启动参数，与JVM进行交互，使Java程序运行的更快，同时避免程序出现“out of memory”错误。但你是否想过，为什么我们必须要通过JVM来运行Java应用程序呢？\n\n#### **什么是JVM**\n\n简单来说，JVM是用于执行Java应用程序和字节码的软件模块，并且可以将字节码转换为特定硬件和特定操作系统的本地代码。正因如此，JVM使Java程序做到了“一次编写，到处运行”。Java语言的可移植性是得到企业级应用程序开发者青睐的关键：开发者无需因平台不同而把程序重新编写一遍，因为有JVM负责处理字节码到本地代码的转换和平台相关优化的工作。\n\n>基本上来说，JVM是一个虚拟运行环境，对于字节码来说就像是一个机器一样，可以执行任务，并通过底层实现执行内存相关的操作。\n\nJVM也可以在运行java应用程序时，很好的管理动态资源。这指的是他可以正确的分配、回收内存，在不同的上维护一个具有一致性的线程模型，并且可以为当前的CPU架构组织可执行指令。JVM解放了程序员，使程序员不必再关系对象的生命周期，使程序员不必再关心应该在何时释放内存。而这，正是使用着类似C语言的非动态语言的程序员心中永远的痛。\n\n你可以将JVM当做是一种专为Java而生的特殊的操作系统，它的工作是管理运行Java应用程序的运行时环境。简单来说，JVM就是运行字节码指令的虚拟执行环境，并且可以分配执行任务，或通过底层实现对内存进行操作。\n\n#### **JVM组件简介**\n\n关于JVM内部原理与性能优化有很多内容可写。作为这个系列的开篇文章，我简单介绍JVM的内部组件。这个简要介绍对于那些JVM新手比较有帮助，也是为后面的深入讨论做个铺垫。\n\n#### **从一种语言到另一种 ―― 关于Java编译器**\n\n`编译器`以一种语言为输入，生成另一种可执行语言作为输出。Java编译器主要完成2个任务：\n\n1.  实现Java语言的可移植性，不必局限于某一特定平台；\n2.  确保输出代码可以在目标平台能够有效率的运行。\n\n编译器可以是静态的，也可以是动态的。静态编译器，如javac，它以Java源代码为输入，将其编译为字节码（一种可以运行JVM中的语言）。*静态编译器*解释输入的源代码，而生成可执行输出代码则会在程序真正运行时用到。因为输入是静态的，所有输出结果总是相同的。只有当你修改的源代码并重新编译时，才有可能看到不同的编译结果。\n\n_动态编译器_，如使用[Just-In-Time(JIT，即时编译)]技术的编译器，会动态的将一种编程语言编译为另一种语言，这个过程是在程序运行中同时进行的。JIT编译器会收集程序的运行时数据（在程序中插入性能计数器），再根据运行时数据和当前运行环境数据动态规划编译方案。动态编译可以生成更好的序列指令，使用更有效率的指令集合替换原指令集合，或剔除冗余操作。收集到的运行时数据的越多，动态编译的效果就越好；这通常称为代码优化或重编译。\n\n动态编译使你的程序可以应对在不同负载和行为下对新优化的需求。这也是为什么动态编译器非常适合Java运行时。这里需要注意的地方是，动态编译器需要动用额外的数据结构、线程资源和CPU指令周期，才能收集运行时信息和优化的工作。若想完成更高级点的优化工作，就需要更多的资源。但是在大多数运行环境中，相对于获得的性能提升来说，动态编译的带来的性能损耗其实是非常小的 ―― 动态编译后的代码的运行效率可以比纯解释执行（即按照字节码运行，不做任何修改）快5到10倍。\n\n#### **内存分配与垃圾回收**\n\n`内存分配`是以线程为单位，在“Java进程专有内存地址空间”中，也就是Java堆中分配的。在普通的客户端Java应用程序中，内存分配都是单线程进行的。但是，在企业级应用程序和服务器端应用程序中，单线程内存分配却并不是个好办法，因为它无法充分利用现代多核时代的并行特性。\n\n并行应用程序设计要求JVM确保多线程内存分配不会在同一时间将同一块地址空间分配给多个线程。你可以在整个内存空间中加锁来解决这个问题，但是这个方法（即所谓的“堆锁”）开销较大，因为它迫使所有线程在分配内存时逐个执行，对资源利用和应用程序性能有较大影响。多核程序的一个额外特点是需要有新的资源分配方案，避免出现单线程、序列化资源分配的性能瓶颈。\n\n常用的解决方案是将堆划分为几个区域，每个区域都有适当的大小，当然具体的大小需要根据实际情况做相应的调整，因为不同应用程序之间，内存分配速率、对象大小和线程数量的差别是非常大的。Thread Local Allocation Buffer（TLAB），有时也称为Thraed Local Area（TLA），是线程自己使用的专用内存分配区域，在使用的时候无需获取堆锁。当这个区域用满的时候，线程会申请新的区域，直到堆中所有预留的区域都用光了。当堆中没有足够的空间来分配内存时，堆就“满”了，即堆上剩余的空间装不下待分配空间的对象。当堆满了的时候，垃圾回收就开始了。\n\n#### **碎片化**\n\n使用TLAB的一个风险是，由于堆上内存碎片的增加，使用内存的效率会下降。如果应用程序创建的对象的大小无法填满TLAB，而这块TLAB中剩下的空间又太小，无法分配给新的对象，那么这块空间就被浪费了，这就是所谓的“碎片”。如果“碎片”周围已分配出去的内存长时间无法回收，那么这块碎片研究长时间无法得到利用。\n\n`碎片化`是指堆上存在了大量的`碎片`，由于这些小碎片的存在而使堆无法得到有效利用，浪费了堆空间。为应用程序设置TLAB的大小时，若是没有对应用程序中对象大小和生命周期和合理评估，导致TLAB的大小设置不当，就会是使堆逐渐碎片化。随着应用程序的运行，被浪费的碎片空间会逐渐增多，导致应用程序性能下降。这是因为系统无法为新线程和新对象分配空间，于是为防止出现OOM（out-of-memory）错误，而频繁GC的缘故。\n\n对于TLAB产生的空间浪费这个问题，可以采用“曲线救国”的策略来解决。例如，可以根据应用程序的具体环境调整TLAB的大小。这个方法既可以临时，也可以彻底的避免堆空间的碎片化，但需要随着应用程序内存分配行为的变化而修改TLAB的值。此外，还可以使用一些复杂的JVM算法和其他的方法来组织堆空间来获得更有效率的内存分配行为。例如，JVM可以实现空闲列表（free-list），空闲列表中保存了堆中指定大小的空闲块。具有类似大小空闲块保存在一个空闲列表中，因此可以创建多个空闲列表，每个空闲列表保存某个范围内的空闲块。在某些事例中，使用空闲列表会比使用按实际大小分配内存的策略更有效率。线程为某个对象分配内存时，可以在空闲列表中寻找与对象大小最接近的空间块使用，相对于使用固定大小的TLAB，这种方法更有利于避免碎片化的出现。\n\n#### **GC往事**\n\n> \n> 早期的垃圾回收器有多个老年代，但实际上，存在多个老年代是弊大于利的。\n> \n\n另一种对抗碎片化的方法是创建一个所谓的年轻代，在这个专有的堆空间中，保存了所有新创建的对象。堆空间中剩余的空间就是所谓的老年代。老年代用于保存具有较长生命周期的对象，即当对象能够挺过几轮GC而不被回收，或者对象本身很大（一般来说，大对象都具有较长的寿命周期）时，它们就会被保存到老年代。为了让你能够更好的理解这个方法，我们有必要谈谈垃圾回收。\n\n#### **垃圾回收与应用程序性能**\n\n垃圾回收就是JVM释放那些没有引用指向的堆内存的操作。当垃圾回收首次触发时，有引用指向的对象会被保存下来，那些没有引用指向的对象占用的空间会被回收。当所有可回收的内存都被回收后，这些空间就可以被分配给新的对象了。\n\n垃圾回收不会回收仍有引用指向的对象；否则就会违反JVM规范。这个规则有一个例外，就是对软引用或弱引用的使用，当垃圾回收器发现内存快要用完时，会回收只有软引用或[弱引用](http://java.sun.com/docs/books/performance/1st_edition/html/JPAppGC.fm.html \"weak reference\")指向的对象所占用的内存。我的建议是，尽量避免使用弱引用，因为Java规范中存在的模糊的表述可能会使你对弱引用的使用产生误解。此外，Java本身是动态内存管理的，你没必要考虑什么时候该释放哪块内存。\n\n对于垃圾回收来说，挑战在于，如何将垃圾回收对应用程序造成的影响降到最小。如果垃圾回收执行的不充分，那么应用程序迟早会发生OOM错误；如果垃圾回收执行的太频繁，会对应用程序的吞吐量和响应时间造成影响，当然，这都不是好的影响。\n\n#### **GC算法**\n\n目前已经出现了很多垃圾回收算法。在这个系列文章中将对其中的一些进行介绍。概括来说，垃圾回收主要有两种方式，引用计数（reference counting）和引用追踪（reference tracing）。\n\n*   引用计数垃圾回收器会记录指向某个对象的引用的数目。当指向某个对象引用数位0时，该对象占用的内存就可以被回收了，这是引用计数垃圾回收的一个主要优点。使用引用计数垃圾回收的需要克服的难点在于如何解决循环引用带来的问题，以及如何保证引用计数的实效性。\n*   引用追踪垃圾回收器会标记所有仍有引用指向的对象，并从已标记的对象出发，继续标记这些对象指向的对象。当所有仍有引用指向的对象都被标记为“live”后，所有未标记的对象会被回收。这种方式可以解决循环引用结果带来的问题，但是大多数情况下，垃圾回收器必须等待标记完全结束才能开始进行垃圾回收。\n\n上面提到的两种算法有多种不同的实现方法，其中最著名可算是标记或拷贝算法（marking or copying algorithm）和并行或并发算法（parallel or concurrent algorithm）。我将在后续的文章中对它们进行介绍。\n\n分代垃圾回收的意思是，将堆划分为几个不同的区域，分别用于存储新对象和老对象。其中“老对象”指的是挺过了几轮垃圾回收而不死的对象。将堆空间分为年轻代和老年代，分别用于存储新对象和老对象可以通过回收生命周期较短的对象，并将生命周期较长的对象从年轻代提升到老年代的方法来减少堆空间中的碎片，降低堆空间碎片化的风险。此外，使用年轻代还有一个好处是，它可以推出对老年代进行垃圾回收的需求（对老年代进行垃圾回收的代价比较大，因为老年代中那些生命周期较长的对象通常包含有更多的引用，遍历一次需要花费更多的时间），因那些生命周期较短的对通常会重用年轻代中的空间。\n\n还有一个值得一提的算法改进是压缩，它可以用来管理堆空间中的碎片。基本上将，压缩就是将对象移动到一起，再释放掉较大的连续空间。如果你对磁盘碎片和处理磁盘碎片的工具比较熟悉的话你就会理解压缩的含义了，只不过这里的压缩是工作在Java堆空间中的。我将在该系列后续的内容中对压缩进行介绍。\n\n#### **结论：回顾与展望**\n\nJVM实现了可移植性（“一次编写，到处运行”）和动态内存管理，这两个特点也是其广受欢迎，并且具有较高生产力的原因。\n\n作为这个系列文章的第一篇，我介绍了编译器如何将字节码转换为平台相关指令的语言，以及如何`动态`优化Java程序的运行性能。不同的编译器迎合了不同应用程序的需要。\n\n此外，简单介绍了内存分配和垃圾回收的一点内容，及其与Java应用程序性能的关系。基本上将，Java应用程序运行的速度越快，填满Java堆所需的时间就越短，触发垃圾回收的频率也越高。这里遇到的问题就是，在应用程序出现OOM错误之前，如何在对应用程序造成的影响尽可能小的情况下，回收足够多的内存空间。将后续的文章中，我们将对传统垃圾回收方法和现今的垃圾回收方法对JVM性能优化的影响做详细讨论。\n\n#### **JVM 性能优化系列**\n\n第一篇 《[JVM性能优化， Part 1 ―― JVM简介](https://chiyuanbo.github.io/2018/04/28/JVM%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%20Part%201%20%E2%80%95%E2%80%95%20JVM%E7%AE%80%E4%BB%8B/) 》\n\n第二篇《[JVM性能优化， Part 2 ―― 编译器](https://chiyuanbo.github.io/2018/04/28/JVM%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%20Part%202%20%E2%80%95%E2%80%95%20%E7%BC%96%E8%AF%91%E5%99%A8/)》\n\n第三篇[《JVM性能优化， Part 3 —— 垃圾回收》](https://chiyuanbo.github.io/2018/04/28/JVM%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%20Part%203%20%20%E2%80%95%E2%80%95%20%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/)\n\n第四篇[《JVM性能优化， Part 4 —— C4 垃圾回收》](https://chiyuanbo.github.io/2018/04/28/JVM%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%20Part%204%20%E2%80%95%E2%80%95%20C4%20%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/)\n\n第五篇[《JVM性能优化， Part 5 —— Java的伸缩性》](https://chiyuanbo.github.io/2018/04/28/JVM%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%20Part%205%20%20%E2%80%95%E2%80%95%20Java%E7%9A%84%E4%BC%B8%E7%BC%A9%E6%80%A7/)","tags":["Java","JVM"]},{"title":"JVM性能优化， Part 2 ―― 编译器","url":"/2022/09/07/cyb-mds/java/jvm/JVM性能优化， Part 2 ―― 编译器/","content":"\n==作者：Eva Andreasson,译者：曹旭东==\n\n[toc]\n\n\n#### **什么是编译器**\n\n简单来说，编译器就是将一种编程语言作为输入，输出另一种可执行语言的工具。大家都熟悉的javac就是一个编译器，所有标准版的JDK中都带有这个工具。javac以Java源代码作为输入，将其翻译为可由JVM执行的字节码。翻译后的字节码存储在.class文件中，在启动Java进程的时候，被载入到Java运行时中。\n\n标准CPU并不能识别字节码，它需要被转换为当前平台所能理解的本地指令。在JVM中，有专门的组件负责将字节码编译为平台相关指令，实际上，这也是一种编译器。有些JVM编译器可以处理多层级的编译工作，例如，编译器在最终将字节码转换为平台相关指令前，会为相关的字节码建立多层级的中间表示（intermediate representation）。\n\n_字节码与JVM_\n\n如果你想了解更多有关字节码与JVM的信息，请阅读 [“Bytecode basics”](http://www.javaworld.com/javaworld/jw-09-1996/jw-09-bytecodes.html)(Bill Venners, JavaWorld)\n\n以平台未知的角度看，我们希望尽可能的保持平台独立性，因此，最后一级的编译，也就是从最低级表示到实际机器码的转换，是与具体平台的处理器架构息息相关的。在最高级的表示上，会因使用静态编译器还是动态编译器而有所区别。在这里，我们可以选择应用程序所以来的可执行环境，期望达到的性能要求，以及我们所面临的资源限制。在本系列的第1篇文章的[静态编译器与动态编译器](http://www.javaworld.com/javaworld/jw-08-2012/120821-jvm-performance-optimization-overview.html)一节中，已经对此有过简要介绍。我将在本文的后续章节中详细介绍这部分内容。\n\n#### **静态编译器与动态编译器**\n\n前文提到的javac就是使用静态编译器的例子。静态编译器解释输入的源代码，并输出程序运行时所需的可执行文件。如果你修改了源代码，那么就需要使用编译器来重新编译代码，否则输出的可执行性文件不会发生变化；这是因为静态编译器的输入是静态的普通文件。\n\n使用静态编译器时，下面的Java代码\n\n````java\nstatic int add7( int x ) {\n     return x+7;\n}\n````\n\n会生成类似如下的字节码：\n\n````java\niload0\nbipush 7\niadd\nireturn\n````\n\n动态编译器会动态的将一种编程语言编译为另一种，即在程序运行时执行编译工作。动态编译与优化使运行时可以根据当前应用程序的负载情况而做出相应的调整。动态编译器非常适合用于Java运行时中，因为Java运行时通常运行在无法预测而又会随着运行而有所变动的环境中。大部分JVM都会使用诸如Just-In-Time编译器的动态编译器。这里面需要注意的是，大部分动态编译器和代码优化有时需要使用额外的数据结构、线程和CPU资源。要做的优化或字节码上下文分析越高级，编译过程所消耗的资源就越多。在大多数运行环境中，相比于经过动态编译和代码优化所获得的性能提升，这些损耗微不足道。\n\n#### **JVM的多样性与Java平台的独立性**\n\n所有的JVM实现都有一个共同点，即它们都试图将应用程序的字节码转换为本地机器指令。一些JVM在载入应用程序后会解释执行应用程序，同时使用性能计数器来查找“热点”代码。还有一些JVM会调用解释执行的阶段，直接编译运行。资源密集型编译任务对应用程序来说可能会产生较大影响，尤其是那些客户端模式下运行的应用程序，但是资源密集型编译任务可以执行一些比较高级的优化任务。\n如果你是Java初学者，JVM本身错综复杂结构会让你晕头转向的。不过，好消息是你无需精通JVM。JVM自己会做好代码编译和优化的工作，所以你无需关心如何针对目标平台架构来编写应用程序才能编译、优化，从而生成更好的本地机器指令。\n\n#### **从字节码到可运行的程序**\n\n当你编写完Java源代码并将之编译为字节码后，下一步就是将字节码指令编译为本地机器指令。这一步会由解释器或编译器完成。\n\n##### **解释**\n\n解释是最简单的字节码编译形式。解释器查找每条字节码指令对应的硬件指令，再由CPU执行相应的硬件指令。\n\n你可以将解释器想象为一个字典：每个单词（字节码指令）都有准确的解释（本地机器指令）。由于解释器每次读取一个字节码指令并立即执行，因此它就没有机会对某个指令集合进行优化。由于每次执行字节码时，解释器都需要做相应的解释工作，因此程序运行起来就很慢。解释执行可以准确执行字节码，但是未经优化而输出的指令集难以发挥目标平台处理器的最佳性能。\n\n##### **编译**\n\n另一方面，编译执行应用程序时，*编译器*会将加载运行时会用到的全部代码。因为编译器可以将字节码编译为本地代码，因此它可以获取到完整或部分运行时上下文信息，并依据收集到的信息决定到底应该如何编译字节码。编译器是根据诸如指令的不同执行分支和运行时上下文数据等代码信息来指定决策的。\n\n当字节码序列被编译为机器代码指令集合时，就可以对这个指令集合做一些优化操作了，优化后的指令集合会被存储到成为code cache的数据结构中。当下一次执行这部分字节码序列时，就会执行这些经过优化后被存储到code cache的指令集合。在某些情况下，性能计数器会失效，并覆盖掉先前所做的优化，这时，编译器会执行一次新的优化过程。使用code cache的好处是优化后的指令集可以立即执行 —— 无需像解释器一样再经过查找的过程或编译过程！这可以加速程序运行，尤其是像Java应用程序这种同一个方法会被多次调用应用程序。\n\n#### **优化**\n\n随着动态编译器一起出现的是性能计数器。例如，编译器会插入性能计数器，以统计每个字节码块（对应与某个被调用的方法）的调用次数。在进行相关优化时，编译器会使用收集到的数据来判断某个字节码块有多“热”，这样可以最大程度的降低对当前应用程序的影响。运行时数据监控有助于编译器完成多种代码优化工作，进一步提升代码执行性能。随着收集到的运行时数据越来越多，编译器就可以完成一些额外的、更加复杂的代码优化工作，例如编译出更高质量的目标代码，使用运行效率更高的代码替换原代码，甚至是剔除冗余操作等。\n\n**示例**\n\n考虑如下代码：\n\n````java\nstatic int add7( int x ) {\n     return x+7;\n}\n````\n\n这段代码经过javac编译后会产生如下的字节码：\n\n````java\niload0\nbipush 7\niadd\nireturn\n````\n\n当调用这段代码时，字节码块会被动态的编译为本地机器指令。当性能计数器（如果这段代码应用了性能计数器的话）发现这段代码的运行次数超过了某个阈值后，动态编译器会对这段代码进行优化编译。后带的代码可能会是下面这个样子：\n\n````java\nlea rax,[rdx+7]\nret\n````\n\n#### **各擅胜场**\n\n不同的Java应用程序需要满足不同的需求。相对来说，企业级服务器端应用程序需要长时间运行，因此可以做更多的优化，而稍小点的客户端应用程序可能要求快速启动运行，占资源少。接下来我们考察三种编译器设置及其各自的优缺点。\n\n##### **客户端编译器**\n\n即大家熟知的优化编译器C1。在启动应用程序时，添加JVM启动参数“-client”可以启用C1编译器。正如启动参数所表示的，C1是一个客户端编译器，它专为客户端应用程序而设计，资源消耗更少，并且在大多数情况下，对应用程序的启动时间很敏感。C1编译器使用性能计数器来收集代码的运行时信息，执行一些简单、无侵入的代码优化任务。\n\n##### **服务器端编译器**\n\n对于那些需要长时间运行的应用程序，例如服务器端的企业级Java应用程序来说，客户端编译器所实现的功能还略有不足，因此服务器端的编译会使用类似C2这类的编译器。启动应用程序时添加命令行参数“-server”可以启用C2编译器。由于大多数服务器端应用程序都会长时间运行，因此相对于运行时间稍短的轻量级客户端应用程序，在服务器端应用程序中启用C2编译器可以收集到更多的运行时数据，也就可以执行一些更高级的编译技术与算法。\n\n**_提示：给服务器端编译器热身_**\n\n对于服务器端编译器来说，在应用程序开始运行之后，编译器可能会在一段时间之后才开始优化“热点”代码，所以服务器端编译器通常需要经过一个“热身”阶段。在服务器端编译器执行性能优化任务之前，要确保应用程序的各项准备工作都已就绪。给予编译器足够多的时间来完成编译、优化的工作才能取得更好的效果。（更多关于编译器热身与监控原理的内容请参见JavaWorld的文章”[Watch your HotSpot compiler go](http://www.javaworld.com/javaqa/2003-04/01-qa-0411-hotspot.html)“。）\n\n在执行编译任务优化任务时，服务器端编译器要比客户端编译器综合考虑更多的运行时信息，执行更复杂的分支分析，即对哪种优化路径能取得更好的效果作出判断。获取的运行时数据越多，编译优化所产生的效果越好。当然，要完成一些复杂的、高级的性能分析任务，编译器就需要消耗更多的资源。使用了C2编译器的JVM会消耗更多的资源，例如更多的线程，更多的CPU指令周期，以及更大的code cache等。\n\n##### **层次编译**\n\n层次编译综合了服务器端编译器和客户端编译器的特点。Azul首先在其Zing JVM中实现了层次编译。最近（就是Java SE 7版本），Oracle Java HotSpot VM也采用了这种设计。在应用程序启动阶段，客户端编译器最为活跃，执行一些由较低的性能计数器阈值出发的性能优化任务。此外，客户端编译器还会插入性能计数器，为一些更复杂的性能优化任务准备指令集，这些任务将在后续的阶段中由服务器端编译器完成。层次编译可以更有效的利用资源，因为编译器在执行一些对应用程序影响较小的编译活动时仍可以继续收集运行时信息，而这些信息可以在将来用于完成更高级的优化任务。使用层次编译可以比解释性的代码性能计数器手机到更多的信息。\n\nFigure 1中展示了纯解释运行、客户端模式运行、服务器端模式运行和层次编译模式运行下性能之间的区别。X轴表示运行时间（单位时间）Y轴表示性能（每单位时间内的操作数）。\n\n![](https://raw.githubusercontent.com/chiyuanbo/pic/master/20221028102329.png)\n\nFigure 1\\. Performance differences between compilers (click to enlarge)\n\n**编译性能对比**\n\n相比于纯解释运行的的代码，以客户端模式编译运行的代码在性能（指单位时间执行的操作）上可以达到约5到10倍，因此而提升了应用程序的运行性能。其间的区别主要在于编译器的效率、编译器所作的优化，以及应用程序在设计实现时针对目标平台做了何种程度的优化。实际上，最后一条不在Java程序员的考虑之列。\n\n相比于客户端编译器，使用服务器端编译器通常会有30%到50%的性能提升。在大多数情况下，这种程度的性能提升足以弥补使用服务器端编译所带来的额外资源消耗。\n\n层次编译综合了服务器端编译器和客户端编译器的优点，使用客户端编译模式实现快速启动和快速优化，使用服务器端编译模式在后续的执行周期中完成高级优化的编译任务。\n\n#### **常用编译优化手段**\n\n到目前为止，已经介绍了优化代码的价值，以及常用JVM编译器是如何以及何时编译代码的。接下来，将用一些实际的例子做个总结。JVM所作的性能优化通常在字节码这一层级（或者是更底层的语言表示），但这里我将使用Java编程语言对优化措施进行介绍。在这一节中，我无法涵盖JVM中所作的所有性能优化，相反，我希望可以激发你的兴趣，使你主动挖掘并学习编译器技术中所包含了数百种高级优化技术（参见相关资源）。\n\n##### **死代码剔除**\n\n死代码剔除指的是，将用于无法被调用的代码，即“死代码”，从源代码中剔除。如果编译器在运行时发现某些指令是不必要的，它会简单的将其从可执行指令集中剔除。例如，在Listing 1中，变量被赋予了确定值，却从未被使用，因此可以在执行时将其完全忽略掉。在字节码这一层级，也就不会有将数值载入到寄存器的操作。没有载入操作意味着可以更少的CPU时间，更好的运行性能，尤其是当这段代码是“热点”代码的时候。\n\nListing 1中展示了示例代码，其中被赋予了固定值的代码从未被使用，属于无用不必要的操作。\n\nListing 1\\. Dead code\n\n````java\nint timeToScaleMyApp(boolean endlessOfResources) {\n  int reArchitect = 24;\n  int patchByClustering = 15;\n  int useZing = 2;\n \n  if(endlessOfResources)\n      return reArchitect + useZing;\n  else\n      return useZing;\n}\n````\n\n在字节码这一层级，如果变量被载入但从未使用，编译器会检测到并剔除这个死代码，如Listing 2所示。剔除死代码可以节省CPU时间，从而提升应用程序的运行速度。\n\nListing 2\\. The same code following optimization\n\n````java\nint timeToScaleMyApp(boolean endlessOfResources) {\n  int reArchitect = 24;\n  //unnecessary operation removed here...\n  int useZing = 2;\n \n  if(endlessOfResources)\n      return reArchitect + useZing;\n  else\n      return useZing;\n}\n````\n\n冗余剔除是一种类似的优化手段，通过剔除掉重复的指令来提升应用程序性能。\n\n##### **内联**\n\n许多优化手段都试图消除机器级跳转指令（例如，x86架构的JMP指令）。跳转指令会修改指令指针寄存器，因此而改变了执行流程。相比于其他汇编指令，跳转指令是一个代价高昂的指令，这也是为什么大多数优化手段会试图减少甚至是消除跳转指令。内联是一种家喻户晓而且好评如潮的优化手段，这是因为跳转指令代价高昂，而内联技术可以将经常调用的、具有不容入口地址的小方法整合到调用方法中。Listing 3到Listing 5中的Java代码展示了使用内联的用法。\n\nListing 3\\. Caller method\n\n````java\nint whenToEvaluateZing(int y) {\n  return daysLeft(y) + daysLeft(0) + daysLeft(y+1);\n}\n````\n\nListing 4\\. Called method\n\n````java\nint daysLeft(int x){\n  if (x == 0)\n     return 0;\n  else\n     return x - 1;\n}\n````\n\nListing 5\\. Inlined method\n\n````java\nint whenToEvaluateZing(int y){\n  int temp = 0;\n \n  if(y == 0) temp += 0; else temp += y - 1;\n  if(0 == 0) temp += 0; else temp += 0 - 1;\n  if(y+1 == 0) temp += 0; else temp += (y + 1) - 1;\n \n  return temp; \n}\n````\n\n在Listing 3到Listing 5的代码中，展示了将调用3次小方法进行内联的示例，这里我们认为使用内联比跳转有更多的优势。\n\n如果被内联的方法本身就很少被调用的话，那么使用内联也没什么意义，但是对频繁调用的“热点”方法进行内联在性能上会有很大的提升。此外，经过内联处理后，就可以对内联后的代码进行进一步的优化，正如Listing 6中所展示的那样。\n\nListing 6\\. After inlining, more optimizations can be applied\n\n````java\nint whenToEvaluateZing(int y){\n  if(y == 0) return y;\n  else if (y == -1) return y - 1;\n  else return y + y - 1;\n}\n````\n\n##### **循环优化**\n\n当涉及到需要减少执行循环时的性能损耗时，循环优化起着举足轻重的作用。执行循环时的性能损耗包括代价高昂的跳转操作，大量的条件检查，和未经优化的指令流水线（即引起CPU空操作或额外周期的指令序列）等。循环优化可以分为很多种，在各种优化手段中占有重要比重。其中值得注意的包括以下几种：\n\n*   合并循环：当两个相邻循环的迭代次数相同时，编译器会尝试将两个循环体进行合并。当两个循环体中没有相互引用的情况，即各自独立时，可以同时执行（并行执行）。\n\n*   反转循环：基本上将就是用do-while循环体换掉常规的while循环，这个do-while循环嵌套在if语句块中。这个替换操作可以节省两次跳转操作，但是，会增加一个条件检查的操作，因此增加的代码量。这种优化方式完美的展示了以少量增加代码量为代价换取较大性能的提升 —— 编译器需要在运行时需要权衡这种得与失，并制定编译策略。\n\n*   分块循环：重新组织循环体，以便迭代数据块时，便于缓存的应用。\n\n*   展开循环：减少判断循环条件和跳转的次数。你可以将之理解为将一些迭代的循环体“内联”到一起，而无需跨越循环条件。展开循环是有风险的，它有可能会降低应用程序的运行性能，因为它会影响流水线的运行，导致产生了冗余指令。再强调一遍，展开循环是编译器在运行时根据各种信息来决定是否使用的优化手段，如果有足够的收益的话，那么即使有些性能损耗也是值得的。\n\n至此，已经简要介绍了编译器对字节码层级（以及更底层）进行优化，以提升应用程序在目标平台的执行性能的几种方式。这里介绍的几种优化手段是比较常用的几种，只是众多优化技术中的几种。在介绍优化方法时配以简单示例和相关解释，希望可以洗发你进行深度探索的兴趣。更多相关内容请参见相关资源。\n\n#### **总结：回顾**\n\n为满足不同需要而使用不同的编译器。\n\n*   解释是将字节码转换为本地机器指令的最简单方式，其工作方式是基于对本地机器指令表的查找。\n*   编译器可以基于性能计数器进行性能优化，但是需要消耗更多的资源（如code cache，优化线程等）。\n*   相比于纯解释执行代码，客户端编译器可以将应用程序的执行性能提升一个数量级（约5到10倍）。\n*   相比于客户端编译器，服务器端编译器可以将应用程序的执行性能提升30%到50%，但会消耗更多的资源。\n*   层次编译综合了客户端编译器和服务器端编译器的优点，既可以像客户端编译器那样快速启动，又可以像服务器端编译器那样，在长时间收集运行时信息的基础上，优化应用程序的性能。\n\n目前，已经出现了很多代码优化的手段。对编译器来说，一个主要的任务就是分析所有的可能性，权衡使用某种优化手段的利弊，在此基础上编译代码，优化应用程序的性能。\n\n#### **JVM 性能优化系列**\n\n第一篇 《[JVM性能优化， Part 1 ―― JVM简介](https://chiyuanbo.github.io/2018/04/28/JVM%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%20Part%201%20%E2%80%95%E2%80%95%20JVM%E7%AE%80%E4%BB%8B/) 》\n\n第二篇《[JVM性能优化， Part 2 ―― 编译器](https://chiyuanbo.github.io/2018/04/28/JVM%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%20Part%202%20%E2%80%95%E2%80%95%20%E7%BC%96%E8%AF%91%E5%99%A8/)》\n\n第三篇[《JVM性能优化， Part 3 —— 垃圾回收》](https://chiyuanbo.github.io/2018/04/28/JVM%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%20Part%203%20%20%E2%80%95%E2%80%95%20%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/)\n\n第四篇[《JVM性能优化， Part 4 —— C4 垃圾回收》](https://chiyuanbo.github.io/2018/04/28/JVM%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%20Part%204%20%E2%80%95%E2%80%95%20C4%20%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/)\n\n第五篇[《JVM性能优化， Part 5 —— Java的伸缩性》](https://chiyuanbo.github.io/2018/04/28/JVM%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%20Part%205%20%20%E2%80%95%E2%80%95%20Java%E7%9A%84%E4%BC%B8%E7%BC%A9%E6%80%A7/)","tags":["Java","JVM"]},{"title":"Jvm内存模型","url":"/2022/09/07/cyb-mds/java/jvm/Jvm内存模型/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n### **内存模型**\nJava内存模型，往往是指Java程序在运行时内存的模型，而Java代码是运行在Java虚拟机之上的，由Java虚拟机通过解释执行(解释器)或编译执行(即时编译器)来完成，故Java内存模型，也就是指Java虚拟机的运行时内存模型。\n\n运行时内存模型，分为线程私有和共享数据区两大类，其中线程私有的数据区包含程序计数器、虚拟机栈、本地方法区，所有线程共享的数据区包含Java堆、方法区，在方法区内有一个常量池。\n\n![](https://raw.githubusercontent.com/chiyuanbo/pic/master/jvm_memory_1.png)\n\n 1. 线程私有区：\n程序计数器，记录正在执行的虚拟机字节码的地址；\n虚拟机栈：方法执行的内存区，每个方法执行时会在虚拟机栈中创建栈帧；\n本地方法栈：虚拟机的Native方法执行的内存区；\n\n 2. 线程共享区：\nJava堆：对象分配内存的区域；\n方法区：存放类信息、常量、静态变量、编译器编译后的代码等数据；\n常量池：存放编译器生成的各种字面量和符号引用，是方法区的一部分。\n\n对于大多数的程序员来说，Java内存比较流行的说法便是堆和栈，这其实是非常粗略的一种划分，这种划分的”堆”对应内存模型的Java堆，”栈”是指虚拟机栈，然而Java内存模型远比这更复杂。\n\n### **详细模型**\n运行时内存分为五大块区域（常量池属于方法区，算作一块区域），前面简要介绍了每个区域的功能，那接下来再详细说明每个区域的内容，Java内存总体结构图如下：\n\n![](https://raw.githubusercontent.com/chiyuanbo/pic/master/stack_heap_info.png)\n\n#### **程序计数器PC**\n程序计数器PC，当前线程所执行的字节码行号指示器。每个线程都有自己计数器，是私有内存空间，该区域是整个内存中较小的一块。\n\n当线程正在执行一个Java方法时，PC计数器记录的是正在执行的虚拟机字节码的地址；当线程正在执行的一个Native方法时，PC计数器则为空（Undefined）。\n\n#### **虚拟机栈**\n虚拟机栈，生命周期与线程相同，是Java方法执行的内存模型。每个方法(不包含native方法)执行的同时都会创建一个栈帧结构，方法执行过程，对应着虚拟机栈的入栈到出栈的过程。\n\n栈帧(Stack Frame)结构\n\n栈帧是用于支持虚拟机进行方法执行的数据结构，是属性运行时数据区的虚拟机站的栈元素。见上图， 栈帧包括：\n\n局部变量表 (locals大小，编译期确定)，一组变量存储空间， 容量以slot为最小单位。\n操作栈(stack大小，编译期确定)，操作栈元素的数据类型必须与字节码指令序列严格匹配\n动态连接， 指向运行时常量池中该栈帧所属方法的引用，为了 动态连接使用。\n前面的解析过程其实是静态解析；\n对于运行期转化为直接引用，称为动态解析。\n方法返回地址\n正常退出，执行引擎遇到方法返回的字节码，将返回值传递给调用者\n异常退出，遇到Exception,并且方法未捕捉异常，那么不会有任何返回值。\n额外附加信息，虚拟机规范没有明确规定，由具体虚拟机实现。\n因此，一个栈帧的大小不会受到\n\n异常(Exception)\n\nJava虚拟机规范规定该区域有两种异常：\n\nStackOverFlowError：当线程请求栈深度超出虚拟机栈所允许的深度时抛出\nOutOfMemoryError：当Java虚拟机动态扩展到无法申请足够内存时抛出\n#### **本地方法栈**\n本地方法栈则为虚拟机使用到的Native方法提供内存空间，而前面讲的虚拟机栈式为Java方法提供内存空间。有些虚拟机的实现直接把本地方法栈和虚拟机栈合二为一，比如非常典型的Sun HotSpot虚拟机。\n\n异常(Exception)：Java虚拟机规范规定该区域可抛出StackOverFlowError和OutOfMemoryError。\n\n#### **Java堆**\nJava堆，是Java虚拟机管理的最大的一块内存，也是GC的主战场，里面存放的是几乎所有的对象实例和数组数据。JIT编译器有栈上分配、标量替换等优化技术的实现导致部分对象实例数据不存在Java堆，而是栈内存。\n\n从内存回收角度，Java堆被分为新生代和老年代；这样划分的好处是为了更快的回收内存；\n从内存分配角度，Java堆可以划分出线程私有的分配缓冲区(Thread Local Allocation Buffer,TLAB)；这样划分的好处是为了更快的分配内存；\n对象创建的过程是在堆上分配着实例对象，那么对象实例的具体结构如下：\n\n![](https://raw.githubusercontent.com/chiyuanbo/pic/master/java_object.png)\n\n对于填充数据不是一定存在的，仅仅是为了字节对齐。HotSpot VM的自动内存管理要求对象起始地址必须是8字节的整数倍。对象头本身是8的倍数，当对象的实例数据不是8的倍数，便需要填充数据来保证8字节的对齐。该功能类似于高速缓存行的对齐。\n\n另外，关于在堆上内存分配是并发进行的，虚拟机采用CAS加失败重试保证原子操作，或者是采用每个线程预先分配TLAB内存.\n\n异常(Exception)：Java虚拟机规范规定该区域可抛出OutOfMemoryError。\n\n#### **方法区**\n方法区主要存放的是已被虚拟机加载的类信息、常量、静态变量、编译器编译后的代码等数据。GC在该区域出现的比较少。\n\n异常(Exception)：Java虚拟机规范规定该区域可抛出OutOfMemoryError。\n\n#### **运行时常量池**\n运行时常量池也是方法区的一部分，用于存放编译器生成的各种字面量和符号引用。运行时常量池除了编译期产生的Class文件的常量池，还可以在运行期间，将新的常量加入常量池，比较常见的是String类的intern()方法。\n\n字面量：与Java语言层面的常量概念相近，包含文本字符串、声明为final的常量值等。\n符号引用：编译语言层面的概念，包括以下3类：\n类和接口的全限定名\n字段的名称和描述符\n方法的名称和描述符\n但是该区域不会抛出OutOfMemoryError异常。\n\n","tags":["Java","JVM"]},{"title":"AtomicInteger","url":"/2022/09/07/cyb-mds/java/J.U.C/AtomicInteger/","content":"\n==作者：YB-Chi==\n\nAtomicInteger 是一个支持原子操作的 Integer 类，就是保证对AtomicInteger类型变量的增加和减少操作是原子性的，不会出现多个线程下的数据不一致问题。通常情况下，在Java里面，++i或者--i不是线程安全的，这里面有三个独立的操作：获取变量当前值，为该值+1/-1，然后写回新的值。在没有额外资源可以利用的情况下，只能使用加锁才能保证读-改-写这三个操作时“原子性”的。\n       \n先看AtomicInteger中的几个方法：\n\n````java\nint addAndGet(int delta)\n\t\t 以原子方式将给定值与当前值相加。实际上就是等于线程安全版本的i=i+delta操作。\nboolean compareAndSet(int expect, int update)\n\t\t 如果当前值 ==预期值，则以原子方式将该值设置为给定的更新值。如果成功就返回true，否则返  回false，并且不修改原值。\nint decrementAndGet()\n\t\t  以原子方式将当前值减 1。相当于线程安全版本的--i操作。\nint get()\n\t\t  获取当前值。\nint getAndAdd(intdelta)\n\t\t 以原子方式将给定值与当前值相加。相当于线程安全版本的t=i;i+=delta;returnt;操作。\nint getAndDecrement()\n\t\t 以原子方式将当前值减 1。相当于线程安全版本的i--操作。\nint getAndIncrement()\n\t\t 以原子方式将当前值加 1。相当于线程安全版本的i++操作。\nint getAndSet(intnewValue)\n\t\t 以原子方式设置为给定值，并返回旧值。相当于线程安全版本的t=i;i=newValue;returnt;操作。\nint incrementAndGet()\n\t\t  以原子方式将当前值加 1。相当于线程安全版本的++i操作。 \n````\n\n再看源码：\n\n````java\nimport sun.misc.Unsafe;  \n\npublic class AtomicInteger extends Number implements java.io.Serializable {  \n\tprivate static final long serialVersionUID = 6214790243416807050L;  \n\n\t// setup to use Unsafe.compareAndSwapInt for updates  \n\tprivate static final Unsafe unsafe = Unsafe.getUnsafe();  \n\tprivate static final long valueOffset;  \n\n\tstatic {  \n\t  try {  \n\t\tvalueOffset = unsafe.objectFieldOffset  \n\t\t\t(AtomicInteger.class.getDeclaredField(\"value\"));  \n\t  } catch (Exception ex) { throw new Error(ex); }  \n\t}  \n\n\tprivate volatile int value;  \n\n\n\tpublic AtomicInteger(int initialValue) {  \n\t\tvalue = initialValue;  \n\t}  \n\tpublic AtomicInteger() {  \n\t}  \n\n\n\tpublic final int get() {  \n\t\treturn value;  \n\t}  \n\n\n\tpublic final void set(int newValue) {  \n\t\tvalue = newValue;  \n\t}  \n\n\n\tpublic final void lazySet(int newValue) {  \n\t\tunsafe.putOrderedInt(this, valueOffset, newValue);  \n\t}  \n\n   //以原子方式设置为给定值，并返回旧值。  \n\tpublic final int getAndSet(int newValue) {  \n\t\tfor (;;) {  \n\t\t\tint current = get();  \n\t\t\tif (compareAndSet(current, newValue))  \n\t\t\t\treturn current;  \n\t\t}  \n\t}  \n\n   //如果当前值 ==预期值，则以原子方式将该值设置为给定的更新值。如果成功就返回，否则返回，并且不修改原值。  \npublic final boolean compareAndSet(int expect, int update) {  \n\treturn unsafe.compareAndSwapInt(this, valueOffset, expect, update);  \n\t}  \n\n\n   public final boolean weakCompareAndSet(int expect, int update) {  \n\treturn unsafe.compareAndSwapInt(this, valueOffset, expect, update);  \n\t}  \n\n\t//以原子方式将当前值加1。相当于线程安全版本的i++操作。  \n\tpublic final int getAndIncrement() {  \n\t\tfor (;;) {  \n\t\t\tint current = get();  \n\t\t\tint next = current + 1;  \n\t\t\tif (compareAndSet(current, next))  \n\t\t\t\treturn current;  \n\t\t}  \n\t}  \n\n  //以原子方式将当前值减 1相当于线程安全版本的i--操作。  \n\tpublic final int getAndDecrement() {  \n\t\tfor (;;) {  \n\t\t\tint current = get();  \n\t\t\tint next = current - 1;  \n\t\t\tif (compareAndSet(current, next))  \n\t\t\t\treturn current;  \n\t\t}  \n\t}  \n\n\t//以原子方式将给定值与当前值相加。相当于线程安全版本的操作。  \n\tpublic final int getAndAdd(int delta) {  \n\t\tfor (;;) {  \n\t\t\tint current = get();  \n\t\t\tint next = current + delta;  \n\t\t\tif (compareAndSet(current, next))  \n\t\t\t\treturn current;  \n\t\t}  \n\t}  \n\n  //以原子方式将当前值加 1相当于线程安全版本的++i操作。  \n\tpublic final int incrementAndGet() {  \n\t\tfor (;;) {  \n\t\t\tint current = get();  \n\t\t\tint next = current + 1;  \n\t\t\tif (compareAndSet(current, next))  \n\t\t\t\treturn next;  \n\t\t}  \n\t}  \n\n   //以原子方式将当前值减 1。相当于线程安全版本的--i操作。  \n\tpublic final int decrementAndGet() {  \n\t\tfor (;;) {  \n\t\t\tint current = get();  \n\t\t\tint next = current - 1;  \n\t\t\tif (compareAndSet(current, next))  \n\t\t\t\treturn next;  \n\t\t}  \n\t}  \n\n   //以原子方式将给定值与当前值相加。实际上就是等于线程安全版本的i=i+delta操作  \n\tpublic final int addAndGet(int delta) {  \n\t\tfor (;;) {  \n\t\t\tint current = get();  \n\t\t\tint next = current + delta;  \n\t\t\tif (compareAndSet(current, next))  \n\t\t\t\treturn next;  \n\t\t}  \n\t}  \n\n}\n````\n\n通过源码可以发现，AtomicInteger并没有使用Synchronized关键字实现原子性，几乎所有的数据更新都用到了compareAndSet(int expect, int update)这个方法。那么就不难看出AtomicInteger这个类的最核心的函数就是compareAndSet(int expect, int update)。\n      \n那么compareAndSet(int expect, int update)是干什么的呢？？？？\n\n我们以getAndIncrement()方法为例，它的源码如下：    \n\n````java\nfor (;;) {  \n\t\tint current = get();  \n\t\tint next = current + 1;  \n\t\tif (compareAndSet(current, next))  \n\t\t\treturn current;  \n\t}  \n} \n````\n\n==单看这段 代码 很难保证原子性， 因为根本没有更新value 的操作==\n\n 重点在于compareAndSet() 函数\n\n    public final boolean compareAndSet(int expect,int update)\n如果==当前值== == ==预期值==，则以原子方式将== 当前值 设置为给定的更新值。==\n\n````java\n参数：\nexpect - 预期值\nupdate - 新值\n返回：\n如果成功，则返回 true。返回 False 指示实际值与预期值不相等。\n````\n\n该函数 只有两个参数，可操作的确实三个值 ，即 value ,expect, update. 他使用了 由硬件保证其原子性的指令 CAS （compare and swap）。\n\ncompareAndSet  函数保证了 比较，赋值这两步操作可以通过一个原子操作完成。\n\n然后看整个函数， 所有代码被放到了一个循环里面， \n\n如果compareAndSet（）执行失败，则说明 在int current = get(); 后，其他线程对value进行了更新， 于是就循环一次，重新获取当前值，直到compareAndSet（）执行成功为止。\n这里需要注意的是AtomicInteger所利用的是基于冲突检测的乐观并发策略（CAS自旋锁）。 所以这种乐观在线程数目非常多的情况下，失败的概率会指数型增加。","tags":["Java","J.U.C"]},{"title":"《JAVA与模式》之单例模式","url":"/2022/09/07/cyb-mds/java/Design pattern/《JAVA与模式》之单例模式/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n在阎宏博士的《JAVA与模式》一书中开头是这样描述单例模式的：\n\n　　==作为对象的创建模式，单例模式确保某一个类只有一个实例，而且自行实例化并向整个系统提供这个实例。这个类称为单例类。==\n\n### 单例模式的结构\n==单例模式的特点：==\n\n    单例类只能有一个实例。\n    \n    单例类必须自己创建自己的唯一实例。\n    \n    单例类必须给所有其他对象提供这一实例。\n#### 饿汉式单例类\n\n````java\n    public class EagerSingleton {\n        private static EagerSingleton instance = new EagerSingleton();\n        /**\n         * 私有默认构造子\n         */\n        private EagerSingleton(){}\n        /**\n         * 静态工厂方法\n         */\n        public static EagerSingleton getInstance(){\n            return instance;\n        }\n    }\n````\n　　上面的例子中，在这个类被加载时，静态变量instance会被初始化，此时类的私有构造子会被调用。这时候，单例类的唯一实例就被创建出来了。\n\n　　饿汉式其实是一种比较形象的称谓。既然饿，那么在创建对象实例的时候就比较着急，饿了嘛，于是在装载类的时候就创建对象实例。\n\n    private static EagerSingleton instance = new EagerSingleton();\n　　饿汉式是典型的空间换时间，当类装载的时候就会创建类的实例，不管你用不用，先创建出来，然后每次调用的时候，就不需要再判断，节省了运行时间。\n\n \n\n#### 懒汉式单例类\n\n````java\n    public class LazySingleton {\n        private static LazySingleton instance = null;\n        /**\n         * 私有默认构造子\n         */\n        private LazySingleton(){}\n        /**\n         * 静态工厂方法\n         */\n        public static synchronized LazySingleton getInstance(){\n            if(instance == null){\n                instance = new LazySingleton();\n            }\n            return instance;\n        }\n    }\n````\n　　上面的懒汉式单例类实现里对静态工厂方法使用了同步化，以处理多线程环境。\n　　\n\n懒汉式其实是一种比较形象的称谓。既然懒，那么在创建对象实例的时候就不着急。会一直等到马上要使用对象实例的时候才会创建，懒人嘛，总是推脱不开的时候才会真正去执行工作，因此在装载对象的时候不创建对象实例。\n\n    private static LazySingleton instance = null;\n懒汉式是典型的时间换空间,就是每次获取实例都会进行判断，看是否需要创建实例，浪费判断的时间。当然，如果一直没有人使用的话，那就不会创建实例，则节约内存空间\n\n由于懒汉式的实现是线程安全的，这样会降低整个访问的速度，而且每次都要判断。那么有没有更好的方式实现呢？\n\n#### 双重检查加锁\n可以使用“双重检查加锁”的方式来实现，就可以既实现线程安全，又能够使性能不受很大的影响。那么什么是“双重检查加锁”机制呢？\n\n所谓“双重检查加锁”机制，指的是：并不是每次进入getInstance方法都需要同步，而是先不同步，进入方法后，先检查实例是否存在，如果不存在才进行下面的同步块，这是第一重检查，进入同步块过后，再次检查实例是否存在，如果不存在，就在同步的情况下创建一个实例，这是第二重检查。这样一来，就只需要同步一次了，从而减少了多次在同步情况下进行判断所浪费的时间。\n\n　　“双重检查加锁”机制的实现会使用关键字volatile，它的意思是：被volatile修饰的变量的值，将不会被本地线程缓存，所有对该变量的读写都是直接操作共享内存，从而确保多个线程能正确的处理该变量。\n\n　　注意：在java1.4及以前版本中，很多JVM对于volatile关键字的实现的问题，会导致“双重检查加锁”的失败，因此“双重检查加锁”机制只只能用在java5及以上的版本。\n\n````java\n    public class Singleton {\n        private volatile static Singleton instance = null;\n        private Singleton(){}\n        public static Singleton getInstance(){\n            //先检查实例是否存在，如果不存在才进入下面的同步块\n            if(instance == null){\n                //同步块，线程安全的创建实例\n                synchronized (Singleton.class) {\n                    //再次检查实例是否存在，如果不存在才真正的创建实例\n                    if(instance == null){\n                        instance = new Singleton();\n                    }\n                }\n            }\n            return instance;\n        }\n    }\n````\n　　这种实现方式既可以实现线程安全地创建实例，而又不会对性能造成太大的影响。它只是第一次创建实例的时候同步，以后就不需要同步了，从而加快了运行速度。\n\n　　==提示：由于volatile关键字可能会屏蔽掉虚拟机中一些必要的代码优化，所以运行效率并不是很高。因此一般建议，没有特别的需要，不要使用。也就是说，虽然可以使用“双重检查加锁”机制来实现线程安全的单例，但并不建议大量采用，可以根据情况来选用。==\n\n　　根据上面的分析，常见的两种单例实现方式都存在小小的缺陷，那么有没有一种方案，既能实现延迟加载，又能实现线程安全呢？\n#### Lazy initialization holder class模式\n\n　　这个模式综合使用了Java的类级内部类和多线程缺省同步锁的知识，很巧妙地同时实现了延迟加载和线程安全。\n\n##### 1.相应的基础知识\n\n　什么是类级内部类？\n　　简单点说，类级内部类指的是，有static修饰的成员式内部类。如果没有static修饰的成员式内部类被称为对象级内部类。\n\n　　类级内部类相当于其外部类的static成分，它的对象与外部类对象间不存在依赖关系，因此可直接创建。而对象级内部类的实例，是绑定在外部对象实例中的。\n\n　　类级内部类中，可以定义静态的方法。在静态方法中只能够引用外部类中的静态成员方法或者成员变量。\n\n　　类级内部类相当于其外部类的成员，只有在第一次被使用的时候才被会装载。\n\n　多线程缺省同步锁的知识\n　　大家都知道，在多线程开发中，为了解决并发问题，主要是通过使用synchronized来加互斥锁进行同步控制。但是在某些情况中，JVM已经隐含地为您执行了同步，这些情况下就不用自己再来进行同步控制了。这些情况包括：\n\n    1.由静态初始化器（在静态字段上或static{}块中的初始化器）初始化数据时\n\n    2.访问final字段时\n\n    3.在创建线程之前创建对象时\n\n    4.线程可以看见它将要处理的对象时\n\n##### 2.解决方案的思路\n\n　　要想很简单地实现线程安全，可以采用静态初始化器的方式，它可以由JVM来保证线程的安全性。比如前面的饿汉式实现方式。但是这样一来，不是会浪费一定的空间吗？因为这种实现方式，会在类装载的时候就初始化对象，不管你需不需要。\n\n　　如果现在有一种方法能够让类装载的时候不去初始化对象，那不就解决问题了？一种可行的方式就是采用类级内部类，在这个类级内部类里面去创建对象实例。这样一来，只要不使用到这个类级内部类，那就不会创建对象实例，从而同时实现延迟加载和线程安全。\n\n　　示例代码如下：\n  \n````java\n    public class Singleton {\n        \n        private Singleton(){}\n        /**\n         *    类级的内部类，也就是静态的成员式内部类，该内部类的实例与外部类的实例\n         *    没有绑定关系，而且只有被调用到时才会装载，从而实现了延迟加载。\n         */\n        private static class SingletonHolder{\n            /**\n             * 静态初始化器，由JVM来保证线程安全\n             */\n            private static Singleton instance = new Singleton();\n        }\n        \n        public static Singleton getInstance(){\n            return SingletonHolder.instance;\n        }\n    }\n````\n　　当getInstance方法第一次被调用的时候，它第一次读取SingletonHolder.instance，导致SingletonHolder类得到初始化；而这个类在装载并被初始化的时候，会初始化它的静态域，从而创建Singleton的实例，由于是静态的域，因此只会在虚拟机装载类的时候初始化一次，并由虚拟机来保证它的线程安全性。\n\n　　这个模式的优势在于，getInstance方法并没有被同步，并且只是执行一个域的访问，因此延迟初始化并没有增加任何访问成本。\n\n　　\n\n　　单例和枚举\n\n　　按照《高效Java 第二版》中的说法：单元素的枚举类型已经成为实现Singleton的最佳方法。用枚举来实现单例非常简单，只需要编写一个包含单个元素的枚举类型即可。\n\n````java\n    public enum Singleton {\n        /**\n         * 定义一个枚举的元素，它就代表了Singleton的一个实例。\n         */\n        \n        uniqueInstance;\n        \n        /**\n         * 单例可以有自己的操作\n         */\n        public void singletonOperation(){\n            //功能处理\n        }\n    }\n````\n　　使用枚举来实现单实例控制会更加简洁，而且无偿地提供了序列化机制，并由JVM从根本上提供保障，绝对防止多次实例化，是更简洁、高效、安全的实现单例的方式。","tags":["Java"]},{"title":"常用","url":"/2022/09/07/cyb-mds/java/code/常用/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n==注意！Hadoop需要配置环境变量==\n\n.class.getResourceAsStream()读取的文件路径只局限与工程的源文件夹中，包括在工程src根目录下，以及类包里面任何位置，但是如果配置文件路径是在除了源文件夹之外的其他文件夹中时，该方法是用不了的。\n\n连接phoenix\n\n    sqlline.py localhost\n    \n    !help查看帮助指令\n    !all                Execute the specified SQL against all the current connections\n    !autocommit         Set autocommit mode on or off\n    !batch              Start or execute a batch of statements\n    !brief              Set verbose mode off\n    !call               Execute a callable statement\n    !close              Close the current connection to the database\n    !closeall           Close all current open connections\n    !columns            List all the columns for the specified table\n    !commit             Commit the current transaction (if autocommit is off)\n    !connect            Open a new connection to the database.\n    !dbinfo             Give metadata information about the database\n    !describe           Describe a table\n    !dropall            Drop all tables in the current database\n    !exportedkeys       List all the exported keys for the specified table\n    !go                 Select the current connection\n    !help               Print a summary of command usage\n    !history            Display the command history\n    !importedkeys       List all the imported keys for the specified table\n    !indexes            List all the indexes for the specified table\n    !isolation          Set the transaction isolation for this connection\n    !list               List the current connections\n    !manual             Display the SQLLine manual\n    !metadata           Obtain metadata information\n    !nativesql          Show the native SQL for the specified statement\n    !outputformat       Set the output format for displaying results\n                        (table,vertical,csv,tsv,xmlattrs,xmlelements)\n    !primarykeys        List all the primary keys for the specified table\n    !procedures         List all the procedures\n    !properties         Connect to the database specified in the properties file(s)\n    !quit               Exits the program\n    !reconnect          Reconnect to the database\n    !record             Record all output to the specified file\n    !rehash             Fetch table and column names for command completion\n    !rollback           Roll back the current transaction (if autocommit is off)\n    !run                Run a script from the specified file\n    !save               Save the current variabes and aliases\n    !scan               Scan for installed JDBC drivers\n    !script             Start saving a script to a file\n    !set                Set a sqlline variable\n    \n    !tables查看所有表\n    select * from FJUDM4.HBASE_MD_OMS_T_PROTECTAZFXYB ;//查询的时候库名+表名\nlinux使用jdbc的方式去连接hive 可以测试jdbc是否能用\n\nbeeline -u jdbc:hive2://192.168.6.1:10009 -n yarn\njdbc的ResultSetMetaData.getColumnCount()是列的数量\n\n北京实验室环境\n\nD:\\电科院\\集群环境-新.html\n\ntab的反向是shift+tab\n\ndm的查询不加库名和中间名会查不出来\n复制路径打开dm的说明：D:\\电科院\\随记\\DM.docx\n\n![](https://raw.githubusercontent.com/chiyuanbo/pic/master/5930d29698dfa.png)\n\n\n\nhive文档的路径：D:\\电科院\\随记\\hive.docx\n\n遍历ResultSet\n\n为什么遍历ResultSet，行列要从1开始。\n\n因为Resultset的第一行的第一列都是空的，要用rs.next()到第一行才能进行读取。\n\n    Statement stmt=null;\n    ResultSet rs=null;\n    ResultSetMetaDatam=null;//获取 列信息\n    \n    try\n    {\n    stmt=con.createStatement();\n    rs=stmt.executeQuery(sql);\n    m=rs.getMetaData();\n    \n    int columns=m.getColumnCount();\n    ==//显示列,表格的表头==\n    ==(绝对不可以使用while(rs.next())去遍历)==\n    for(int i=1;i<=columns;i++)\n    {\n    System.out.print(m.getColumnName(i));\n    System.out.print(\"\\t\\t\");\n    }\n    \n    System.out.println();\n    ==//显示表格内容==\n    while(rs.next())\n    {\n    for(int i=1;i<=columns;i++)\n    {\n    System.out.print(rs.getString(i));\n    System.out.print(\"\\t\\t\");\n    }\n    System.out.println();\n    }\nhive查询的double格式能显示全部小数，dm显示4位\nhive查询的表结构小写，dm大写\n\n将double类型保留两位小数\n\n    double d = (double)Math.round(Double.valueOf(str)*100)/100;这个方法有问题如果是5.00  转换为5.0\n    \n    Double d = Double.valueOf(str);\n    DecimalFormat df = new DecimalFormat(\"#.00\");\n    String d2 =  df.format(d);这个方法不好用  比如0.25  处理后会变为.25\n    \n    String.format(\"%.2f\", Double.valueOf(str))使用这个","tags":["Code","java"]},{"title":"枚举实现单例连接数据库","url":"/2022/09/07/cyb-mds/java/code/枚举实现单例连接数据库/","content":"\n==作者：YB-Chi==\n\n创建一个jdbc.propertis文件，其内容如下：\n\n````java\ndriverClass = com.mysql.jdbc.Driver\njdbcUrl = jdbc:mysql://localhost:3306/liaokailin\nuser = root\npassword = mysqladmin\nmaxPoolSize = 20\nminPoolSize = 5\n````\n\n创建一个MyDataBaseSource的枚举：\n\n````java\nimport java.sql.Connection;\nimport java.sql.SQLException;\nimport java.util.ResourceBundle;\n\nimport com.mchange.v2.c3p0.ComboPooledDataSource;\n\npublic enum MyDataBaseSource {\n\tDATASOURCE;\n\tprivate ComboPooledDataSource cpds = null;\n\nprivate MyDataBaseSource() {\n\ttry {\n\t\t\t/*--------获取properties文件内容------------*/\n\n\t\t\t// 方法一:\n\t\t\t/*\n\t\t\t * InputStream is =\n\t\t\t * MyDBSource.class.getClassLoader().getResourceAsStream(\"jdbc.properties\");\n\t\t\t * Properties p = new Properties(); p.load(is);\n\t\t\t * System.out.println(p.getProperty(\"driverClass\") );\n\t\t\t */\n\n\t\t\t// 方法二：(不需要properties的后缀)\n\t\t\t/*\n\t\t\t * ResourceBundle rb = PropertyResourceBundle.getBundle(\"jdbc\") ;\n\t\t\t * System.out.println(rb.getString(\"driverClass\"));\n\t\t\t */\n\n\t\t\t// 方法三：(不需要properties的后缀)\n\t\t\tResourceBundle rs = ResourceBundle.getBundle(\"jdbc\");\n\t\t\tcpds = new ComboPooledDataSource();\n\t\t\tcpds.setDriverClass(rs.getString(\"driverClass\"));\n\t\t\tcpds.setJdbcUrl(rs.getString(\"jdbcUrl\"));\n\t\t\tcpds.setUser(rs.getString(\"user\"));\n\t\t\tcpds.setPassword(rs.getString(\"password\"));\n\t\t\tcpds.setMaxPoolSize(Integer.parseInt(rs.getString(\"maxPoolSize\")));\n\t\t\tcpds.setMinPoolSize(Integer.parseInt(rs.getString(\"minPoolSize\")));    \n\t\t\t System.out.println(\"-----调用了构造方法------\");\n\n\t\t} catch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t}\n\t}\n\n\tpublic Connection getConnection() {\n\t\ttry {\n\t\t\treturn cpds.getConnection();\n\t\t} catch (SQLException e) {\n\t\t\treturn null;\n\t\t}\n\t}\n\n}\n````\n测试代码：\n\n````java\npublic class Test {\n\tpublic static void main(String[] args) {\n\t\tMyDataBaseSource.DATASOURCE.getConnection() ;\n\t\tMyDataBaseSource.DATASOURCE.getConnection() ;\n\t\tMyDataBaseSource.DATASOURCE.getConnection() ;\n\t}\n}\n````\n结果如下：\n````java\n    -----调用了构造方法------\n    2013-7-17 17:10:57 com.mchange.v2.c3p0.impl.AbstractPoolBackedDataSource getPoolManager\n    信息: Initializing c3p0 pool... com.mchange.v2.c3p0.ComboPooledDataSource [ acquireIncrement -> 3, acquireRetryAttempts -> 30, acquireRetryDelay -> 1000, autoCommitOnClose -> false, automaticTestTable -> null, breakAfterAcquireFailure -> false, checkoutTimeout -> 0, connectionCustomizerClassName -> null, connectionTesterClassName -> com.mchange.v2.c3p0.impl.DefaultConnectionTester, dataSourceName -> 1hge16d8v1tgb0wppydrzz|2c1e6b, debugUnreturnedConnectionStackTraces -> false, description -> null, driverClass -> com.mysql.jdbc.Driver, factoryClassLocation -> null, forceIgnoreUnresolvedTransactions -> false, identityToken -> 1hge16d8v1tgb0wppydrzz|2c1e6b, idleConnectionTestPeriod -> 0, initialPoolSize -> 3, jdbcUrl -> jdbc:mysql://localhost:3306/kaoqin, maxAdministrativeTaskTime -> 0, maxConnectionAge -> 0, maxIdleTime -> 0, maxIdleTimeExcessConnections -> 0, maxPoolSize -> 20, maxStatements -> 0, maxStatementsPerConnection -> 0, minPoolSize -> 5, numHelperThreads -> 3, preferredTestQuery -> null, properties -> {user=******, password=******}, propertyCycle -> 0, statementCacheNumDeferredCloseThreads -> 0, testConnectionOnCheckin -> false, testConnectionOnCheckout -> false, unreturnedConnectionTimeout -> 0, usesTraditionalReflectiveProxies -> false ]\n````\n\n很显然获得了三个Connection连接，但是只调用了一次枚举的构造方法,从而通过枚举实现了单例的设计","tags":["Code","java"]},{"title":"编码与乱码","url":"/2022/09/07/cyb-mds/java/code/编码与乱码/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n\n### 乱码的根源\n\n源编码与目标编码的不一致. 而中文window系统默认编码GBK,害惨了多少程序员.\n\n要尽量减少出现乱码,我个人认为要做到5码合一, IDE(Eclipse/idea),页面(jsp/其他模板引擎),应用服务器(tomcat等), 源码(Java源码及周边文件),数据库编码.\n### 将Eclipse设置为UTF-8\n\n打开Eclipse安装目录下的eclipse.ini,在最末尾新增一行\n\n    -Dfile.encoding=UTF-8\n\n修改之后的,重启eclipse即可.\n### JSP页面编码\n\n    <%@ page language=\"java\" contentType=\"text/html; charset=UTF-8\" pageEncoding=\"UTF-8\" %>\n    可以设置eclipse的jsp设置\n\n### tomcat编码\n\n打开 tomcat安装目录下的 binsetenv.bat ,该文件通常不存在,新建之, 添加如下内容\n\n    set JAVA_OPTS=-Dfile.encoding=UTF-8\n\n打开confserver.conf, 在8080端口所属的Connector节点,添加URIEncoding,可解决大部分GET请求中文乱码的问题\n\n    URIEncoding=\"UTF-8\"\n\n### 源码的编码\n\n通常情况下, 文件本身的编码,取决于新建文件时,IDE或Project的编码.\n\n另外一个隐藏的编码,是maven/ant编译java源文件时使用的编码\n\nmaven的配置如下\n\n    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n\n### 数据库编码\n\n通常来说,如果其他4码解决了,那大部分情况下是mysql会出现编码问题\n\nmysql有4个编码\n\n    Server characterset:    utf8      // 服务器字节集\n    Db     characterset:    utf8      // 当前数据集字节集\n    Client characterset:    utf8      // 客户端字节集\n    Conn.  characterset:    utf8      // 当前连接的字节集\n\n修改my.ini的mysqld小节,设置服务器字符集,是最佳解决方式\n\n    [mysqld]\n    character-set-server = utf8\n\n然而,对于现有系统,全局修改的风险比较大,所以,可以在客户端解决,即jdbcurl上做配置\n\n    jdbc:mysql://127.0.0.1/nutzdemo?useUnicode=true&characterEncoding=UTF-8\n\n### Nutz的相关日志\n\nnutz在启动时也会打印周围的编码信息,帮助排查.\n\n    21:22:23.235 INFO  (NutLoading.java:55) load - Nutz Version : 1.r.59 \n    21:22:23.235 INFO  (NutLoading.java:56) load - Nutz.Mvc[nutz] is initializing ...\n    21:22:23.235 DEBUG (NutLoading.java:60) load - Web Container Information:\n    21:22:23.237 DEBUG (NutLoading.java:61) load -  - Default Charset : UTF-8\n    21:22:23.237 DEBUG (NutLoading.java:62) load -  - Current . path  : D:\\nutzbook\\eclipse\\.\n    21:22:23.237 DEBUG (NutLoading.java:63) load -  - Java Version    : 1.8.0_112\n    21:22:23.237 DEBUG (NutLoading.java:65) load -  - Timezone        : Asia/Shanghai\n    21:22:23.238 DEBUG (NutLoading.java:66) load -  - OS              : Windows 7 amd64\n    21:22:23.238 DEBUG (NutLoading.java:67) load -  - ServerInfo      : Apache Tomcat/9.0.0.M13\n    21:22:23.238 DEBUG (NutLoading.java:68) load -  - Servlet API     : 4.0\n    21:22:23.238 DEBUG (NutLoading.java:73) load -  - ContextPath     : /nutzbook\n    21:22:25.134 DEBUG (DaoSupport.java:199) invoke - JDBC Name   --> MySQL Connector Java\n    21:22:25.135 DEBUG (DaoSupport.java:201) invoke - JDBC URL    --> jdbc:mysql://127.0.0.1:3306/nutzbook\n    21:22:25.145 DEBUG (MysqlJdbcExpert.java:212) checkDataSource - Mysql : character_set_client=utf8\n    21:22:25.146 DEBUG (MysqlJdbcExpert.java:212) checkDataSource - Mysql : character_set_connection=utf8\n    21:22:25.146 DEBUG (MysqlJdbcExpert.java:212) checkDataSource - Mysql : character_set_database=utf8\n    21:22:25.146 DEBUG (MysqlJdbcExpert.java:212) checkDataSource - Mysql : character_set_filesystem=binary\n    21:22:25.146 DEBUG (MysqlJdbcExpert.java:212) checkDataSource - Mysql : character_set_results=\n    21:22:25.146 DEBUG (MysqlJdbcExpert.java:212) checkDataSource - Mysql : character_set_server=utf8\n    21:22:25.147 DEBUG (MysqlJdbcExpert.java:212) checkDataSource - Mysql : character_set_system=utf8\n\nDefault Charset的编码,在Eclipse环境内, 通过eclipse.ini调整,在tomcat内的话,通过setenv.bat调整.\n\nmysql的编码,通过修改my.ini或jdbc url进行调整","tags":["Code","Java"]},{"title":"Caffeine缓存","url":"/2022/09/07/cyb-mds/java/code/Caffeine/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n#### Caffeine介绍\n\nCaffeine是基于Java 1.8的高性能本地缓存库，由Guava改进而来，而且在Spring5开始的默认缓存实现就将Caffeine代替原来的Google Guava，官方说明指出，其缓存命中率已经接近最优值。实际上Caffeine这样的本地缓存和ConcurrentMap很像，即支持并发，并且支持O(1)时间复杂度的数据存取。二者的主要区别在于：\n\nConcurrentMap将存储所有存入的数据，直到你显式将其移除；\nCaffeine将通过给定的配置，自动移除“不常用”的数据，以保持内存的合理占用。\n因此，一种更好的理解方式是：Cache是一种带有存储和移除策略的Map。\n\n#### 引入依赖\n\n```\n<dependency>\n    <groupId>com.github.ben-manes.caffeine</groupId>\n    <artifactId>caffeine</artifactId>\n    <version>3.1.1</version>\n</dependency>\n```\n\n集成springboot\n\n```\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-cache</artifactId>\n</dependency>\n```\n\n#### Caffeine配置说明\n\n| 参数              |   类型   | 描述                                                         |\n| ----------------- | :------: | ------------------------------------------------------------ |\n| initialCapacity   | integer  | 初始的缓存空间大小                                           |\n| maximumSize       |   long   | 缓存的最大条数                                               |\n| maximumWeight     |   long   | 缓存的最大权重                                               |\n| expireAfterAccess | duration | 最后一次写入或访问后，指定经过多长的时间过期                 |\n| expireAfterWrite  | duration | 最后一次写入后，指定经过多长的时间缓存过期                   |\n| refreshAfterWrite | duration | 创建缓存或者最近一次更新缓存后，经过指定的时间间隔后刷新缓存 |\n| weakKeys          | boolean  | 打开 key 的弱引用                                            |\n| weakValues        | boolean  | 打开 value 的弱引用                                          |\n| softValues        | boolean  | 打开 value 的软引用                                          |\n| recordStats       |    -     | 开发统计功能                                                 |\n\n**注意：**\n\n- `weakValues` 和 `softValues` 不可以同时使用。\n- `maximumSize` 和 `maximumWeight` 不可以同时使用。\n- `expireAfterWrite` 和 `expireAfterAccess` 同时存在时，以 `expireAfterWrite` 为准。\n\n#### 注解说明\n\n- @EnableCaching：开启基于注解的缓存-\n- @Cacheable：表示该方法支持缓存。当调用被注解的方法时，如果对应的键已经存在缓存，则不再执行方法体，而从缓存中直接返回。当方法返回null时，将不进行缓存操作。\n- @CachePut：表示执行该方法后，其值将作为最新结果更新到缓存中，每次都会执行该方法。\n- @CacheEvict：表示执行该方法后，将触发缓存清除操作。\n- @Caching：用于组合前三个注解\n\n##### @EnableCaching\n\n```java\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cache.annotation.EnableCaching;\n\n@SpringBootApplication\n@EnableCaching\npublic class SpringbootCacheApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(SpringbootCacheApplication.class, args);\n    }\n\n}\n```\n\n##### @Cacheable\n\n- cacheNames/value：指定缓存组件的名字，数组形式\n- key：缓存数据使用的key，确定缓存可以用唯一key进行指定；eg：编写SpEL； #id，参数id的值 ,，#a0(第一个参数)， #p0(和a0的一样的意义) ，#root.args[0]\n- keyGenerator：key的生成器；可以自己指定key的生成器的组件id（注意： **key/keyGenerator：二选一使用**;不能同时使用）\n- cacheManager：指定缓存管理器；或者cacheResolver指定获取解析器\n- condition：指定符合条件的情况下才缓存；使用SpEl表达式，eg：condition = \"#a0>1\"：第一个参数的值>1的时候才进行缓存\n- unless:否定缓存；当unless指定的条件为true，方法的返回值就不会被缓存；eg：unless = \"#a0！=2\":如果第一个参数的值不是2，结果不缓存；\n- sync：是否使用异步模式\n\n```java\n@Cacheable(\n    value = {\"emp\"}, \n    /*keyGenerator = \"myKeyGenerator\",*/\n    key = \"#id\",\n    condition = \"#a0>=1\",\n    unless = \"#a0!=2\"\n)\npublic Employee getEmp(Integer id) {\n    Employee employee = this.employeeMapper.getEmpById(id);\n    LOG.info(\"查询{}号员工数据\",id);\n    return employee;\n}\n```\n\n这里也可以使用自定义的keyGenerator，使用属性keyGenerator = \"myKeyGenerator\n\n定义一个@Bean类，将KeyGenerator添加到Spring容器\n\n```java\n@Configuration\npublic class CacheConfig {\n\n    @Bean(value = {\"myKeyGenerator\"})\n    public KeyGenerator keyGenerator(){\n        return new KeyGenerator() {\n            @Override\n            public Object generate(Object target, Method method, Object... params) {\n                return method.getName()+\"[\"+ Arrays.asList(params).toString()+\"]\";\n            }\n        };\n    }\n}\n```\n\n##### @CachePut\n\n@CachePut注解也是一个用来缓存的注解，不过缓存和@Cacheable有明显的区别是**既调用方法，又更新缓存数据**，也就是执行方法操作之后再来同步更新缓存，所以这个主键常用于更新操作，也可以用于查询，主键属性和@Cacheable有很多类似的，详情参看@CachePut源码\n\n```java\n /**\n *  @CachePut：既调用方法，又更新缓存数据；同步更新缓存\n *  修改了数据，同时更新缓存\n */\n @CachePut(value = {\"emp\"}, key = \"#result.id\")\n public Employee updateEmp(Employee employee){\n     employeeMapper.updateEmp(employee);\n     LOG.info(\"更新{}号员工数据\",employee.getId());\n     return employee;\n }\n```\n\n##### @CacheEvict\n\n- key：指定要清除的数据\n- allEntries = true：指定清除这个缓存中所有的数据\n- beforeInvocation = false：默认代表缓存清除操作是在方法执行之后执行\n- beforeInvocation = true：代表清除缓存操作是在方法运行之前执行\n\n```java\n@CacheEvict(value = {\"emp\"}, beforeInvocation = true,key=\"#id\")\npublic void deleteEmp(Integer id){\n    employeeMapper.deleteEmpById(id);\n    //int i = 10/0;\n}\n```\n\n##### @Caching\n\n@Caching 用于定义复杂的缓存规则，可以集成@Cacheable和 @CachePut\n\n```java\n // @Caching 定义复杂的缓存规则\n    @Caching(\n            cacheable = {\n                    @Cacheable(/*value={\"emp\"},*/key = \"#lastName\")\n            },\n            put = {\n                    @CachePut(/*value={\"emp\"},*/key = \"#result.id\"),\n                    @CachePut(/*value={\"emp\"},*/key = \"#result.email\")\n            }\n    )\n    public Employee getEmpByLastName(String lastName){\n        return employeeMapper.getEmpByLastName(lastName);\n    }\n```\n\n**附录拓展：SpEL表达式用法**\n\nCache SpEL available metadata\n\n| 名称          | 位置                           | 描述                                                         | 示例                 |\n| ------------- | ------------------------------ | ------------------------------------------------------------ | -------------------- |\n| methodName    | root对象                       | 当前被调用的方法名                                           | #root.methodname     |\n| method        | root对象                       | 当前被调用的方法                                             | #root.method.name    |\n| target        | root对象                       | 当前被调用的目标对象实例                                     | #root.target         |\n| targetClass   | root对象                       | 当前被调用的目标对象的类                                     | #root.targetClass    |\n| args          | root对象                       | 当前被调用的方法的参数列表                                   | #root.args[0]        |\n| caches        | root对象                       | 当前方法调用使用的缓存列表                                   | #root.caches[0].name |\n| argument Name | 执行上下文(avaluation context) | 当前被调用的方法的参数，如findArtisan(Artisan artisan),可以通过#artsian.id获得参数 | #artsian.id          |\n| result        | 执行上下文(evaluation context) | 方法执行后的返回值（仅当方法执行后的判断有效，如 unless cacheEvict的beforeInvocation=false） | #result              |\n\n#### 实战\n\n**@Cacheable**,查询用户功能,走了缓存就不走方法\n\n```java\n@Cacheable(value = \"user\", key = \"#user.id\")\n    public User getUserById(User user) {\n        User userByUserId = userMapper.findUserByUserId(user.getId());\n        System.out.println(\"走了数据库:\" + user.getId());\n        return userByUserId;\n    }\n```\n\n**@CachePut**,注册用户功能,走方法也更新缓存\n\n```java\n@CachePut(value = \"user\", key = \"#user.id\")\npublic User register(User user) {\n    user.setCreateTime(new Date());\n    user.setStatus(\"0\");\n    //查询是否有相同用户名的用户\n    List<User> userList = userMapper.findByUsername(user.getUserName());\n    if (userList.size() > 0) return null;\n    //将密码进行加密操作\n    String encodePassword = passwordEncoder.encode(user.getPassword());\n    user.setPassword(encodePassword);\n    userMapper.register(user);\n    return user;\n}\n```\n\n**@CacheEvict**,标记用户删除状态,删除指定缓存,默认缓存清除操作是在方法执行之后执行\n\n```java\n@CacheEvict(value = \"user\", key = \"#user.id\")\n    public void markUserDelByUserId(User user) {\n        System.out.println(\"删除id:\" + user.getId());\n        userMapper.markUserDelByUserId(user.getId());\n    }\n```\n\n","tags":["Code","Java"]},{"title":"不得不进行的JS学习_1","url":"/2022/09/07/cyb-mds/FE/不得不进行的JS学习_1/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n### 数据类型\n\n- [`Number`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Number)（数字）\n\n- [`String`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/String)（字符串）\n\n- [`Boolean`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Boolean)（布尔）\n\n- [`Symbol`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Symbol)（符号）（ES2015 新增）\n\n- `Object`\n\n  （对象）\n\n  - [`Function`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Function)（函数）\n  - [`Array`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Array)（数组）\n  - [`Date`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Date)（日期）\n  - [`RegExp`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/RegExp)（正则表达式）\n\n- [`null`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Operators/null)（空）\n\n- [`undefined`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/undefined)（未定义）\n\n### 数字\n\n在 JavaScript（除了[`BigInt`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/BigInt)）当中，**并不存在整数/整型 (Integer)。**因此在处理如下的场景时候，您一定要小心：\n\n```javascript\nconsole.log(3 / 2);             // 1.5,not 1\nconsole.log(Math.floor(3 / 2)); // 1\n```\n\n一个看上去是整数的东西，其实都是浮点数。\n\n当然，您也需要小心这种情况：\n\n```javascript\n0.1 + 0.2 = 0.30000000000000004\n```\n\nJavaScript 支持标准的[算术运算符 (en-US)](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators)，包括加法、减法、取模（或取余）等等。还有一个之前没有提及的内置对象 [`Math`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Math)（数学对象），用以处理更多的高级数学函数和常数：\n\n```javascript\nMath.sin(3.5);\nvar circumference = 2 * Math.PI * r;\n```\n\n你可以使用内置函数 [`parseInt()`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/parseInt) 将字符串转换为整型。该函数的第二个可选参数表示字符串所表示数字的基（进制）：\n\n```javascript\nparseInt(\"123\", 10); // 123\nparseInt(\"010\", 10); // 10\n```\n\n如果想把一个二进制数字字符串转换成整数值，只要把第二个参数设置为 2 就可以了：\n\n```javascript\nparseInt(\"11\", 2); // 3\n```\n\nJavaScript 还有一个类似的内置函数 [`parseFloat()`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/parseFloat)，用以解析浮点数字符串，与[`parseInt()`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/parseInt)不同的地方是，`parseFloat()` 只应用于解析十进制数字。\n\n一元运算符 + 也可以把数字字符串转换成数值：\n\n```javascript\n+ \"42\";   // 42\n+ \"010\";  // 10\n+ \"0x10\"; // 16\n```\n\n如果给定的字符串不存在数值形式，函数会返回一个特殊的值 [`NaN`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/NaN)（Not a Number 的缩写）：\n\n```javascript\nparseInt(\"hello\", 10); // NaN\n```\n\n要小心 NaN：如果把 `NaN` 作为参数进行任何数学运算，结果也会是 `NaN`：\n\n```javascript\nNaN + 5; //NaN\n```\n\n可以使用内置函数 [`isNaN()`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/isNaN) 来判断一个变量是否为 `NaN`：\n\n```javascript\nisNaN(NaN); // true\n```\n\nJavaScript 还有两个特殊值：[`Infinity`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Infinity)（正无穷）和 `-Infinity`（负无穷）：\n\n```javascript\n1 / 0; //  Infinity\n-1 / 0; // -Infinity\n```\n\n可以使用内置函数 [`isFinite()`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/isFinite) 来判断一个变量是否是一个有穷数， 如果类型为`Infinity`, `-Infinity` 或 `NaN 则返回 false`：\n\n```javascript\nisFinite(1/0); // false\nisFinite(Infinity); // false\nisFinite(-Infinity); // false\nisFinite(NaN); // false\n\nisFinite(0); // true\nisFinite(2e64); // true\n\nisFinite(\"0\"); // true\n// 如果是纯数值类型的检测，则返回 false：\nNumber.isFinite(\"0\"); // false\n```\n\n### 字符串\n\nJavaScript 中的字符串是一串[Unicode 字符](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Guide/Grammar_and_types#unicode.e7.bc.96.e7.a0.81)序列。这对于那些需要和多语种网页打交道的开发者来说是个好消息。更准确地说，它们是一串 UTF-16 编码单元的序列，每一个编码单元由一个 16 位二进制数表示。每一个 Unicode 字符由一个或两个编码单元来表示。\n\n如果想表示一个单独的字符，只需使用长度为 1 的字符串。\n\n通过访问字符串的 [`length`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/String/length)（编码单元的个数）属性，可以得到它的长度。\n\n```javascript\n\"hello\".length; // 5\n```\n\n这是我们第一次碰到 JavaScript 对象。我们有没有提过你可以像 [object](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Object) 一样使用字符串？是的，字符串也有 [methods](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/String#methods)（方法）能让你操作字符串和获取字符串的信息。\n\n```javascript\n\"hello\".charAt(0); // \"h\"\n\"hello, world\".replace(\"world\", \"mars\"); // \"hello, mars\"\n\"hello\".toUpperCase(); // \"HELLO\"\n```\n\n### 其他类型\n\n与其他类型不同，JavaScript 中的 [`null`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Operators/null) 表示一个空值（non-value），必须使用 null 关键字才能访问，[`undefined`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/undefined) 是一个“undefined（未定义）”类型的对象，表示一个未初始化的值，也就是还没有被分配的值。我们之后再具体讨论变量，但有一点可以先简单说明一下，JavaScript 允许声明变量但不对其赋值，一个未被赋值的变量就是 `undefined` 类型。还有一点需要说明的是，`undefined` 实际上是一个不允许修改的常量。\n\nJavaScript 包含布尔类型，这个类型的变量有两个可能的值，分别是 `true` 和 `false`（两者都是关键字）。根据具体需要，JavaScript 按照如下规则将变量转换成布尔类型：\n\n1. `false`、`0`、空字符串（`\"\"`）、`NaN`、`null` 和 `undefined` 被转换为 `false`\n2. 所有其他值被转换为 `true`\n\n也可以使用 `Boolean()` 函数进行显式转换：\n\n```javascript\nBoolean(''); // false\nBoolean(234); // true\n```\n\n不过一般没必要这么做，因为 JavaScript 会在需要一个布尔变量时隐式完成这个转换操作（比如在 `if` 条件语句中）。所以，有时我们可以把转换成布尔值后的变量分别称为 真值（true values）——即值为 true 和 假值（false values）——即值为 false；也可以分别称为“真的”（truthy）和“假的”（falsy）。\n\nJavaScript 支持包括 `&&`（逻辑与）、`||` （逻辑或）和`!`（逻辑非）在内的一些逻辑运算符。\n\n### 变量\n\n在 JavaScript 中声明一个新变量的方法是使用关键字 [`let`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/let) 、[`const`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/const) 和 [`var`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Statements/var)：\n\n**`let`** 语句声明一个块级作用域的本地变量，并且可选的将其初始化为一个值。\n\n```javascript\nlet a;\nlet name = 'Simon';\n```\n\n下面是使用 **`let`** 声明变量作用域的例子：\n\n```javascript\n// myLetVariable 在这里 *不能* 被引用\n\nfor (let myLetVariable = 0; myLetVariable < 5; myLetVariable++) {\n  // myLetVariable 只能在这里引用\n}\n\n// myLetVariable 在这里 *不能* 被引用\n```\n\n**`const`** 允许声明一个不可变的常量。这个常量在定义域内总是可见的。\n\n```javascript\nconst Pi = 3.14; // 设置 Pi 的值\nPi = 1; // 将会抛出一个错误因为你改变了一个常量的值。\n```\n\n**`var`** 是最常见的声明变量的关键字。它没有其他两个关键字的种种限制。这是因为它是传统上在 JavaScript 声明变量的唯一方法。使用 **`var`** 声明的变量在它所声明的整个函数都是可见的。\n\n```javascript\nvar a;\nvar name = \"simon\";\n```\n\n一个使用 **`var` **声明变量的语句块的例子：\n\n```javascript\n// myVarVariable 在这里 *能* 被引用\n\nfor (var myVarVariable = 0; myVarVariable < 5; myVarVariable++) {\n  // myVarVariable 整个函数中都能被引用\n}\n\n// myVarVariable 在这里 *能* 被引用\n```\n\n如果声明了一个变量却没有对其赋值，那么这个变量的类型就是 `undefined`。\n\nJavaScript 与其他语言的（如 Java）的重要区别是在 JavaScript 中语句块（blocks）是没有作用域的，只有函数有作用域。因此如果在一个复合语句中（如 if 控制结构中）使用 var 声明一个变量，那么它的作用域是整个函数（复合语句在函数中）。 但是从 ECMAScript Edition 6 开始将有所不同的， [`let`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/let) 和 [`const`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/const) 关键字允许你创建块作用域的变量。\n\n### 运算符\n\nJavaScript 的算术操作符包括 `+`、`-`、`*`、`/` 和 `%` ——求余（[与模运算相同 (en-US)](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators#求余_())）。赋值使用 `=` 运算符，此外还有一些复合运算符，如 `+=` 和 `-=`，它们等价于 `x = x operator y`。\n\n```javascript\nx += 5; // 等价于 x = x + 5;\n```\n\n可以使用 `++` 和 `--` 分别实现变量的自增和自减。两者都可以作为前缀或后缀操作符使用。\n\n[+ 操作符 (en-US)](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators#.e5.8a.a0.e6.b3.95_(.2b))还可以用来连接字符串：\n\n```javascript\n\"hello\" + \" world\"; // hello world\n```\n\n如果你用一个字符串加上一个数字（或其他值），那么操作数都会被首先转换为字符串。如下所示：\n\n```javascript\n\"3\" + 4 + 5; // 345\n3 + 4 + \"5\"; // 75\n```\n\n这里不难看出一个实用的技巧——通过与空字符串相加，可以将某个变量快速转换成字符串类型。\n\nJavaScript 中的[比较操作 (en-US)](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators)使用 `<`、`>`、`<=` 和 `>=`，这些运算符对于数字和字符串都通用。相等的比较稍微复杂一些。由两个“`=`（等号）”组成的相等运算符有类型自适应的功能，具体例子如下：\n\n```javascript\n123 == \"123\" // true\n1 == true; // true\n```\n\nJavaScript 还支持 `!=` 和 `!==` 两种不等运算符，具体区别与两种相等运算符的区别类似。\n\nJavaScript 还提供了 [位操作符 (en-US)](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators)。\n\n### 控制结构\n\nJavaScript 的控制结构与其他类 C 语言类似。可以使用 `if` 和 `else` 来定义条件语句，还可以连起来使用：\n\n```javascript\nvar name = \"kittens\";\nif (name == \"puppies\") {\n  name += \"!\";\n} else if (name == \"kittens\") {\n  name += \"!!\";\n} else {\n  name = \"!\" + name;\n}\nname == \"kittens!!\"; // true\n```\n\nJavaScript 支持 `while` 循环和 `do-while` 循环。前者适合常见的基本循环操作，如果需要循环体至少被执行一次则可以使用 `do-while`：\n\n```javascript\nwhile (true) {\n  // 一个无限循环！\n}\n\nvar input;\ndo {\n  input = get_input();\n} while (inputIsNotValid(input))\n```\n\nJavaScript 的 [`for`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Statements/for) 循环与 C 和 Java 中的相同：使用时可以在一行代码中提供控制信息。\n\n```javascript\nfor (var i = 0; i < 5; i++) {\n  // 将会执行五次\n}\n```\n\nJavaScript 也还包括其他两种重要的 for 循环： [`for`...`of`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Statements/for...of)\n\n```javascript\nfor (let value of array) {\n  // do something with value\n}\n```\n\n和 [`for`...`in`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Statements/for...in) ：\n\n```javascript\nfor (let property in object) {\n  // do something with object property\n}\n```\n\n`&&` 和 `||` 运算符使用短路逻辑（short-circuit logic），是否会执行第二个语句（操作数）取决于第一个操作数的结果。在需要访问某个对象的属性时，使用这个特性可以事先检测该对象是否为空：\n\n```javascript\nvar name = o && o.getName();\n```\n\n或用于缓存值（当错误值无效时）：\n\n```javascript\nvar name = cachedName || (cachedName = getName());\n```\n\n类似地，JavaScript 也有一个用于条件表达式的三元操作符：\n\n```javascript\nvar allowed = (age > 18) ? \"yes\" : \"no\";\n```\n\n在需要多重分支时可以使用基于一个数字或字符串的 `switch` 语句：\n\n```javascript\nswitch(action) {\n    case 'draw':\n        drawIt();\n        break;\n    case 'eat':\n        eatIt();\n        break;\n    default:\n        doNothing();\n}\n```\n\n如果你不使用 `break` 语句，JavaScript 解释器将会执行之后 `case` 中的代码。除非是为了调试，一般你并不需要这个特性，所以大多数时候不要忘了加上 `break。`\n\n```javascript\nswitch(a) {\n    case 1: // 继续向下\n    case 2:\n        eatIt();\n        break;\n    default:\n        doNothing();\n}\n```\n\n`default` 语句是可选的。`switch` 和 `case` 都可以使用需要运算才能得到结果的表达式；在 `switch` 的表达式和 `case` 的表达式是使用 `===` 严格相等运算符进行比较的：\n\n```javascript\nswitch(1 + 3){\n    case 2 + 2:\n        yay();\n        break;\n    default:\n        neverhappens();\n}\n```\n\n### 对象\n\nJavaScript 中的对象，Object，可以简单理解成“名称 - 值”对（而不是键值对：现在，ES 2015 的映射表（Map），比对象更接近键值对），不难联想 JavaScript 中的对象与下面这些概念类似：\n\n- Python 中的字典（Dictionary）\n- Perl 和 Ruby 中的散列/哈希（Hash）\n- C/C++ 中的散列表（Hash table）\n- Java 中的散列映射表（HashMap）\n- PHP 中的关联数组（Associative array）\n\n这样的数据结构设计合理，能应付各类复杂需求，所以被各类编程语言广泛采用。正因为 JavaScript 中的一切（除了核心类型，core object）都是对象，所以 JavaScript 程序必然与大量的散列表查找操作有着千丝万缕的联系，而散列表擅长的正是高速查找。\n\n“名称”部分是一个 JavaScript 字符串，“值”部分可以是任何 JavaScript 的数据类型——包括对象。这使用户可以根据具体需求，创建出相当复杂的数据结构。\n\n有两种简单方法可以创建一个空对象：\n\n```javascript\nvar obj = new Object();\n```\n\n和：\n\n```javascript\nvar obj = {};\n```\n\n这两种方法在语义上是相同的。第二种更方便的方法叫作“对象字面量（object literal）”法。这种也是 JSON 格式的核心语法，一般我们优先选择第二种方法。\n\n“对象字面量”也可以用来在对象实例中定义一个对象：\n\n```javascript\nvar obj = {\n    name: \"Carrot\",\n    _for: \"Max\",//'for' 是保留字之一，使用'_for'代替\n    details: {\n        color: \"orange\",\n        size: 12\n    }\n}\n```\n\n对象的属性可以通过链式（chain）表示方法进行访问：\n\n```javascript\nobj.details.color; // orange\nobj[\"details\"][\"size\"]; // 12\n```\n\n下面的例子创建了一个对象原型，**`Person`**，和这个原型的实例，**`You`**。\n\n```javascript\nfunction Person(name, age) {\n  this.name = name;\n  this.age = age;\n}\n\n// 定义一个对象\nvar You = new Person('You', 24);\n// 我们创建了一个新的 Person，名称是 \"You\"\n// (\"You\" 是第一个参数，24 是第二个参数..)\n```\n\n完成创建后，对象属性可以通过如下两种方式进行赋值和访问：\n\n```javascript\n// 点表示法 (dot notation)\nobj.name = 'Simon';\nvar name = obj.name;\n```\n\n和：\n\n```javascript\n// 括号表示法 (bracket notation)\nobj['name'] = 'Simon';\nvar name = obj['name'];\n// can use a variable to define a key\nvar user = prompt('what is your key?')\nobj[user] = prompt('what is its value?')\n```\n\n这两种方法在语义上也是相同的。第二种方法的优点在于属性的名称被看作一个字符串，这就意味着它可以在运行时被计算，缺点在于这样的代码有可能无法在后期被解释器优化。它也可以被用来访问某些以[预留关键字](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Lexical_grammar#keywords)作为名称的属性的值：\n\n```javascript\nobj.for = 'Simon'; // 语法错误，因为 for 是一个预留关键字\nobj[\"for\"] = 'Simon'; // 工作正常\n```\n\n关于对象和原型的详情参见： [Object.prototype (en-US)](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object). 解释对象原型和对象原型链可以参见：[继承与原型链](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Inheritance_and_the_prototype_chain)。\n\n### 数组\n\nJavaScript 中的数组是一种特殊的对象。它的工作原理与普通对象类似（以数字为属性名，但只能通过`[]` 来访问），但数组还有一个特殊的属性——`length`（长度）属性。这个属性的值通常比数组最大索引大 1。\n\n创建数组的传统方法是：\n\n```javascript\nvar a = new Array();\na[0] = \"dog\";\na[1] = \"cat\";\na[2] = \"hen\";\na.length; // 3\n```\n\n使用数组字面量（array literal）法更加方便：\n\n```javascript\nvar a = [\"dog\", \"cat\", \"hen\"];\na.length; // 3\n```\n\n注意，`Array.length` 并不总是等于数组中元素的个数，如下所示：\n\n```javascript\nvar a = [\"dog\", \"cat\", \"hen\"];\na[100] = \"fox\";\na.length; // 101\n```\n\n记住：**数组的长度是比数组最大索引值多一的数**。\n\n如果试图访问一个不存在的数组索引，会得到 `undefined`：\n\n```javascript\ntypeof(a[90]); // undefined\n```\n\n可以通过如下方式遍历一个数组：\n\n```javascript\nfor (var i = 0; i < a.length; i++) {\n    // Do something with a[i]\n}\n```\n\nES2015 引入了更加简洁的 [`for`...`of`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Statements/for...of) 循环，可以用它来遍历可迭代对象，例如数组：\n\n```javascript\nfor (const currentValue of a) {\n  // Do something with currentValue\n}\n```\n\n遍历数组的另一种方法是使用 [`for...in`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Statements/for...in) 循环， 然而这并不是遍历数组元素而是数组的索引。注意，如果哪个家伙直接向 `Array.prototype` 添加了新的属性，使用这样的循环这些属性也同样会被遍历。所以并不推荐使用这种方法遍历数组：\n\n```javascript\nfor (var i in a) {\n  // 操作 a[i]\n}\n```\n\nECMAScript 5 增加了另一个遍历数组的方法，[`forEach()`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Array/forEach)：\n\n```javascript\n[\"dog\", \"cat\", \"hen\"].forEach(function(currentValue, index, array) {\n  // 操作 currentValue 或者 array[index]\n});\n```\n\n如果想在数组后追加元素，只需要：\n\n```javascript\na.push(item);\n```\n\n除了 `forEach()` 和 `push()`，Array（数组）类还自带了许多方法。建议查看 [Array 方法的完整文档](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Array)。\n\n| 方法名称                                             | 描述                                                         |\n| :--------------------------------------------------- | :----------------------------------------------------------- |\n| `a.toString()`                                       | 返回一个包含数组中所有元素的字符串，每个元素通过逗号分隔。   |\n| `a.toLocaleString()`                                 | 根据宿主环境的区域设置，返回一个包含数组中所有元素的字符串，每个元素通过逗号分隔。 |\n| `a.concat(item1[, item2[, ...[, itemN]]])`           | 返回一个数组，这个数组包含原先 `a` 和 `item1、item2、……、itemN` 中的所有元素。 |\n| `a.join(sep)`                                        | 返回一个包含数组中所有元素的字符串，每个元素通过指定的 `sep` 分隔。 |\n| `a.pop()`                                            | 删除并返回数组中的最后一个元素。                             |\n| `a.push(item1, ..., itemN)`                          | 将 `item1、item2、……、itemN` 追加至数组 `a`。                |\n| `a.reverse()`                                        | 数组逆序（会更改原数组 `a`）。                               |\n| `a.shift()`                                          | 删除并返回数组中第一个元素。                                 |\n| `a.slice(start, end)`                                | 返回子数组，以 `a[start]` 开头，以 `a[end]` 前一个元素结尾。 |\n| `a.sort([cmpfn])`                                    | 依据可选的比较函数 `cmpfn` 进行排序，如果未指定比较函数，则按字符顺序比较（即使被比较元素是数字）。 |\n| `a.splice(start, delcount[, item1[, ...[, itemN]]])` | 从 `start` 开始，删除 `delcount` 个元素，然后插入所有的 `item`。 |\n| `a.unshift(item1[, item2[, ...[, itemN]]])`          | 将 `item` 插入数组头部，返回数组新长度（考虑 `undefined`）。 |\n\n### 函数\n\n学习 JavaScript 最重要的就是要理解对象和函数两个部分。最简单的函数就像下面这个这么简单：\n\n```javascript\nfunction add(x, y) {\n    var total = x + y;\n    return total;\n}\n```\n\n这个例子包括你需要了解的关于基本函数的所有部分。一个 JavaScript 函数可以包含 0 个或多个已命名的变量。函数体中的表达式数量也没有限制。你可以声明函数自己的局部变量。`return` 语句在返回一个值并结束函数。如果没有使用 `return` 语句，或者一个没有值的 `return` 语句，JavaScript 会返回 `undefined`。\n\n已命名的参数更像是一个指示而没有其他作用。如果调用函数时没有提供足够的参数，缺少的参数会被 `undefined` 替代。\n\n```javascript\nadd(); // NaN\n// 不能在 undefined 对象上进行加法操作\n```\n\n你还可以传入多于函数本身需要参数个数的参数：\n\n```javascript\nadd(2, 3, 4); // 5\n // 将前两个值相加，4 被忽略了\n```\n\n这看上去有点蠢。函数实际上是访问了函数体中一个名为 [`arguments`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Functions/arguments) 的内部对象，这个对象就如同一个类似于数组的对象一样，包括了所有被传入的参数。让我们重写一下上面的函数，使它可以接收任意个数的参数：\n\n```javascript\nfunction add() {\n    var sum = 0;\n    for (var i = 0, j = arguments.length; i < j; i++) {\n        sum += arguments[i];\n    }\n    return sum;\n}\n\nadd(2, 3, 4, 5); // 14\n```\n\n这跟直接写成 `2 + 3 + 4 + 5` 也没什么区别。我们还是创建一个求平均数的函数吧：\n\n```javascript\nfunction avg() {\n    var sum = 0;\n    for (var i = 0, j = arguments.length; i < j; i++) {\n        sum += arguments[i];\n    }\n    return sum / arguments.length;\n}\navg(2, 3, 4, 5); // 3.5\n```\n\n这个就有用多了，但是却有些冗长。为了使代码变短一些，我们可以使用[剩余参数](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Functions/Rest_parameters)来替换 arguments 的使用。在这方法中，我们可以传递任意数量的参数到函数中同时尽量减少我们的代码。这个**剩余参数操作符**在函数中以：**...variable** 的形式被使用，它将包含在调用函数时使用的未捕获整个参数列表到这个变量中。我们同样也可以将 **for** 循环替换为 **for...of** 循环来返回我们变量的值。\n\n```javascript\nfunction avg(...args) {\n  var sum = 0;\n  for (let value of args) {\n    sum += value;\n  }\n  return sum / args.length;\n}\n\navg(2, 3, 4, 5); // 3.5\n```\n\n> **备注：** 在上面这段代码中，所有被传入该函数的参数都被变量 **args** 所持有。需要注意的是，无论“剩余参数操作符”被放置到函数声明的哪里，它都会把除了自己之前的所有参数存储起来。比如函数：function avg(**firstValue**, ...args) 会把传入函数的第一个值存入 **firstValue**，其他的参数存入 **args**。这是虽然一个很有用的语言特性，却也会带来新的问题。`avg()` 函数只接受逗号分开的参数列表 -- 但是如果你想要获取一个数组的平均值怎么办？一种方法是将函数按照如下方式重写：\n\n```javascript\nfunction avgArray(arr) {\n    var sum = 0;\n    for (var i = 0, j = arr.length; i < j; i++) {\n        sum += arr[i];\n    }\n    return sum / arr.length;\n}\navgArray([2, 3, 4, 5]); // 3.5\n```\n\n但如果能重用我们已经创建的那个函数不是更好吗？幸运的是 JavaScript 允许你通过任意函数对象的 [`apply()`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Function/apply) 方法来传递给它一个数组作为参数列表。\n\n```javascript\navg.apply(null, [2, 3, 4, 5]); // 3.5\n```\n\n传给 `apply()` 的第二个参数是一个数组，它将被当作 `avg()` 的参数列表使用，至于第一个参数 `null`，我们将在后面讨论。这也正说明了一个事实——函数也是对象。\n\n> **备注：** 通过使用[展开语法](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Operators/Spread_syntax)，你也可以获得同样的效果。比如说：`avg(...numbers)`\n\nJavaScript 允许你创建匿名函数：\n\n```javascript\nvar avg = function() {\n    var sum = 0;\n    for (var i = 0, j = arguments.length; i < j; i++) {\n        sum += arguments[i];\n    }\n    return sum / arguments.length;\n};\n```\n\n这个函数在语义上与 `function avg()` 相同。你可以在代码中的任何地方定义这个函数，就像写普通的表达式一样。基于这个特性，有人发明出一些有趣的技巧。与 C 中的块级作用域类似，下面这个例子隐藏了局部变量：\n\n```javascript\nvar a = 1;\nvar b = 2;\n(function() {\n    var b = 3;\n    a += b;\n})();\n\na; // 4\nb; // 2\n```\n\nJavaScript 允许以递归方式调用函数。递归在处理树形结构（比如浏览器 [DOM](https://developer.mozilla.org/zh-CN/docs/Web/API/Document_Object_Model)）时非常有用。\n\n```javascript\nfunction countChars(elm) {\n    if (elm.nodeType == 3) { // 文本节点\n        return elm.nodeValue.length;\n    }\n    var count = 0;\n    for (var i = 0, child; child = elm.childNodes[i]; i++) {\n        count += countChars(child);\n    }\n    return count;\n}\n```\n\n这里需要说明一个潜在问题——既然匿名函数没有名字，那该怎么递归调用它呢？在这一点上，JavaScript 允许你命名这个函数表达式。你可以命名立即调用的函数表达式（IIFE——Immediately Invoked Function Expression），如下所示：\n\n```javascript\nvar charsInBody = (function counter(elm) {\n    if (elm.nodeType == 3) { // 文本节点\n        return elm.nodeValue.length;\n    }\n    var count = 0;\n    for (var i = 0, child; child = elm.childNodes[i]; i++) {\n        count += counter(child);\n    }\n    return count;\n})(document.body);\n```\n\n如上所提供的函数表达式的名称的作用域仅仅是该函数自身。这允许引擎去做更多的优化，并且这种实现更可读、友好。该名称也显示在调试器和一些堆栈跟踪中，节省了调试时的时间。\n\n需要注意的是 JavaScript 函数是它们本身的对象——就和 JavaScript 其他一切一样——你可以给它们添加属性或者更改它们的属性，这与前面的对象部分一样。\n\n### 自定义对象\n\n在经典的面向对象语言中，对象是指数据和在这些数据上进行的操作的集合。与 C++ 和 Java 不同，JavaScript 是一种基于原型的编程语言，并没有 class 语句，而是把函数用作类。那么让我们来定义一个人名对象，这个对象包括人的姓和名两个域（field）。名字的表示有两种方法：“名 姓（First Last）”或“姓，名（Last, First）”。使用我们前面讨论过的函数和对象概念，可以像这样完成定义：\n\n```javascript\nfunction makePerson(first, last) {\n    return {\n        first: first,\n        last: last\n    };\n}\nfunction personFullName(person) {\n    return person.first + ' ' + person.last;\n}\nfunction personFullNameReversed(person) {\n    return person.last + ', ' + person.first;\n}\n\nvar s = makePerson('Simon', 'Willison');\npersonFullName(s); // \"Simon Willison\"\npersonFullNameReversed(s); // \"Willison, Simon\"\n```\n\n上面的写法虽然可以满足要求，但是看起来很麻烦，因为需要在全局命名空间中写很多函数。既然函数本身就是对象，如果需要使一个函数隶属于一个对象，那么不难得到：\n\n```javascript\nfunction makePerson(first, last) {\n    return {\n        first: first,\n        last: last,\n        fullName: function() {\n            return this.first + ' ' + this.last;\n        },\n        fullNameReversed: function() {\n            return this.last + ', ' + this.first;\n        }\n    }\n}\ns = makePerson(\"Simon\", \"Willison\");\ns.fullName(); // \"Simon Willison\"\ns.fullNameReversed(); // Willison, Simon\n```\n\n我们引入了另外一个关键字：[`new`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Operators/new)，它和 `this` 密切相关。它的作用是创建一个崭新的空对象，然后使用指向那个对象的 `this` 调用特定的函数。注意，含有 `this` 的特定函数不会返回任何值，只会修改 `this` 对象本身。`new` 关键字将生成的 `this` 对象返回给调用方，而被 `new` 调用的函数称为构造函数。习惯的做法是将这些函数的首字母大写，这样用 `new` 调用他们的时候就容易识别了。\n\n不过，这个改进的函数还是和上一个例子一样，在单独调用`fullName()` 时，会产生相同的问题。\n\n我们的 Person 对象现在已经相当完善了，但还有一些不太好的地方。每次我们创建一个 Person 对象的时候，我们都在其中创建了两个新的函数对象——如果这个代码可以共享不是更好吗？\n\n```javascript\nfunction personFullName() {\n    return this.first + ' ' + this.last;\n}\nfunction personFullNameReversed() {\n    return this.last + ', ' + this.first;\n}\nfunction Person(first, last) {\n    this.first = first;\n    this.last = last;\n    this.fullName = personFullName;\n    this.fullNameReversed = personFullNameReversed;\n}\n```\n\n这种写法的好处是，我们只需要创建一次方法函数，在构造函数中引用它们。那是否还有更好的方法呢？答案是肯定的。\n\n```javascript\nfunction Person(first, last) {\n    this.first = first;\n    this.last = last;\n}\nPerson.prototype.fullName = function() {\n    return this.first + ' ' + this.last;\n}\nPerson.prototype.fullNameReversed = function() {\n    return this.last + ', ' + this.first;\n}\n```\n\n`Person.prototype` 是一个可以被 `Person` 的所有实例共享的对象。它是一个名叫原型链（prototype chain）的查询链的一部分：当你试图访问 `Person` 某个实例（例如上个例子中的 s）一个没有定义的属性时，解释器会首先检查这个 `Person.prototype` 来判断是否存在这样一个属性。所以，任何分配给 `Person.prototype` 的东西对通过 `this` 对象构造的实例都是可用的。\n\n这个特性功能十分强大，JavaScript 允许你在程序中的任何时候修改原型（prototype）中的一些东西，也就是说你可以在运行时 (runtime) 给已存在的对象添加额外的方法：\n\n```javascript\ns = new Person(\"Simon\", \"Willison\");\ns.firstNameCaps();  // TypeError on line 1: s.firstNameCaps is not a function\n\n\nPerson.prototype.firstNameCaps = function() {\n    return this.first.toUpperCase()\n}\ns.firstNameCaps(); // SIMON\n```\n\n有趣的是，你还可以给 JavaScript 的内置函数原型（prototype）添加东西。让我们给 `String` 添加一个方法用来返回逆序的字符串：\n\n```javascript\nvar s = \"Simon\";\ns.reversed(); // TypeError on line 1: s.reversed is not a function\n\nString.prototype.reversed = function() {\n    var r = \"\";\n    for (var i = this.length - 1; i >= 0; i--) {\n        r += this[i];\n    }\n    return r;\n}\ns.reversed(); // nomiS\n```\n\n定义新方法也可以在字符串字面量上用（string literal）。\n\n```javascript\n\"This can now be reversed\".reversed(); // desrever eb won nac sihT\n```\n\n正如我前面提到的，原型组成链的一部分。那条链的根节点是 `Object.prototype`，它包括 `toString()` 方法——将对象转换成字符串时调用的方法。这对于调试我们的 `Person` 对象很有用：\n\n```javascript\nvar s = new Person(\"Simon\", \"Willison\");\ns; // [object Object]\n\nPerson.prototype.toString = function() {\n    return '<Person: ' + this.fullName() + '>';\n}\ns.toString(); // <Person: Simon Willison>\n```\n\n你是否还记得之前我们说的 `avg.apply()` 中的第一个参数 `null`？现在我们可以回头看看这个东西了。`apply()` 的第一个参数应该是一个被当作 `this` 来看待的对象。下面是一个 `new` 方法的简单实现：\n\n```javascript\nfunction trivialNew(constructor, ...args) {\n    var o = {}; // 创建一个对象\n    constructor.apply(o, args);\n    return o;\n}\n```\n\n这并不是 `new` 的完整实现，因为它没有创建原型（prototype）链。想举例说明 new 的实现有些困难，因为你不会经常用到这个，但是适当了解一下还是很有用的。在这一小段代码里，`...args`（包括省略号）叫作[剩余参数（rest arguments）](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Functions/Rest_parameters)。如名所示，这个东西包含了剩下的参数。\n\n因此，调用\n\n```javascript\nvar bill = trivialNew(Person, \"William\", \"Orange\");\n```\n\n可认为和调用如下语句是等效的\n\n```javascript\nvar bill = new Person(\"William\", \"Orange\");\n```\n\n`apply()` 有一个姐妹函数，名叫 [`call`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Function/call)，它也可以允许你设置 `this`，但它带有一个扩展的参数列表而不是一个数组。\n\n```javascript\nfunction lastNameCaps() {\n    return this.last.toUpperCase();\n}\nvar s = new Person(\"Simon\", \"Willison\");\nlastNameCaps.call(s);\n// 和以下方式等价\ns.lastNameCaps = lastNameCaps;\ns.lastNameCaps();\n```\n\n**内部函数**\n\nJavaScript 允许在一个函数内部定义函数，这一点我们在之前的 `makePerson()` 例子中也见过。关于 JavaScript 中的嵌套函数，一个很重要的细节是，它们可以访问父函数作用域中的变量：\n\n```javascript\nfunction parentFunc() {\n  var a = 1;\n\n  function nestedFunc() {\n    var b = 4; // parentFunc 无法访问 b\n    return a + b;\n  }\n  return nestedFunc(); // 5\n}\n```\n\n如果某个函数依赖于其他的一两个函数，而这一两个函数对你其余的代码没有用处，你可以将它们嵌套在会被调用的那个函数内部，这样做可以减少全局作用域下的函数的数量，这有利于编写易于维护的代码。\n\n这也是一个减少使用全局变量的好方法。当编写复杂代码时，程序员往往试图使用全局变量，将值共享给多个函数，但这样做会使代码很难维护。内部函数可以共享父函数的变量，所以你可以使用这个特性把一些函数捆绑在一起，这样可以有效地防止“污染”你的全局命名空间——你可以称它为“局部全局（local global）”。虽然这种方法应该谨慎使用，但它确实很有用，应该掌握。\n\n### 闭包\n\n闭包是 JavaScript 中最强大的抽象概念之一——但它也是最容易造成困惑的。它究竟是做什么的呢？\n\n```javascript\nfunction makeAdder(a) {\n  return function(b) {\n    return a + b;\n  }\n}\nvar add5 = makeAdder(5);\nvar add20 = makeAdder(20);\nadd5(6); // ?\nadd20(7); // ?\n```\n\n`makeAdder` 这个名字本身，便应该能说明函数是用来做什么的：它会用一个参数来创建一个新的“adder”函数，再用另一个参数来调用被创建的函数时，`makeAdder` 会将一前一后两个参数相加。\n\n从被创建的函数的视角来看的话，这两个参数的来源问题会更显而易见：新函数自带一个参数——在新函数被创建时，便钦定、钦点了前一个参数（如上方代码中的 a、5 和 20，参考 `makeAdder` 的结构，它应当位于新函数外部）；新函数被调用时，又接收了后一个参数（如上方代码中的 b、6 和 7，位于新函数内部）。最终，新函数被调用的时候，前一个参数便会和由外层函数传入的后一个参数相加。\n\n这里发生的事情和前面介绍过的内嵌函数十分相似：一个函数被定义在了另外一个函数的内部，内部函数可以访问外部函数的变量。唯一的不同是，外部函数已经返回了，那么常识告诉我们局部变量“应该”不再存在。但是它们却仍然存在——否则 `adder` 函数将不能工作。也就是说，这里存在 `makeAdder` 的局部变量的两个不同的“副本”——一个是 `a` 等于 5，另一个是 `a` 等于 20。那些函数的运行结果就如下所示：\n\n```javascript\nadd5(6); // 返回 11\nadd20(7); // 返回 27\n```\n\n下面来说说，到底发生了什么了不得的事情。每当 JavaScript 执行一个函数时，都会创建一个作用域对象（scope object），用来保存在这个函数中创建的局部变量。它使用一切被传入函数的变量进行初始化（初始化后，它包含一切被传入函数的变量）。这与那些保存的所有全局变量和函数的全局对象（global object）相类似，但仍有一些很重要的区别：第一，每次函数被执行的时候，就会创建一个新的，特定的作用域对象；第二，与全局对象（如浏览器的 `window` 对象）不同的是，你不能从 JavaScript 代码中直接访问作用域对象，也没有 可以遍历当前作用域对象中的属性 的方法。\n\n所以，当调用 `makeAdder` 时，解释器创建了一个作用域对象，它带有一个属性：`a`，这个属性被当作参数传入 `makeAdder` 函数。然后 `makeAdder` 返回一个新创建的函数（暂记为 `adder`）。通常，JavaScript 的垃圾回收器会在这时回收 `makeAdder` 创建的作用域对象（暂记为 `b`），但是，`makeAdder` 的返回值，新函数 `adder`，拥有一个指向作用域对象 `b` 的引用。最终，作用域对象 `b` 不会被垃圾回收器回收，直到没有任何引用指向新函数 `adder`。\n\n作用域对象组成了一个名为作用域链（scope chain）的（调用）链。它和 JavaScript 的对象系统使用的原型（prototype）链相类似。\n\n一个**闭包**，就是 一个函数 与其 被创建时所带有的作用域对象 的组合。闭包允许你保存状态——所以，它们可以用来代替对象。[这个 StackOverflow 帖子里](https://stackoverflow.com/questions/111102/how-do-javascript-closures-work)有一些关于闭包的详细介绍。","tags":["JS"]},{"title":"Hive元数据中文乱码","url":"/2022/09/07/cyb-mds/database/sql/mysql/Hive元数据中文乱码/","content":"\n==作者：YB-Chi==\n\n```sql\n #修改字段注释字符集\n alter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8;\n #修改表注释字符集\n alter table TABLE_PARAMS modify column PARAM_VALUE varchar(20000) character set utf8;\n #修改分区参数，支持分区建用中文表示\n alter table PARTITION_PARAMS modify column PARAM_VALUE varchar(20000) character set utf8;\n alter table PARTITION_KEYS modify column PKEY_COMMENT varchar(20000) character set utf8;\n #修改索引名注释，支持中文表示\n alter table INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;\n #修改视图，支持视图中文\n ALTER TABLE TBLS modify COLUMN VIEW_EXPANDED_TEXT mediumtext CHARACTER SET utf8;\n ALTER TABLE TBLS modify COLUMN VIEW_ORIGINAL_TEXT mediumtext CHARACTER SET utf8;\n```\n\n","tags":["hive","sql"]},{"title":"MariaDB安装及操作","url":"/2022/09/07/cyb-mds/database/MariaDB安装及操作/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n### 准备软件\n    cupid-mariadb-5.5.52\n    mariadb-5.5.31-winx64\n### 安装步奏\n解压缩软件\n文件夹中，一般包含5个MySQL自带的配置文件，\n    \t\n        my-small.ini、my-medium.ini、my-large.ini、my-huge.ini和my-innodb-heavy-4G.ini，\n请你根据自己机器的内存大小，选择其一，并把它重新命名为my.ini用作基本配置文件。\n\n````java\n[WinMySQLAdmin]\nServer=D:\\YuanBo_Chi\\mariadb-5.5.31-winx64\\bin\\mysqld.exe\n\n[client]\n#password\t= [your_password]\nport\t\t= 3306\nsocket\t\t= /tmp/mysql.sock\ndefault-character-set=utf-8\n\n# *** Application-specific options follow here ***\n\n#\n# The MariaDB server\n#\n[mysqld]\n\n# generic configuration options\nport\t\t= 3306\nsocket\t\t= /tmp/mysql.sock\nbasedir=D:\\YuanBo_Chi\\mariadb-5.5.31-winx64\ndatadir=D:\\YuanBo_Chi\\mariadb-5.5.31-winx64\\data\ndefault-character-set=utf-8\n````\n在目录下运行 mysqld –install servicename则会创建名为servicename的Windows服务。我使用的是\n    \n    mysqld --install MariaDB \n等待成功后，\n输入 \n\n    net start MariaDB \n即可启动服务开始你的MariaDB之旅了。\n 如果需要停止该服务，\n \n 输入 \n \n    net stop MariaDB \n即可停止服务\n将创建完的服务的启动类型设为自动启动，并启动MariaDB。启动MariaDB时，会在data目录内创建数据文件和日志文件。\n\n注：启动后的MariaDB 有一个默认的 root用户，其访问密码为空。修改密码的方法与MySQL类似，执行如下命令，即可修改root的访问密码。\n\n    mysqladmin -u root password \"root\"\n顺便提一句，删除的时候也很简单，输入 \n\n    mysqld.exe --remove MariaDB\n即可    \n### 图形化操作\n下载Navicat Premium\n\n链接MariaDB\n\n\nok","tags":["MariaDB"]},{"title":"gp常用操作","url":"/2022/09/07/cyb-mds/database/gp常用操作/","content":"\n[toc]\n\n##### 表操作\n```sql\n--创建用户\nCREATE USER 'commondata' WITH PASSWORD 'pwd';\n--给用户访问某库权限\nGRANT ALL PRIVILEGES ON DATABASE commondata TO commondata;\n```\n\n```shell\n# 导出表结构 -s表结构 -a数据 不加参数为所有\npg_dump -E UTF8 -s pe2e_lte -n smartinsight -t kpiday202106_hi_test -f kpiday202106_hi_test.sql\n# 脚本中获取count\nc1=$(psql -d lte_mr -c \"select count(1) from table\")\necho ${c1}\n```\n\n##### 数据库维护\n```shell\n# 设置主从32G\ngpconfig -c gp_vmem_protect_limit -m 32768 -v 32768\n# 查询配置\ngpconfig -s gp_vmem_protect_limit\ngpconfig -s查看参数 -c设置参数 -u重新加载参数\n\n# 停止数据库\ngpstop -M fast\n\n# 关闭master节点\ngpstop -m\n\n# 只启动master\ngpstart -m\n\n# 启动所有节点\ngpstart -a\n```\n\n","tags":["DATABASE,POSTGRES,OP"]},{"title":"CentOS 6.7yum安装mysql","url":"/2022/09/07/cyb-mds/database/CentOS 6.7yum安装Mysql/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n#### 一、卸载掉原有mysql\n\n因为mysql数据库在Linux上实在是太流行了，所以目前下载的主流Linux系统版本基本上都集成了mysql数据库在里面，我们可以通过如下命令来查看我们的操作系统上是否已经安装了mysql数据库\n\n```powershell\n[root@jpress /]# rpm -qa | grep mysql\t// 这个命令就会查看该操作系统上是否已经安装了mysql数据库\nmysql-libs-5.1.73-5.el6_6.x86_64　　\n```\n\n有的话，我们就通过 rpm -e 命令 或者 rpm -e --nodeps 命令来卸载掉\n\n```powershell\n[root@xiaoluo ~]# rpm -e mysql-libs-5.1.73-5.el6_6.x86_64　　// 普通删除模式\n[root@xiaoluo ~]# rpm -e --nodeps mysql-libs-5.1.73-5.el6_6.x86_64　　// 强力删除模式，如果使用上面命令删除时，提示有依赖的其它文件，则用该命令可以对其进行强力删除\n```\n\n在删除完以后我们可以通过 rpm -qa | grep mysql 命令来查看mysql是否已经卸载成功！！\n\n#### 二、通过yum来进行mysql的安装\n\n我是通过yum的方式来进行mysql的数据库安装，首先我们可以输入 yum list | grep mysql 命令来查看yum上提供的mysql数据库可下载的版本：\n\n```powershell\n[root@xiaoluo ~]# yum list | grep mysql\n```\n\n就可以得到yum服务器上mysql数据库的可下载版本信息：\n\n\n然后我们可以通过输入 `yum install -y mysql-server mysql mysql-devel` 命令将`mysql mysql-server mysql-devel`都安装好(注意:安装mysql时我们并不是安装了mysql客户端就相当于安装好了mysql数据库了，我们还需要安装mysql-server服务端才行)\n\n```powershell\n[root@xiaoluo ~]# yum install -y mysql-server mysql mysql-deve\n```\n\n在等待了一番时间后，yum会帮我们选择好安装mysql数据库所需要的软件以及其它附属的一些软件!\n\n我们发现，通过yum方式安装mysql数据库省去了很多没必要的麻烦，当出现下面的结果时，就代表mysql数据库安装成功了\n\n\n此时我们可以通过如下命令，查看刚安装好的mysql-server的版本\n\n```powershell\n[root@xiaoluo ~]# rpm -qi mysql-server\n```\n\n我们安装的mysql-server并不是最新版本，如果你想尝试最新版本，那就去mysql官网下载rpm包安装就行了，至此我们的mysql数据库已经安装完成了。\n\n#### 三、mysql数据库的初始化及相关配置\n\n我们在安装完mysql数据库以后，会发现会多出一个mysqld的服务，这个就是咱们的数据库服务，我们通过输入 service mysqld start 命令就可以启动我们的mysql服务。\n\n注意：如果我们是第一次启动mysql服务，mysql服务器首先会进行初始化的配置，如：\n\n```powershell\n[root@xiaoluo ~]# service mysqld start\n\n初始化 MySQL 数据库： WARNING: The host 'xiaoluo' could not be looked up with resolveip.\nThis probably means that your libc libraries are not 100 % compatible\nwith this binary MySQL version. The MySQL daemon, mysqld, should work\nnormally with the exception that host name resolving will not work.\nThis means that you should use IP addresses instead of hostnames\nwhen specifying MySQL privileges !\nInstalling MySQL system tables...\nOK\nFilling help tables...\nOK\n\nTo start mysqld at boot time you have to copy\nsupport-files/mysql.server to the right place for your system\n\nPLEASE REMEMBER TO SET A PASSWORD FOR THE MySQL root USER !\nTo do so, start the server, then issue the following commands:\n\n/usr/bin/mysqladmin -u root password 'new-password'\n/usr/bin/mysqladmin -u root -h xiaoluo password 'new-password'\n\nAlternatively you can run:\n/usr/bin/mysql_secure_installation\n\nwhich will also give you the option of removing the test\ndatabases and anonymous user created by default.  This is\nstrongly recommended for production servers.\n\nSee the manual for more instructions.\n\nYou can start the MySQL daemon with:\ncd /usr ; /usr/bin/mysqld_safe &\n\nYou can test the MySQL daemon with mysql-test-run.pl\ncd /usr/mysql-test ; perl mysql-test-run.pl\n\nPlease report any problems with the /usr/bin/mysqlbug script!\n\n                                                           [确定]\n正在启动 mysqld：                                            [确定]\n```\n\n这时我们会看到第一次启动mysql服务器以后会提示非常多的信息，目的就是对mysql数据库进行初始化操作，当我们再次重新启动mysql服务时，就不会提示这么多信息了，如：\n\n```powershell\n[root@xiaoluo ~]# service mysqld restart\n停止 mysqld：                                             [确定]\n正在启动 mysqld：                                          [确定]\n```\n\n我们在使用mysql数据库时，都得首先启动mysqld服务，我们可以 通过  chkconfig --list | grep mysqld 命令来查看mysql服务是不是开机自动启动，如：\n\n```powershell\n[root@xiaoluo ~]# chkconfig --list | grep mysqld\nmysqld             0:关闭    1:关闭    2:关闭    3:关闭    4:关闭    5:关闭    6:关闭\n```\n\n我们发现mysqld服务并没有开机自动启动，我们当然可以通过 chkconfig mysqld on 命令来将其设置成开机启动，这样就不用每次都去手动启动了\n\n```powershell\n[root@xiaoluo ~]# chkconfig mysqld on\n[root@xiaoluo ~]# chkconfig --list | grep mysql\nmysqld             0:关闭    1:关闭    2:启用    3:启用    4:启用    5:启用    6:关闭\n```\n\nmysql数据库安装完以后只会有一个root管理员账号，但是此时的root账号还并没有为其设置密码，在第一次启动mysql服务时，会进行数据库的一些初始化工作，在输出的一大串信息中，我们看到有这样一行信息 ：\n\n```powershell\n/usr/bin/mysqladmin -u root password 'new-password'　　// 为root账号设置密码\n```\n\n所以我们可以通过 该命令来给我们的root账号设置密码(注意：这个root账号是mysql的root账号，非Linux的root账号)\n\n```powershell\n[root@xiaoluo ~]# mysqladmin -u root password 'root'　　// 通过该命令给root账号设置密码为 root\n```\n\n此时我们可以通过 mysql -u root -p 命令来登录我们的mysql数据库了\n\n\n\n#### 四、mysql数据库的主要配置文件\n\n1./etc/my.cnf 这是mysql的主配置文件\n\n我们可以查看一下这个文件的一些信息\n\n```powershell\n[root@xiaoluo etc]# ls my.cnf \nmy.cnf\n\n[root@xiaoluo etc]# cat my.cnf \n[mysqld]\ndatadir=/var/lib/mysql\nsocket=/var/lib/mysql/mysql.sock\nuser=mysql\n# Disabling symbolic-links is recommended to prevent assorted security risks\nsymbolic-links=0\n\n[mysqld_safe]\nlog-error=/var/log/mysqld.log\npid-file=/var/run/mysqld/mysqld.pid\n```\n\n2./var/lib/mysql   mysql数据库的数据库文件存放位置\n\n我们的mysql数据库的数据库文件通常是存放在了/ver/lib/mysql这个目录下\n\n```powershell\n[root@xiaoluo ~]# cd /var/lib/mysql/\n[root@xiaoluo mysql]# ls -l\n总用量 20488\n-rw-rw----. 1 mysql mysql 10485760 4月   6 22:01 ibdata1\n-rw-rw----. 1 mysql mysql  5242880 4月   6 22:01 ib_logfile0\n-rw-rw----. 1 mysql mysql  5242880 4月   6 21:59 ib_logfile1\ndrwx------. 2 mysql mysql     4096 4月   6 21:59 mysql　　// 这两个是mysql数据库安装时默认的两个数据库文件\nsrwxrwxrwx. 1 mysql mysql        0 4月   6 22:01 mysql.sock\ndrwx------. 2 mysql mysql     4096 4月   6 21:59 test　　// 这两个是mysql数据库安装时默认的两个数据库文件\n```\n\n我们可以自己创建一个数据库，来验证一下该数据库文件的存放位置\n","tags":["Linux","Mysql"]},{"title":"centos7安装mysql5.5.48","url":"/2022/09/07/cyb-mds/database/centos7安装mysql5.5.48/","content":"\n==作者：YB-Chi==\n\n    由于centos7移掉了mysql，不能够直接用yum安装了\n### 准备软件\n    MySQL5.5-deps.zip（依赖文件 暂时未用到）\n    MySQL-client-5.5.48-1.linux2.6.x86_64.rpm\n    MySQL-server-5.5.48-1.linux2.6.x86_64.rpm\n### 安装步奏\n    1.卸载原mysql\n\n    1 #列出安装的mysql\n    2 rpm -qa | grep mysql\n    \n    1 #干掉列出的items\n    2 rpm -e [item1]\n    3 rpm -e [item2]\n    4 rpm -e [item3]\n    \n    卸掉mariadb：\n    \n    1 #列出\n    2 rpm -qa | grep mariadb\n    \n    1 #卸载\n    2 rpm -e mariadb-devel-5.5.44-2.el7.centos.x86_64\n    3 rpm -e mariadb-libs-5.5.44-2.el7.centos.x86_64\n    \n     删除相关文件夹\n    \n    find / -name mysql\n    #将列出的文件夹一一删除。\n    \n    上传三个文件到/usr/local/下\n    \n    #安装\n    rpm -ivh MySQL-server-5.5.48-1.linux2.6.x86_64.rpm\n    rpm -ivh MySQL-client-5.5.48-1.linux2.6.x86_64.rpm\n\n    #拷贝配置文件\n    cp /usr/share/mysql/my-medium.cnf /etc/my.cnf，改名为my.cnf作为mysql配置文件。\n    #修改响应的配置文件\n    vim /etc/my.cnf\n    在[mysql]后追加\n    default-character-set =utf8\n    \n    更改密码\n    /usr/bin/mysqladmin -u root password 'root'   #仅第一次有用 \n    \n    开始服务\n    service mysql start\n    \n    连接mysql\n    mysql -uroot -proot\n    \n### 更改密码\n    mysql>use mysql;\n    mysql> update user set password=passworD(\"test\") where user='root';\n    mysql> flush privileges;\n    mysql> exit; \n    \n### 删除Mysql\n\n    yum remove  mysql mysql-server mysql-libs mysql-server;\n    find / -name mysql 将找到的相关东西delete掉；\n    rpm -qa|grep mysql(查询出来的东东yum remove掉)\n\n### mysql远程连接1130Host '' is not allowed to connect to this MySQL server\n    \n    解决方法1：\n    1.登录mysql： mysql -uroot -proot\n    2.授权：GRANT ALL PRIVILEGES ON *.* TO 'myuser'@'%' IDENTIFIED BY 'mypassword' WITH GRANT OPTION;\n    3.刷新权限：FLUSH PRIVILEGES;\n    4.退出：EXIT\n    \n    解决方法2：\n    1. 登录mysql： mysql -uroot -proot\n    2. 选择mysql数据库：use mysql;\n    3. 修改访问权限：update user set host = '%' where user = 'root';\n    4. 刷新权限：FLUSH PRIVILEGES;\n    5. 退出：EXIT","tags":["Linux","Mysql"]},{"title":"Java操作Spark学习","url":"/2022/09/07/cyb-mds/bigdata/Spark/Java操作Spark学习/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n### 配置文件\n#### core-site.xml\n````xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<!-- Put site-specific property overrides in this file. -->\n<configuration>\n<property>\n\t<name>fs.defaultFS</name>\n\t<value>hdfs://ns1</value>\n</property>\n<property>\n\t<name>io.file.buffer.size</name>\n\t<value>131072</value>\n</property>\n<property>\n\t<name>hadoop.tmp.dir</name>\n\t<value>/home/yarn/tmp</value>\n</property>\n<property>\n\t<name>dfs.journalnode.edits.dir</name>\n\t<value>/home/yarn/journal</value>\n</property>\n<property>\n\t<name>hadoop.proxyuser.yarn.hosts</name>\n\t<value>EPRI-DCLOUD-HDP01</value>\n</property>\n<property>\n\t<name>hadoop.proxyuser.yarn.groups</name>\n\t<value>yarn</value>\n</property>\n</configuration>\n````\n#### hdfs-site.xml\n````xml\n    从hadoop/ect/hadoop里扒过来\n    不需要最后一行配置\n\t\n    <property>\n    <name>dfs.hosts.exclude</name>\n    <value>/home/yarn/hadoop-2.6.0/etc/hadoop/excludes</value>\n    </property>\n    \n    所需配置如下\n    <?xml version=\"1.0\"?>\n    <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n    <!-- Put site-specific property overrides in this file. -->\n    <configuration>\n    <property>\n    \t<name>dfs.nameservices</name>\n    \t<value>ns1</value>\n    </property>\n    <property>\n    \t<name>dfs.ha.namenodes.ns1</name>\n    \t<value>nn0,nn1</value>\n    </property>\n    <property>\n    \t<name>dfs.namenode.rpc-address.ns1.nn0</name>\n    \t<value>EPRI-DCLOUD-HDP01:9000</value>\n    </property>\n    <property>\n    \t<name>dfs.namenode.http-address.ns1.nn0</name>\n    \t<value>EPRI-DCLOUD-HDP01:50070</value>\n    </property>\n    <property>\n    \t<name>dfs.namenode.rpc-address.ns1.nn1</name>\n    \t<value>EPRI-DCLOUD-HDP02:9000</value>\n    </property>\n    <property>\n    \t<name>dfs.namenode.http-address.ns1.nn1</name>\n    \t<value>EPRI-DCLOUD-HDP02:50070</value>\n    </property>\n    <property>\n    \t<name>dfs.namenode.shared.edits.dir</name>\n    \t<value>qjournal://EPRI-DCLOUD-HDP01:8485;EPRI-DCLOUD-HDP02:8485;EPRI-DCLOUD-HDP03:8485/ns1</value>\n    </property>\n    <property>\n    \t<name>ha.zookeeper.quorum</name>\n    \t<value>EPRI-DCLOUD-HDP01,EPRI-DCLOUD-HDP02,EPRI-DCLOUD-HDP03</value>\n    </property>\n    <property>\n    \t<name>ha.zookeeper.session-timeout.ms</name>\n    \t<value>60000</value>\n    </property>\n    <property>\n    \t<name>dfs.ha.fencing.methods</name>\n    \t<value>sshfence</value>\n    </property>\n    <property>\n    \t<name>dfs.ha.fencing.ssh.private-key-files</name>\n    \t<value>/home/yarn/.ssh/id_rsa</value>\n    </property>\n    <property>\n    \t<name>dfs.ha.automatic-failover.enabled</name>\n    \t<value>true</value>\n    </property>\n    <property>\n    \t<name>dfs.client.failover.proxy.provider.ns1</name>\n    \t<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>\n    </property>\n    <property>\n    \t<name>dfs.replication</name>\n    \t<value>3</value>\n    </property>\n    <property>\n    \t<name>dfs.data.dir</name>\n    \t<value>/home/yarn/data</value>\n    </property>\n    <property>\n    \t<name>dfs.name.dir</name>\n    \t<value>/home/yarn/name</value>\n    </property>\n    <property>\n    \t<name>dfs.permissions</name>\n    \t<value>false</value>\n    </property>\n    \n    <property>\n      <name>dfs.qjournal.write-txns.timeout.ms</name>\n      <value>600000</value>\n    </property>\n    \n    <property>\n    \t<name>dfs.webhdfs.enabled</name>\n    \t<value>true</value>\n    </property>\n    </configuration>\n````\n\n#### hive-site.xml\n    从hive的conf扒过来\n### 所需jar文件    \n    jar包从linux上扒下来\n### 打jar方式\n\n### 代码方面\ndemo1：\n````java\n    SparkConf conf = new SparkConf().setAppName(\"hivePartion\");\n\tSparkContext sc = new SparkContext(conf);\n\tHiveContext hiveCtx = new HiveContext(sc);\n\tString sql = \"select * from fjudm4.hbase_md_load_bus_hisdata limit 1\";\n\tRow[] rows = hiveCtx.sql(sql).collect();\n\t\n\tfor (int i = 0; i < rows.length; i++) {\n\t\tRow row = rows[i];\n\t\tSystem.out.println(\"i为\" + i + \"\ti对应的row为\" + row.toString());\n\t}\n\t\tsc.stop();\n````\t\n\t#Row[] rows里装的是所有的查出来的数据  所有条   注意 它toString无法显示他的所有数据  只显示地址\n\t#row是一条数据  不是数组也不是list  格式:\n\t\n    [20161103,115967690991992834,null,福建.半兰山/220kV.1负荷,null,ss,r,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0\n    .0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2016-11-04,2016-11-03]\n\t#row.get()类似于sql里的rs.getString();\n\t\n## 开发中遇到的问题\nphoenix-4.6.0-HBase-1.1-client.jar和spark-assembly-1.5.2-hadoop2.6.0.jar会冲突thrift包\n````java\n    Exception in thread \"main\" java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:346)\n\tat org.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:171)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.liftedTree1$1(IsolatedClientLoader.scala:183)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.<init>(IsolatedClientLoader.scala:179)\n\tat org.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:226)\n\tat org.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:185)\n\tat org.apache.spark.sql.hive.HiveContext.setConf(HiveContext.scala:392)\n\tat org.apache.spark.sql.hive.HiveContext.defaultOverrides(HiveContext.scala:174)\n\tat org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:177)\n\tat com.sgcc.epri.dcloud.dm_hive_datacheck.query.impl.HiveQueryImpl.queryCount(HiveQueryImpl.java:59)\n\tat com.sgcc.epri.dcloud.dm_hive_datacheck.common.QueryAndCompare.doCheck(QueryAndCompare.java:48)\n\tat com.sgcc.epri.dcloud.dm_hive_datacheck.common.Common.check(Common.java:35)\n\tat com.sgcc.epri.dcloud.dm_hive_datacheck.main.MainCompare.main(MainCompare.java:18)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:674)\n\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)\n\tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n    Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient\n    \tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1412)\n    \tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)\n    \tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)\n    \tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2453)\n    \tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2465)\n    \tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:340)\n    \t... 25 more\n    Caused by: java.lang.reflect.InvocationTargetException\n    \tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n    \tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n    \tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n    \tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n    \tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1410)\n    \t... 30 more\n    Caused by: java.lang.NoSuchMethodError: org.apache.thrift.protocol.TProtocol.getScheme()Ljava/lang/Class;\n    \tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$set_ugi_args.write(ThriftHiveMetastore.java)\n    \tat org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:63)\n    \tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.send_set_ugi(ThriftHiveMetastore.java:3260)\n    \tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.set_ugi(ThriftHiveMetastore.java:3251)\n    \tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:352)\n    \tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:214)\n    \t... 35 more\n````\t\t\n删除了phoenix包里的scheme包解决了\n\n\n\nhiveSQL查询不出数据\n````sql\n    select * from fjudm4.HBASE_MD_MEASANALOG_BREAKER where timestamp >= to_date('2016-11-01 00:00:00.0') and timestamp < to_date('2016-11-01 00:05:00.0') limit 3;\n````\t\nsql错了，hive不可以用timestamp直接比较 应该用时间分区列date='2016-11-01'","tags":["Code","Bigdata","Java","Spark"]},{"title":"spark学习记录","url":"/2022/09/07/cyb-mds/bigdata/Spark/spark学习记录/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n### 知识点\n 用spark-submit命令提交任务运行，具体使用查看：spark-submit --help\n\n````java\n    Options:\n      --master MASTER_URL         spark://host:port, mesos://host:port, yarn, or local.\n      --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (\"client\") or\n                                  on one of the worker machines inside the cluster (\"cluster\")\n                                  (Default: client).\n      --class CLASS_NAME          Your application's main class (for Java / Scala apps).\n      --name NAME                 A name of your application.\n      --jars JARS                 Comma-separated list of local jars to include on the driver\n                                  and executor classpaths.\n      --packages                  Comma-separated list of maven coordinates of jars to include\n                                  on the driver and executor classpaths. Will search the local\n                                  maven repo, then maven central and any additional remote\n                                  repositories given by --repositories. The format for the\n                                  coordinates should be groupId:artifactId:version.\n      --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while\n                                  resolving the dependencies provided in --packages to avoid\n                                  dependency conflicts.\n      --repositories              Comma-separated list of additional remote repositories to\n                                  search for the maven coordinates given with --packages.\n      --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place\n                                  on the PYTHONPATH for Python apps.\n      --files FILES               Comma-separated list of files to be placed in the working\n                                  directory of each executor.\n    \n      --conf PROP=VALUE           Arbitrary Spark configuration property.\n      --properties-file FILE      Path to a file from which to load extra properties. If not\n                                  specified, this will look for conf/spark-defaults.conf.\n    \n      --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).\n      --driver-java-options       Extra Java options to pass to the driver.\n      --driver-library-path       Extra library path entries to pass to the driver.\n      --driver-class-path         Extra class path entries to pass to the driver. Note that\n                                  jars added with --jars are automatically included in the\n                                  classpath.\n    \n      --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).\n    \n      --proxy-user NAME           User to impersonate when submitting the application.\n    \n      --help, -h                  Show this help message and exit\n      --verbose, -v               Print additional debug output\n      --version,                  Print the version of current Spark\n    \n     Spark standalone with cluster deploy mode only:\n      --driver-cores NUM          Cores for driver (Default: 1).\n    \n     Spark standalone or Mesos with cluster deploy mode only:\n      --supervise                 If given, restarts the driver on failure.\n      --kill SUBMISSION_ID        If given, kills the driver specified.\n      --status SUBMISSION_ID      If given, requests the status of the driver specified.\n    \n     Spark standalone and Mesos only:\n      --total-executor-cores NUM  Total cores for all executors.\n    \n     Spark standalone and YARN only:\n      --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode,\n                                  or all available cores on the worker in standalone mode)\n    \n     YARN-only:\n      --driver-cores NUM          Number of cores used by the driver, only in cluster mode\n                                  (Default: 1).\n      --queue QUEUE_NAME          The YARN queue to submit to (Default: \"default\").\n      --num-executors NUM         Number of executors to launch (Default: 2).\n      --archives ARCHIVES         Comma separated list of archives to be extracted into the\n                                  working directory of each executor.\n      --principal PRINCIPAL       Principal to be used to login to KDC, while running on\n                                  secure HDFS.\n      --keytab KEYTAB             The full path to the file that contains the keytab for the\n                                  principal specified above. This keytab will be copied to\n                                  the node running the Application Master via the Secure\n                                  Distributed Cache, for renewing the login tickets and the\n                                  delegation tokens periodically.\n````\n\n### 问题记录\n Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n \n    当前的集群的可用资源不能满足应用程序所请求的资源。我spark submit的时候给的资源太大了\n\n    资源分2类： cores 和 ram\n    Core代表对执行可用的executor slots\n    Ram代表每个Worker上被需要的空闲内存来运行你的Application。\n    解决方法：\n    应用不要请求多余空闲可用资源的\n    关闭掉已经执行结束的Application\n    \n### spark-submit执行命令    \n````java\n    spark-submit --master spark://FJ-DCLOUD-HDP01:7077 --conf \"spark.scheduler.mode=FAIR\" --total-executor-cores 8 --driver-memory 20g --executor-memory 20g /home/yarn/dev/cyb/a.jar \n\n    a.jar不需要jar包   所要用的jar包放到spark服务器的lib下  例如：\n    /home/yarn/spark-1.5.2-bin-hadoop2.6/lib\n    命令会默认读取改路径下的jar\n    关于此设置的配置 见下条\n````\n### spark-submit指定服务器jar包配置  \n\n````powershell\n    [yarn@EPRI-DCLOUD-ETL01 conf]$ pwd\n    /home/yarn/spark-1.5.2-bin-hadoop2.6/conf\n    [root@EPRI-DCLOUD-ETL01 conf]# ls\n    docker.properties.template  fairscheduler.xml.template  hive-site.xml  log4j.properties  metrics.properties.template  slaves  spark-defaults.conf  spark-env.sh\n    [root@EPRI-DCLOUD-ETL01 conf]# vi spark-env.sh \n    \n    export SPARK_YARN_MODE=true\n    export MASTER=yarn-client\n    export JAVA_HOME=/usr/java/jdk1.7.0_55\n    export SCALA_HOME=/usr/share/scala-2.11.6\n    export SPARK_MASTER_IP=EPRI-DCLOUD-HDP01\n    export SPARK_CLASSPATH=$SPARK_CLASSPATH:/usr/share/spark-1.5.2-bin-hadoop2.6/lib/*:/usr/share/hadoop-2.6.0/*:/usr/share/hadoop-2.6.0/etc/*:/usr/share/hadoop-2.6.0/lib/*\n    export HADOOP_HOME=/usr/share/hadoop-2.6.0export HADOOP_CONF_DIR=/usr/share/hadoop-2.6.0/etc/hadoop\n````","tags":["Bigdata","Spark"]},{"title":"基于hadoop3.x&hive3.x的spark-2.3.2单机部署","url":"/2022/09/07/cyb-mds/bigdata/Spark/基于hadoop3.x&hive3.x的spark-2.3.2单机部署/","content":"\n\n","tags":["新建,模板,小书匠"]},{"title":"KAFKA常用操作","url":"/2022/09/07/cyb-mds/bigdata/Kafka/KAFKA常用操作/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n###### kafka消费者远程启动命令\n\n```shell\n# 修改配置\n/home/weihu/cli/kafka/bin/kafka-console-consumer.sh --bootstrap-server 10.198.16.49:9092 --from-beginning  --consumer.config /home/weihu/cli/kafka/config/consumer.properties\nnohup /data1/cli/kafka/bin/kafka-console-consumer.sh --bootstrap-server 10.198.16.49:9092 --topic alarm  --consumer.config /data1/cli/kafka/config/consumer.properties > /data1/cli/kafka/data/alarm_v2.log &\n```\n\n","tags":["BIGDATA,KAFKA,OP"]},{"title":"Kafka总结","url":"/2022/09/07/cyb-mds/bigdata/Kafka/Kafka总结/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n### 1.Kafka的特性\n\n- **高吞吐量、低延迟**：kafka每秒可以处理几十万条消息，它的延迟最低只有几毫秒\n- **可扩展性**：kafka集群支持热扩展\n- **持久性、可靠性**：消息被持久化到本地磁盘，并且支持数据备份防止数据丢失\n- **容错性**：允许集群中节点失败（若副本数量为n,则允许n-1个节点失败）\n- **高并发**：支持数千个客户端同时读写\n\n### 2.Kafka重要设计思想\n\n​\tKafka的主要设计思想\n\n- **Consumergroup**：各个consumer可以组成一个组，每个消息只能被组中的一个consumer消费，**如果一个消息可以被多个consumer消费的话，那么这些consumer必须在不同的组。**\n- **消息状态**：在Kafka中，消息的状态被保存在consumer中，broker不会关心哪个消息被消费了被谁消费了，只记录一个offset值（指向partition中下一个要被消费的消息位置），这就意味着如果consumer处理不好的话，broker上的一个消息可能会被消费多次。\n- **消息持久化**：Kafka中会把消息持久化到本地文件系统中，并且保持极高的效率。\n- **消息有效期**：Kafka会长久保留其中的消息，以便consumer可以多次消费，当然其中很多细节是可配置的。\n- **批量发送**：Kafka支持以消息集合为单位进行批量发送，以提高push效率。\n- **push-and-pull** : Kafka中的Producer和consumer采用的是push-and-pull模式，即Producer只管向broker push消息，consumer只管从broker pull消息，两者对消息的生产和消费是异步的。\n- **Kafka集群中broker之间的关系**：**不是主从关系，各个broker在集群中地位一样，我们可以随意的增加或删除任何一个broker节点。**\n- **负载均衡方面**： Kafka提供了一个 metadata API来管理broker之间的负载（对Kafka0.8.x而言，对于0.7.x主要靠zookeeper来实现负载均衡）。\n- **同步异步**：Producer采用异步push方式，极大提高Kafka系统的吞吐率（可以通过参数控制是采用同步还是异步方式）。\n- **分区机制partition**：Kafka的broker端支持消息分区，Producer可以决定把消息发到哪个分区，在一个分区中消息的顺序就是Producer发送消息的顺序，一个主题中可以有多个分区，具体分区的数量是可配置的。分区的意义很重大，后面的内容会逐渐体现。\n- **离线数据装载**：Kafka由于对可拓展的数据持久化的支持，它也非常适合向Hadoop或者数据仓库中进行数据装载。\n- **插件支持**：现在不少活跃的社区已经开发出不少插件来拓展Kafka的功能，如用来配合Storm、Hadoop、flume相关的插件。\n\n### 3.Kafka架构组件\n\nKafka中发布订阅的对象是topic。我们可以为每类数据创建一个topic，把向topic发布消息的客户端称作producer，从topic订阅消息的客户端称作consumer。Producers和consumers可以同时从多个topic读写数据。一个kafka集群由一个或多个broker服务器组成，它负责持久化和备份具体的kafka消息。\n\n- topic：消息存放的目录即主题\n- Producer：生产消息到topic的一方\n- Consumer：订阅topic消费消息的一方    \n- Broker：Kafka的服务实例就是一个broker\n\n![img](https://raw.githubusercontent.com/chiyuanbo/pic/master/producer_consumer.png)\n\n### 4.Kafka Topic&Partition\n\n在Kafka中，消息是按Topic组织的.\n\n1. **Partition**:topic物理上的分组，一个topic可以分为多个partition，每个partition是一个有序的队列。\n2. **Segment**：partition物理上由多个segment组成\n3. **offse**t：每个partition都由一系列有序的、不可变的消息组成，这些消息被连续的追加到partition中。partition中的每个消息都有一个连续的序列号叫做offset,用于partition唯一标识一条消息.\n\n消息发送时都被发送到一个topic，其本质就是一个目录，而topic由是由一些Partition Logs(分区日志)组成,其组织结构如下图所示：\n\n![img](https://raw.githubusercontent.com/chiyuanbo/pic/master/log_anatomy.png)\n\n- ​\t每个Partition中的消息都是有序的，生产的消息被不断追加到Partition log上，其中的每一个消息都被赋予了一个唯一的offset值。 \n\n\n- ​\tKafka集群会保存所有的消息，不管消息有没有被消费；我们可以设定消息的过期时间，只有过期的数据才会被自动清除以释放磁盘空间。比如我们设置消息过期时间为2天，那么这2天内的所有消息都会被保存到集群中，数据只有超过了两天才会被清除。 \n\n\n- ​\tKafka需要维持的元数据只有一个–消费消息在Partition中的offset值，Consumer每消费一个消息，offset就会加1。其实消息的状态完全是由Consumer控制的，Consumer可以跟踪和重设这个offset值，这样的话Consumer就可以读取任意位置的消息。\n\n\n- ​\t把消息日志以Partition的形式存放有多重考虑，第一，方便在集群中扩展，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据了；第二就是可以提高并发，因为可以以Partition为单位读写了。\n\n### 5.Kafka 核心组件\n\n#### 5.1 Replications、Partitions 和Leaders\n\n- ​\tkafka中的数据是持久化的并且能够容错的。Kafka允许用户为每个topic设置副本数量，副本数量决定了有几个broker来存放写入的数据。如果副本数量设置为3，那么一份数据就会被存放在3台不同的机器上，那么就允许有2个机器失败。一般推荐副本数量至少为2，这样就可以保证增减、重启机器时不会影响到数据消费。如果对数据持久化有更高的要求，可以把副本数量设置为3或者更多。\n\n\n- ​\tKafka中的topic是以partition的形式存放的，每一个topic都可以设置它的partition数量，Partition的数量决定了组成topic的log的数量。Producer在生产数据时，会按照一定规则（这个规则是可以自定义的）把消息发布到topic的各个partition中。上面将的副本都是以partition为单位的，不过只有一个partition的副本会被选举成leader作为读写用。\n\n\n- ​\t关于如何设置partition值需要考虑的因素。一个partition只能被一个消费者消费（一个消费者可以同时消费多个partition），因此，如果设置的partition的数量小于consumer的数量，就会有消费者消费不到数据。所以，推荐partition的数量一定要大于同时运行的consumer的数量。另外一方面，建议partition的数量大于集群broker的数量，这样leader partition就可以均匀的分布在各个broker中，最终使得集群负载均衡。在Cloudera,每个topic都有上百个partition。需要注意的是，kafka需要为每个partition分配一些内存来缓存消息数据，如果partition数量越大，就要为kafka分配更大的heap space。\n\n#### 5.2 Producers\n\n- ​\tProducers直接发送消息到broker上的leader partition，不需要经过任何中介一系列的路由转发。为了实现这个特性，kafka集群中的每个broker都可以响应producer的请求，并返回topic的一些元信息，这些元信息包括哪些机器是存活的，topic的leader partition都在哪，现阶段哪些leader partition是可以直接被访问的。 \n  - ​Producer客户端自己控制着消息被推送到哪些partition。实现的方式可以是随机分配、实现一类随机负载均衡算法，或者指定一些分区算法。Kafka提供了接口供用户实现自定义的分区，用户可以为每个消息指定一个partitionKey，通过这个key来实现一些hash分区算法。比如，把userid作为partitionkey的话，相同userid的消息将会被推送到同一个分区。\n  - ​以Batch的方式推送数据可以极大的提高处理效率，kafka Producer 可以将消息在内存中累计到一定数量后作为一个batch发送请求。Batch的数量大小可以通过Producer的参数控制，参数值可以设置为累计的消息的数量（如500条）、累计的时间间隔（如100ms）或者累计的数据大小(64KB)。通过增加batch的大小，可以减少网络请求和磁盘IO的次数，当然具体参数设置需要在效率和时效性方面做一个权衡。\n  - ​Producers可以异步的并行的向kafka发送消息，但是通常producer在发送完消息之后会得到一个future响应，返回的是offset值或者发送过程中遇到的错误。这其中有个非常重要的参数“acks”,这个参数决定了producer要求leader partition 收到确认的副本个数，如果acks设置数量为0，表示producer不会等待broker的响应，所以，producer无法知道消息是否发送成功，这样有可能会导致数据丢失，但同时，acks值为0会得到最大的系统吞吐量。\n  - ​若acks设置为1，表示producer会在leader partition收到消息时得到broker的一个确认，这样会有更好的可靠性，因为客户端会等待直到broker确认收到消息。若设置为-1，producer会在所有备份的partition收到消息时得到broker的确认，这个设置可以得到最高的可靠性保证。\n  - ​Kafka 消息有一个定长的header和变长的字节数组组成。因为kafka消息支持字节数组，也就使得kafka可以支持任何用户自定义的序列号格式或者其它已有的格式如Apache\n- Avro、protobuf等。Kafka没有限定单个消息的大小，但我们推荐消息大小不要超过1MB,通常一般消息大小都在1~10kB之前。\n\n#### 5.3 Consumers\n\n- ​\tKafka提供了两套consumer api，分为high-level api和sample-api。Sample-api 是一个底层的API，它维持了一个和单一broker的连接，并且这个API是完全无状态的，每次请求都需要指定offset值，因此，这套API也是最灵活的。\n  - ​在kafka中，当前读到消息的offset值是由consumer来维护的，因此，consumer可以自己决定如何读取kafka中的数据。比如，consumer可以通过重设offset值来重新消费已消费过的数据。不管有没有被消费，kafka会保存数据一段时间，这个时间周期是可配置的，只有到了过期时间，kafka才会删除这些数据。\n- ​        High-level API封装了对集群中一系列broker的访问，可以透明的消费一个topic。它自己维持了已消费消息的状态，即每次消费的都是下一个消息。 \n  - ​High-level API还支持以组的形式消费topic，如果consumers有同一个组名，那么kafka就相当于一个队列消息服务，而各个consumer均衡的消费相应partition中的数据。若consumers有不同的组名，那么此时kafka就相当与一个广播服务，会把topic中的所有消息广播到每个consumer。\n\n![img](https://raw.githubusercontent.com/chiyuanbo/pic/master/consumer-groups.png)\n\n### 6.Kafka核心特性\n\n#### 6.1 压缩\n\n​\t我们上面已经知道了Kafka支持以集合（batch）为单位发送消息，在此基础上，Kafka还支持对消息集合进行压缩，Producer端可以通过GZIP或Snappy格式对消息集合进行压缩。Producer端进行压缩之后，在Consumer端需进行解压。压缩的好处就是减少传输的数据量，减轻对网络传输的压力，在对大数据处理上，瓶颈往往体现在网络上而不是CPU（压缩和解压会耗掉部分CPU资源）。\n​\t那么如何区分消息是压缩的还是未压缩的呢，Kafka在消息头部添加了一个描述压缩属性字节，这个字节的后两位表示消息的压缩采用的编码，如果后两位为0，则表示消息未被压缩。\n\n#### 6.2消息可靠性\n\n​\t在消息系统中，保证消息在生产和消费过程中的可靠性是十分重要的，在实际消息传递过程中，可能会出现如下三中情况：\n\n- 一个消息发送失败\n- 一个消息被发送多次\n- 最理想的情况：exactly-once ,一个消息发送成功且仅发送了一次                   \n\n\n\n- ​\t有许多系统声称它们实现了exactly-once，但是它们其实忽略了生产者或消费者在生产和消费过程中有可能失败的情况。比如虽然一个Producer成功发送一个消息，但是消息在发送途中丢失，或者成功发送到broker，也被consumer成功取走，但是这个consumer在处理取过来的消息时失败了。\n  - ​从Producer端看：Kafka是这么处理的，当一个消息被发送后，Producer会等待broker成功接收到消息的反馈（可通过参数控制等待时间），如果消息在途中丢失或是其中一个broker挂掉，Producer会重新发送（我们知道Kafka有备份机制，可以通过参数控制是否等待所有备份节点都收到消息）。\n  - ​从Consumer端看：前面讲到过partition，broker端记录了partition中的一个offset值，这个值指Consumer下一个即将消费message。当Consumer收到了消息，但却在处理过程中挂掉，此时Consumer可以通过这个offset值重新找到上一个消息再进行处理。Consumer还有权限控制这个offset值，对持久化到broker端的消息做任意处理。\n\n#### 6.3 备份机制\n\n​\t备份机制是Kafka0.8版本的新特性，备份机制的出现大大提高了Kafka集群的可靠性、稳定性。有了备份机制后，Kafka允许集群中的节点挂掉后而不影响整个集群工作。一个备份数量为n的集群允许n-1个节点失败。在所有备份节点中，有一个节点作为lead节点，这个节点保存了其它备份节点列表，并维持各个备份间的状体同步。下面这幅图解释了Kafka的备份机制:\n\n![这里写图片描述](https://raw.githubusercontent.com/chiyuanbo/pic/master/20150828162159461)\n\n#### 6.4 Kafka高效性相关设计\n\n##### 6.4.1 消息的持久化\n\n- ​\tKafka高度依赖文件系统来存储和缓存消息，一般的人认为磁盘是缓慢的，这导致人们对持久化结构具有竞争性持怀疑态度。其实，磁盘远比你想象的要快或者慢，这决定于我们如何使用磁盘。 \n  - ​一个和磁盘性能有关的关键事实是：磁盘驱动器的吞吐量跟寻到延迟是相背离的，也就是所，线性写的速度远远大于随机写。比如：在一个6 7200rpm SATA RAID-5 的磁盘阵列上线性写的速度大概是600M/秒，但是随机写的速度只有100K/秒，两者相差将近6000倍。线性读写在大多数应用场景下是可以预测的，因此，操作系统利用read-ahead和write-behind技术来从大的数据块中预取数据，或者将多个逻辑上的写操作组合成一个大写物理写操作中。更多的讨论可以在ACMQueueArtical中找到，他们发现，对磁盘的线性读在有些情况下可以比内存的随机访问要快一些。 \n  - ​为了补偿这个性能上的分歧，现代操作系统都会把空闲的内存用作磁盘缓存，尽管在内存回收的时候会有一点性能上的代价。所有的磁盘读写操作会在这个统一的缓存上进行。 \n\n此外，如果我们是在JVM的基础上构建的，熟悉Java内存应用管理的人应该清楚以下两件事情：\n\n1. 一个对象的内存消耗是非常高的，经常是所存数据的两倍或者更多。\n2. 随着堆内数据的增多，Java的垃圾回收会变得非常昂贵。\n\n\n\n- ​\t基于这些事实，利用文件系统并且依靠页缓存比维护一个内存缓存或者其他结构要好——我们至少要使得可用的缓存加倍，通过自动访问可用内存，并且通过存储更紧凑的字节结构而不是一个对象，这将有可能再次加倍。这么做的结果就是在一台32GB的机器上，如果不考虑GC惩罚，将最多有28-30GB的缓存。此外，这些缓存将会一直存在即使服务重启，然而进程内缓存需要在内存中重构（10GB缓存需要花费10分钟）或者它需要一个完全冷缓存启动（非常差的初始化性能）。它同时也简化了代码，因为现在所有的维护缓存和文件系统之间内聚的逻辑都在操作系统内部了，这使得这样做比one-off in-process attempts更加高效与准确。如果你的磁盘应用更加倾向于顺序读取，那么read-ahead在每次磁盘读取中实际上获取到这人缓存中的有用数据。 \n  - ​以上这些建议了一个简单的设计：不同于维护尽可能多的内存缓存并且在需要的时候刷新到文件系统中，我们换一种思路。所有的数据不需要调用刷新程序，而是立刻将它写到一个持久化的日志中。事实上，这仅仅意味着，数据将被传输到内核页缓存中并稍后被刷新。我们可以增加一个配置项以让系统的用户来控制数据在什么时候被刷新到物理硬盘上。\n\n##### 6.4.2 常数时间性能保证\n\n- ​\t消息系统中持久化数据结构的设计通常是维护者一个和消费队列有关的B树或者其它能够随机存取结构的元数据信息。B树是一个很好的结构，可以用在事务型与非事务型的语义中。但是它需要一个很高的花费，尽管B树的操作需要O(logN)。通常情况下，这被认为与常数时间等价，但这对磁盘操作来说是不对的。磁盘寻道一次需要10ms，并且一次只能寻一个，因此并行化是受限的。\n  - ​直觉上来讲，一个持久化的队列可以构建在对一个文件的读和追加上，就像一般情况下的日志解决方案。尽管和B树相比，这种结构不能支持丰富的语义，但是它有一个优点，所有的操作都是常数时间，并且读写之间不会相互阻塞。这种设计具有极大的性能优势：最终系统性能和数据大小完全无关，服务器可以充分利用廉价的硬盘来提供高效的消息服务。\n  - ​事实上还有一点，磁盘空间的无限增大而不影响性能这点，意味着我们可以提供一般消息系统无法提供的特性。比如说，消息被消费后不是立马被删除，我们可以将这些消息保留一段相对比较长的时间（比如一个星期）。\n\n##### 6.4.3 进一步提高效率\n\n​\t我们已经为效率做了非常多的努力。但是有一种非常主要的应用场景是：处理Web活动数据，它的特点是数据量非常大，每一次的网页浏览都会产生大量的写操作。更进一步，我们假设每一个被发布的消息都会被至少一个consumer消费，因此我们更要怒路让消费变得更廉价。 \n通过上面的介绍，我们已经解决了磁盘方面的效率问题，除此之外，在此类系统中还有两类比较低效的场景：\n\n- 太多小的I/O操作\n- 过多的字节拷贝\n\n为了减少大量小I/O操作的问题，kafka的协议是围绕消息集合构建的。Producer一次网络请求可以发送一个消息集合，而不是每一次只发一条消息。在server端是以消息块的形式追加消息到log中的，consumer在查询的时候也是一次查询大量的线性数据块。消息集合即MessageSet，实现本身是一个非常简单的API，它将一个字节数组或者文件进行打包。所以对消息的处理，这里没有分开的序列化和反序列化的上步骤，消息的字段可以按需反序列化（如果没有需要，可以不用反序列化）。\n另一个影响效率的问题就是字节拷贝。为了解决字节拷贝的问题，kafka设计了一种“标准字节消息”，Producer、Broker、Consumer共享这一种消息格式。Kakfa的message\n log在broker端就是一些目录文件，这些日志文件都是MessageSet按照这种“标准字节消息”格式写入到磁盘的。 \n维持这种通用的格式对这些操作的优化尤为重要：持久化log 块的网络传输。流行的unix操作系统提供了一种非常高效的途径来实现页面缓存和socket之间的数据传递。在Linux操作系统中，这种方式被称作：sendfile system call（Java提供了访问这个系统调用的方法：FileChannel.transferTo api）。\n\n为了理解sendfile的影响，需要理解一般的将数据从文件传到socket的路径：\n\n1. 操作系统将数据从磁盘读到内核空间的页缓存中\n2. 应用将数据从内核空间读到用户空间的缓存中\n3. 应用将数据写回内核空间的socket缓存中\n4. 操作系统将数据从socket缓存写到网卡缓存中，以便将数据经网络发出\n\n这种操作方式明显是非常低效的，这里有四次拷贝，两次系统调用。如果使用sendfile，就可以避免两次拷贝：操作系统将数据直接从页缓存发送到网络上。所以在这个优化的路径中，只有最后一步将数据拷贝到网卡缓存中是需要的。 \n我们期望一个主题上有多个消费者是一种常见的应用场景。利用上述的zero-copy，数据只被拷贝到页缓存一次，然后就可以在每次消费时被重得利用，而不需要将数据存在内存中，然后在每次读的时候拷贝到内核空间中。这使得消息消费速度可以达到网络连接的速度。这样以来，通过页面缓存和sendfile的结合使用，整个kafka集群几乎都已以缓存的方式提供服务，而且即使下游的consumer很多，也不会对整个集群服务造成压力。\n\n### 7.SHELL使用\n\n```powershell\n#列出所有topic\nbin/kafka-topics.sh --zookeeper  192.168.12.102:2181,192.168.12.103:2181,192.168.12.104:2181 --list\n\n#查看某topic的信息\nbin/kafka-topics.sh --zookeeper  192.168.12.102:2181,192.168.12.103:2181,192.168.12.104:2181 --describe --topic test1\n\n#查看某个数据日志的信息\nbin/kafka-run-class.sh kafka.tools.DumpLogSegments --files /var/local/kafka/data/test-0/00000000000000000000.log  --print-data-log\n\n#创建某topic\nbin/kafka-topics.sh --zookeeper  192.168.12.102:2181,192.168.12.103:2181,192.168.12.104:2181 --create --topic cyb --partitions 1 --replication-factor 1\n\n#使用alter configs命令更改或设置重写。这个例子更新了my主题的最大消息大小:\nbin/kafka-configs.sh --zookeeper 192.168.12.102:2181,192.168.12.103:2181,192.168.12.104:2181 --entity-type topics --entity-name test1 --alter --add-config max.message.bytes=128000\n\nbin/kafka-configs.sh --zookeeper 192.168.12.102:2181,192.168.12.103:2181,192.168.12.104:2181  --entity-type topics --entity-name test1 --alter --delete-config max.message.bytes\n\n#为Topic增加 partition数目kafka-add-partitions.sh\nbin/kafka-add-partitions.sh --topic singertest --partition 2  --zookeeper  192.168.12.102:2181,192.168.12.103:2181,192.168.12.104:2181 （为topic singertest增加2个分区）\n\n#删除topic,慎用，只会删除zookeeper中的元数据，消息文件须手动删除  位置在配置里 \nbin/kafka-topics.sh --delete --topic cyb --zookeeper 192.168.12.102:2181,192.168.12.103:2181,192.168.12.104:2181\n\n!通过配置config下server.properties在最下边加入一行\ndelete.topic.enable=true\n这样删除topic时可以连带消息文件一起删除\n\n#往某topic生产消息\nbin/kafka-console-producer.sh --broker-list BDS-CM:9092,BDS-DATA1:9092,BDS-DATA2:9092 --topic test1\n\n#从某topic消费消息\nbin/kafka-console-consumer.sh --zookeeper 192.168.12.102:2181,192.168.12.103:2181,192.168.12.104:2181 --topic test1 --from-beginning\n\n#登录zkClient\ncd /opt/cloudera/parcels/CDH/lib/zookeeper/bin\n./zkCli.sh -server BDS-CM:2181,BDS-DATA1:2181,BDS-DATA2:2181\n\n#给用户yuanbo分配读写test1这个topic的权限   默认是禁止的    默认不限制ip\nbin/kafka-acls.sh --authorizer-properties zookeeper.connect=192.168.12.102:2181,192.168.12.103:2181,192.168.12.104:2181 --add --allow-principal User:yuanbo --operation Read --operation Write --topic test1\n\n#给俩用户授予从ip1和ip2两台机器上读写test1这个topic的权限\nbin/kafka-acls.sh --authorizer-properties zookeeper.connect=192.168.12.102:2181,192.168.12.103:2181,192.168.12.104:2181 --add --allow-principal User:yuanbo --allow-principal User:jinyu --allow-host 192.168.12.102 --allow-host 192.168.12.103 --allow-host 192.168.12.104 --operation Read --operation Write --topic test1\n\n#查看某个topic的全部acls信息\nbin/kafka-acls.sh --authorizer-properties zookeeper.connect=192.168.12.102:2181,192.168.12.103:2181,192.168.12.104:2181 --list --topic test2\n\n#sasl消费\nbin/kafka-console-consumer-sasl.sh  --bootstrap-server 192.168.56.212:9092 --topic test1 --consumer.config config/consumer.properties \n#sasl生产\nbin/kafka-console-producer-sasl.sh  --broker-list 192.168.56.212:9092 --topic test1 --producer.config config/producer.properties \n```\n### 8.Kafka  Leader选举\n\n```powershell\nAs with most distributed systems automatically handling failures requires having a precise definition of what it means for a node to be \"alive\". For Kafka node liveness has two conditions\n和大多数分布式系统一样，自动处理失败的节点。需要精确的定义什么样的节点是“活着”的，对于kafka的节点活着有2个条件：\n1.A node must be able to maintain its session with ZooKeeper (via ZooKeeper's heartbeat mechanism)\n一个节点必须能维持与zookeeper的会话（通过zookeeper的心跳机制）\n2.If it is a slave it must replicate the writes happening on the leader and not fall \"too far\" behind\n如果它是一个slave，它必须复制leader并且不能落后\"太多\"\n```","tags":["Bigdata","Kafka"]},{"title":"Hive自定义UDF函数","url":"/2022/09/07/cyb-mds/bigdata/Hive/Hive自定义UDF函数/","content":"\n==作者：YB-Chi==\n\n1. idea新建个maven项目，引入依赖\n\n```xml\n\t<dependencies>\n        <dependency>\n            <groupId>org.apache.hive</groupId>\n            <artifactId>hive-exec</artifactId>\n            <version>2.1.1</version>\n            <exclusions>\n                <exclusion>\n                    <groupId>*</groupId>\n                    <artifactId>*</artifactId>\n                </exclusion>\n            </exclusions>\n        </dependency>\n    </dependencies>\n```\n\n2. 编写函数类，继承UDF，并实现evaluate方法\n\n```java\nimport org.apache.hadoop.hive.ql.exec.UDF;\npublic class split_distinct extends UDF {\n\n    public String evaluate(String line, String split) {\n        if (null == line) return null;\n        StringBuffer sb = new StringBuffer();\n        String[] sp = line.split(split);\n        for (String s : sp) {\n            if (null != s && sb.indexOf(s) < 0) sb.append(s).append(\",\");\n        }\n        return sb.deleteCharAt(sb.length() - 1).toString();\n    }\n}\n```\n\n3. maven-package打包，上传不带依赖的包并测试\n\n```shell\nhive>add jar /home/module/testdata/HiveUDF-1.0-SNAPSHOT.jar;\nCREATE TEMPORARY FUNCTION split_distinct AS 'split_distinct';\n#测试\nselect split_distinct('[[\"C8:13:8B:81:32:C1\"],[\"C8:13:8B:81:32:C1\"],[\"C8:13:8B:81:32:C1\"],[\"C8:13:8B:81:32:C2\"]]',',');\n```\n\n4. 测试成功后，注册永久函数\n\n```\n#建立hdfs文件夹\nhdfs dfs -mkdir /out/hive_udf_jars\n#上传\nhdfs dfs -put HiveUDF-1.0-SNAPSHOT.jar /out/hive_udf_jars/\n#注册\ncreate function split_distinct as 'split_distinct' using jar 'hdfs://xbsafe102/out/hive_udf_jars/HiveUDF-1.0-SNAPSHOT.jar';\n#删除函数\ndrop function split_distinct;\n#注意更新jar包需要重启hive或者spark-sql客户端！！！！！\n```\n\n","tags":["Code","Hadoop","Hive","Java"]},{"title":"hive开发过程中遇到的一些问题","url":"/2022/09/07/cyb-mds/bigdata/Hive/hive开发过程中遇到的一些问题/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n### 1、连接hive的时候发现连接不上，后来发现是hdfs有损坏文件，\n\n\nhadoop fsck的一些命令\n\n    Hadoop fsck [-move | -delete | -openforwrite] [-files [-blocks [-locations | -racks]]]\n    \n    <path>        检查的起始目录\n    \n    -move        将损坏的文件移动到/lost+found下面\n    \n    -delete        删除损坏的文件\n    \n    -openforwrite    打印出正在写的文件\n    \n    -files        打印出所有被检查的文件\n    \n    -blocks        打印出block报告\n    \n    -locations    打印出每个block的位置\n    \n    -racks        打印出datanode的网络拓扑结构\n    \n    \n==hadoop fsck -list-corruptfileblocks命令查看==\n\n\n\nhadoop fsck -delete命令清理\n\n\n### hiveserver2启动:\n1.启动 metastort\n    \n    nohup  hive --service metastore >/dev/null  2>&1 &\n\t\n2.进入 /home/yarn/apache-hive-1.2.1-bin/bin 下启动\n\n    startserver2.sh +10009\n\t\t\t\n\tbeeline -u jdbc:hive2://192.168.6.1:10009 -n yarn -p yarn\t#检测\n3.进入 /home/yarn/spark-1.5.2-bin-hadoop2.6/sbin 下启动spark的\n\t\n\tstart-server2.sh +14000","tags":["Code","Hadoop","Hive","Java"]},{"title":"hive常用操作","url":"/2022/09/07/cyb-mds/bigdata/Hive/hive常用操作/","content":"\n[toc]\n\n###### 建分区表\n\n```sql\n CREATE EXTERNAL TABLE `ods_o_mdt_huawei_log_mdt_h_fenqu`(         \n   `recloggrpttimestamp` string,                    \n   `measstarabsolutetimestamp` string\n   )\n PARTITIONED BY (    \n   `dayid` string,   \n   `hourid` string)  \n ROW FORMAT SERDE    \n   'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'  \n WITH SERDEPROPERTIES (                             \n   'field.delim'=',',\n   'serialization.format'=',')                      \n STORED AS INPUTFORMAT                              \n   'org.apache.hadoop.mapred.TextInputFormat'       \n OUTPUTFORMAT        \n   'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' \n LOCATION            \n   'hdfs://192.168.1.34:9000/apps/hive/warehouse/hiveods.db/ods_o_mdt_huawei_log_mdt_h' \n TBLPROPERTIES (     \n   'bucketing_version'='2',                         \n   'last_modified_by'='ica',                        \n   'last_modified_time'='1607600876',               \n   'transient_lastDdlTime'='1607600876')\n```\n\n","tags":["DATABASE,HIVE,OP"]},{"title":"hive3.1 op&dev","url":"/2022/09/07/cyb-mds/bigdata/Hive/hive3.1 op&dev/","content":"\n版本：\nhadoop-3.3.0.tar.gz\napache-hive-3.1.2-bin.tar.gz\nzookeeper-3.4.9.tar.gz\nspark-2.3.2-bin-without-hadoop.gz\n\n##### 配置\n\n环境变量\n```shell\nexport MYSQL_HOME=/usr/local/mysql\nexport PATH=$PATH:$MYSQL_HOME/bin\nexport JAVA_HOME=/usr/java/jdk1.8.0_131\nexport JRE_HOME=$JAVA_HOME/jre\nexport CLASSPATH=$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATH\nexport PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin\nexport HADOOP_HOME=/home/software/hadoop-3.3.0\nexport PATH=$PATH:$HADOOP_HOME/bin\nexport PATH=$PATH:$HADOOP_HOME/sbin\nexport HIVE_HOME=/home/software/apache-hive-3.1.2-bin\nexport PATH=$PATH:$HIVE_HOME/bin\n```\n\ncp hive-site.xml模板\n```xml\n<configuration>\n  <property>\n    <name>javax.jdo.option.ConnectionURL</name>\n    <value>jdbc:mysql://sdw34:3306/hive?createDatabaseIfNotExist=true&amp;useSSL=false</value>\n  </property>\n  <property>\n    <name>javax.jdo.option.ConnectionDriverName</name>\n    <value>com.mysql.jdbc.Driver</value>\n  </property>\n  <property>\n    <name>javax.jdo.option.ConnectionUserName</name>\n    <value>dev</value>\n  </property>\n  <property>\n    <name>javax.jdo.option.ConnectionPassword</name>\n    <value>dev</value>\n  </property>\n<property>\n\t<name>hive.cluster.delegation.token.store.zookeeper.connectString</name>\n    <value>sdw34</value>\n</property>\n<property>\n\t<name>hive.zookeeper.quorum</name>\n    <value>sdw34:2181</value>\n</property>\n<property>\n          <name>hive.metastore.uris</name>\n          <value>thrift://192.168.1.34:9083</value>\n   </property>\n<property>\n    <name>hive.execution.engine</name>\n    <value>mr</value>\n</property>\n<property>\n    <name>hive.metastore.event.db.notification.api.auth</name>\n    <value>false</value>\n</property>\n\n <property>\n    <name>hive.server2.authentication</name>\n    <value>NONE</value>\n  </property>\n\n<property>\n    <name>hive.server2.thrift.client.user</name>\n    <value>root</value>\n    <description>Username to use against thrift client</description>\n  </property>\n  <property>\n    <name>hive.server2.thrift.client.password</name>\n    <value>root</value>\n    <description>Password to use against thrift client</description>\n  </property>\n<property>\n <name>hive.server2.enable.doAs</name>\n <value>true</value>\n</property>\n<property>\n      <name>hive.server2.allow.user.substitution</name>\n      <value>true</value>\n</property>\n<property>\n    <name>datanucleus.schema.autoCreateAll</name>\n    <value>true</value>\n </property>\n<property>\n    <name>hive.metastore.schema.verification</name>\n    <value>false</value>\n    <description>\n      Enforce metastore schema version consistency.\n      True: Verify that version information stored in is compatible with one from Hive jars.  Also disable automatic\n            schema migration attempt. Users are required to manually migrate schema after Hive upgrade which ensures\n            proper metastore schema migration. (Default)\n      False: Warn if the version information stored in metastore doesn't match with one from in Hive jars.\n    </description>\n  </property>\n<!--<property>\n     <name>hive.server2.thrift.bind.host</name>\n     <value>sdw34</value>\n  </property>-->\n</configuration>\n```\ncp hive-log4j2.properties模板\n```YAML\n# 改个路径\nproperty.hive.log.dir = /home/software/apache-hive-3.1.2-bin/logs\n```\ncp hive-env.sh模板\n```sh\n# 加上环境变量\nexport HADOOP_HOME=/home/software/hadoop-3.3.0\nexport JAVA_HOME=/usr/java/jdk1.8.0_131\n```\nmysql-connector的jar包放到lib下\n将hadoop/share/下的guava-27移到hive/lib下并删除hive的低版本guava\ncp core-site.xml文件到hive/conf下\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<!--\n  Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n-->\n\n<!-- Put site-specific property overrides in this file. -->\n\n<configuration>\n <property>\n  <name>fs.defaultFS</name>\n <!-- xwzx.com为当前机器名或者ip号 -->\n  <value>hdfs://sdw34:9000</value>\n </property>\n <property>\n  <name>hadoop.tmp.dir</name>\n    <!-- 以下为存放临时文件的路径 -->\n  <value>/home/software/hadoop-3.3.0/data/tmp</value>\n </property>\n <property>\n    <name>hadoop.proxyuser.dev.hosts</name>\n    <value>*</value>\n </property>\n <property>\n    <name>hadoop.proxyuser.dev.groups</name>\n    <value>*</value>\n </property>\n<property>\n    <name>hadoop.proxyuser.root.hosts</name>\n    <value>*</value>\n </property>\n <property>\n    <name>hadoop.proxyuser.root.groups</name>\n    <value>*</value>\n </property>\n<!--<property>\n    <name>hadoop.proxyuser.hive.hosts</name>\n    <value>*</value>\n </property>\n <property>\n    <name>hadoop.proxyuser.hive.groups</name>\n    <value>*</value>\n </property>-->\n<property>\n    <name>hadoop.proxyuser.hadoop.hosts</name>\n    <value>*</value>\n</property>\n<property>\n    <name>hadoop.proxyuser.hadoop.groups</name>\n    <value>*</value>\n</property>\n<property>    \n  <name>io.compression.codecs</name>    \n  <value>    \n  org.apache.hadoop.io.compress.DefaultCodec,    \n  org.apache.hadoop.io.compress.GzipCodec,    \n  org.apache.hadoop.io.compress.BZip2Codec,     \n  org.apache.hadoop.io.compress.Lz4Codec,    \n  org.apache.hadoop.io.compress.SnappyCodec    \n  </value>    \n  <description>A comma-separated list of the compression codec classes that can    \n  be used for compression/decompression. In addition to any classes specified    \n  with this property (which take precedence), codec classes on the classpath    \n  are discovered using a Java ServiceLoader.</description>    \n</property>\n</configuration>\n```\n\n##### 启动\n```shell\n# 初始化\nschematool -dbType mysql -initSchema\n# 启动metastore\nhive --service metastore &\n# 启动hiveserver2 hiveserver2的服务端口默认是10000，WebUI端口默认是10002\nhive --service hiveserver2 &\n# 或者 cd hiveserver2 &\n```\n\n##### hql\n\n```sql\n# 删除hive内部表，不会删除数据\n# 将内部表改成外部表\nalter table table_name set TBLPROPERTIES('EXTERNAL'='TRUE');\n\n# 改名\nALTER TABLE name RENAME TO new_name\n```\n\n##### 文档\nhttps://www.docs4dev.com/docs/zh/apache-hive/3.1.1/reference/Configuration_Properties.html\n","tags":["hive"]},{"title":"centos7安装hive1.2.1","url":"/2022/09/07/cyb-mds/bigdata/Hive/centos7安装hive1.2.1/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n### 准备软件\n    apache-hive-1.2.1-bin.tar.gz\n### 安装步奏\n    将文件解压在/usr/local/下并改名hive_1.2.1\n    \n    #配置系统环境变量/etc/profile\n\n    # Hive Environment Variables\n    export HIVE_HOME=/usr/local/hive_1.2.1\n    export PATH=$PATH:$HIVE_HOME/bin:$HIVE_HOME/conf\n\n    source /etc/profile 使刚刚的配置生效\n    \n    #配置Hive\n\n    hive的配置文件放在$HIVE_HOME/conf下，里面有4个默认的配置文件模板\n\n    hive-default.xml.template                           默认模板\n    \n    hive-env.sh.template                hive-env.sh默认配置\n    \n    hive-exec-log4j.properties.template    exec默认配置\n    \n    hive-log4j.properties.template               log默认配置\n    \n    可不做任何修改hive也能运行，默认的配置元数据是存放在Derby数据库里面的，\n    大多数人都不怎么熟悉，我们得改用mysql来存储我们的元数据，\n    以及修改数据存放位置和日志存放位置等使得我们必须配置自己的环境，下面介绍如何配置。\n    \n（1）创建配置文件，直接copy默认配置文件再修改即可，用户自定义配置会覆盖默认配置\n\n    cp $HIVE_HOME/conf/hive-default.xml.template $HIVE_HOME/conf/hive-site.xml\n    cp $HIVE_HOME/conf/hive-env.sh.template $HIVE_HOME/conf/hive-env.sh\n    cp $HIVE_HOME/conf/hive-exec-log4j.properties.template $HIVE_HOME/conf/hive-exec-log4j.properties\n    cp $HIVE_HOME/conf/hive-log4j.properties.template $HIVE_HOME/conf/hive-log4j.properties\n\n（2）修改 hive-env.sh \n\n    vi $HIVE_HOME/conf/hive-env.sh \n    \n    export HADOOP_HOME=/usr/local/hadoop\n    export HIVE_CONF_DIR=/usr/local/hive_1.2.1/conf\n\n（3）修改 hive-log4j.properties\n\n    mkdir $HIVE_HOME/logs\n    vi $HIVE_HOME/conf/hive-log4j.properties\n    \n    hive.log.dir=/usr/local/hive_1.2.1/logs\n（4）修改 hive-site.xml\n    \n    vi $HIVE_HOME/conf/hive-site.xml\n    \n    <property>\n    <name>hive.metastore.warehouse.dir</name>\n    <value>/usr/local/hive_1.2.1/warehouse</value>\n    <description>location of default database for the warehouse</description>\n    </property>\n    \n    <property>\n    <name>hive.exec.scratchdir</name>\n    <value>/usr/local/hive_1.2.1/scratchdir</value>\n    <description>HDFS root scratch dir for Hive jobs which gets created with write all (733) permission. For each connecting user, an HDFS scratch dir: ${hive.exec.scratchdir}/&lt;username&gt; is created, with ${hive.scratch.dir.permission}.</description>\n    </property>\n    \n    <property>\n    <name>hive.querylog.location</name>\n    <value>/usr/local/hive_1.2.1/logs</value>\n    <description>Location of Hive run time structured log file</description>\n    </property>\n    \n    <property>\n    <name>javax.jdo.option.ConnectionURL</name>\n    <value>jjdbc:mysql://192.168.15.131:3306/hivedb?createDatabaseIfNotExist=true</value>\n    <description>JDBC connect string for a JDBC metastore</description>\n    </property>\n    \n    <property>\n    <name>javax.jdo.option.ConnectionDriverName</name>\n    <value>com.mysql.jdbc.Driver</value>\n    <description>Driver class name for a JDBC metastore</description>\n    </property>\n    \n    <property>\n    <name>javax.jdo.option.ConnectionUserName</name>\n    <value>root</value>\n    <description>Username to use against metastore database</description>\n    </property>\n    \n    <property>\n    <name>javax.jdo.option.ConnectionPassword</name>\n    <value>root</value>\n    <description>password to use against metastore database</description>\n    </property>\n    \n","tags":["Bigdata","Hive"]},{"title":"centos7安装hbase1.1.4","url":"/2022/09/07/cyb-mds/bigdata/Hbase/centos7安装hbase1.1.4/","content":"\n==作者：YB-Chi==\n\n[TOC]\n\n### 准备软件\n    hbase-1.1.4-bin.tar.gz\n### 安装步奏\n    #解压\n    tar zxvf hbase-1.1.4-bin.tar.gz\n\n    #修改环境变量   \n    vi /etc/profile\n    \n    # Hbase Environment Variables\n    export HBASE_HOME=/usr/local/hbase-1.1.4\n    export PATH=$HBASE_HOME/bin:$PATH\n    source /etc/profile生效。\n\n    #修改hbase-env.sh，添加：\n    vi conf/hbase-env.sh\n    \n    export JAVA_HOME=/usr/local/jdk_1.7\n\n    #修改配置文件\n    vi conf/hbase-site.xml\n    \n    <configuration>\n        <property>\n                <name>hbase.rootdir</name>\n                <value>hdfs://192.168.15.131:9000/hbase</value>\n        </property>\n        <property>\n                <name>hbase.cluster.distributed</name>\n                <value>true</value>\n        </property>\n    </configuration>","tags":["Bigdata","Hbase"]},{"title":"hbase常用操作","url":"/2022/09/07/cyb-mds/bigdata/Hbase/hbase常用操作/","content":"\n==作者：YB-Chi==\n\n\n\n###### 修改hbase表为snappy压缩表\n\n```sql\nalter 'geo_cells',{NAME => 'cell_parameter', COMPRESSION => 'SNAPPY'}\n\nalter 'geo_cells_current',{NAME => 'cell_parameter', COMPRESSION => 'SNAPPY'}\n\nalter 'v2.mdt.grid.date_gh_cgi',{NAME => 'attr', COMPRESSION => 'SNAPPY'},{NAME => 'ftp', COMPRESSION => 'SNAPPY'},{NAME => 'http', COMPRESSION => 'SNAPPY'},{NAME => 'imm', COMPRESSION => 'SNAPPY'},{NAME => 'log', COMPRESSION => 'SNAPPY'},{NAME => 'rcef', COMPRESSION => 'SNAPPY'},{NAME => 'rlf', COMPRESSION => 'SNAPPY'},{NAME => 's1mme', COMPRESSION => 'SNAPPY'},{NAME => 'volte', COMPRESSION => 'SNAPPY'}\n\nalter 'v2.mdt.grid.date_cgi_gh',{NAME => 'attr', COMPRESSION => 'SNAPPY'},{NAME => 'ftp', COMPRESSION => 'SNAPPY'},{NAME => 'http', COMPRESSION => 'SNAPPY'},{NAME => 'imm', COMPRESSION => 'SNAPPY'},{NAME => 'log', COMPRESSION => 'SNAPPY'},{NAME => 'rcef', COMPRESSION => 'SNAPPY'},{NAME => 'rlf', COMPRESSION => 'SNAPPY'},{NAME => 's1mme', COMPRESSION => 'SNAPPY'},{NAME => 'volte', COMPRESSION => 'SNAPPY'}\n\n```\n\n###### hbase建snappy表sql\n\n```sql\ncreate 'geo_cells_current',{NAME => 'cell_parameter', COMPRESSION => 'SNAPPY'}\n\ncreate 'v2.mdt.grid.date_gh_cgi',{NAME => 'attr', COMPRESSION => 'SNAPPY'},{NAME => 'ftp', COMPRESSION => 'SNAPPY'},{NAME => 'http', COMPRESSION => 'SNAPPY'},{NAME => 'imm', COMPRESSION => 'SNAPPY'},{NAME => 'log', COMPRESSION => 'SNAPPY'},{NAME => 'rcef', COMPRESSION => 'SNAPPY'},{NAME => 'rlf', COMPRESSION => 'SNAPPY'},{NAME => 's1mme', COMPRESSION => 'SNAPPY'},{NAME => 'volte', COMPRESSION => 'SNAPPY'}\n\ncreate 'v2.mdt.grid.date_cgi_gh',{NAME => 'attr', COMPRESSION => 'SNAPPY'},{NAME => 'ftp', COMPRESSION => 'SNAPPY'},{NAME => 'http', COMPRESSION => 'SNAPPY'},{NAME => 'imm', COMPRESSION => 'SNAPPY'},{NAME => 'log', COMPRESSION => 'SNAPPY'},{NAME => 'rcef', COMPRESSION => 'SNAPPY'},{NAME => 'rlf', COMPRESSION => 'SNAPPY'},{NAME => 's1mme', COMPRESSION => 'SNAPPY'},{NAME => 'volte', COMPRESSION => 'SNAPPY'}\n\n```\n\n","tags":["Bigdata,Hbase"]},{"title":"centos7安装hbase2.10.0","url":"/2022/09/07/cyb-mds/bigdata/Hbase/centos7安装hbase2.1.0（未修改）/","content":"\n==作者：YB-Chi==\n\nHBase简介\n　　HBase是Apache Hadoop中的一个子项目，是一个HBase是一个开源的、分布式的、多版本的、面向列的、非关系(NoSQL)的、可伸缩性分布式数据存储模型，Hbase依托于Hadoop的HDFS作为最基本存储基础单元。HBase的服务器体系结构遵从简单的主从服务器架构，它由HRegion Server群和HMaster Server构成。HMaster Server负责管理所有的HRegion Server，而HBase中的所有Server都是通过Zookeeper进行的分布式信息共享与任务协调的工作。HMaster Server本身并不存储HBase中的任何数据，HBase逻辑上的表可能会被划分成多个Region，然后存储到HRegionServer群中，HRegionServer响应用户I/O请求，向HDFS文件系统中读写数据。HBase Master Server中存储的是从数据到HRegion Server的映射。\n\n　　下面一幅图是Hbase在Hadoop Ecosystem中的位置\n\n![enter description here][1]\n\n\n　　上图描述了Hadoop EcoSystem中的各层系统，其中HBase位于结构化存储层，Hadoop HDFS为HBase提供了高可靠性的底层存储支持，Hadoop MapReduce为HBase提供了高性能的计算能力，Zookeeper为HBase提供了稳定服务和failover机制。 此外，Pig和Hive还为HBase提供了高层语言支持，使得在HBase上进行数据统计处理变的非常简单。 Sqoop则为HBase提供了方便的RDBMS数据导入功能，使得传统数据库数据向HBase中迁移变的非常方便。\n\n1，系统环境配置\n安装hadoop\n安装zookeeper \n2，下载与安装:\nHbase 版本必需 与 Hadoop 版本匹配，否则会安装失败或不能正常使用。关于两者何种版本能正常匹配，可以看官方文档查看 hbase 官方文档（http://hbase.apache.org/book.html#basic.prerequisites），找到与 hadoop 版本对应的 hbase 并下载（http://archive.apache.org/dist/hbase/）\n使用tar解压hbase\n```powershell\ncd /usr/local\ntar -zxvf hbase-1.2.1-bin.tar.gz\nmv /home/hbase\n```\n使用vi /etc/profile设置环境变量\n\n\n3，系统参数配置\n配置工作具体如下：\n\n使用 vi /home/hbase/conf/hbase-env.sh 修改系统环境\n```powershell\nexport JAVA_HOME=/usr/local/jdk1.8\nexport HBASE_PID_DIR=/home/hbase/pid #使用mkdir /home/hbase/pid命令先创建\nexport HBASE_MANAGES_ZK=false #不适用内置zookeeper,使用我们自己安装的（具体指定\n```\n使用哪个zookeeper是通过/etc/profile中的ZK_HOME变量来指定的）\nvi conf/hbase-site.xml 配置系统参数\n```xml\n<configuration>\n  <property>\n    <name>hbase.rootdir</name>\n    <value>hdfs://master:9000/hbase</value>\n    <description>设置 hbase 数据库存放数据的目录,这里是放在hadoop hdfs上,这里要与hadoop的core-site.xml文件中的fs.default.name中的值一致,然后在后面添加自己的子目录，我这里定义是hbase</description>\n  </property>\n  <property>\n    <name>hbase.cluster.distributed</name>\n    <value>true</value>\n    <description>打开 hbase 分布模式</description>\n  </property>\n  <property>\n    <name>hbase.master</name>\n    <value>master</value>\n    <description>指定 hbase 集群主控节点</description>\n  </property>\n  <property>\n    <name>hbase.tmp.dir</name>\n    <value>/home/user/tmp/hbase</value>\n    <description>hbase的一些临时文件存放目录。</description>\n   </property>\n  <property>\n    <name>hbase.zookeeper.quorum</name>\n    <value>master,slave1,slave2</value>\n    <description> 指定 zookeeper 集群节点名 , 因为是由 zookeeper 表决算法决定的</description>\n  </property>\n  <property>\n    <name>hbase.zookeeper.property.clientPort</name>\n    <value>2181</value>\n   <description> 连接到zookeeper的端口，默认是2181</description>\n  </property>\n</configuration>\n```\n\nvi  conf/regionservers 该文件指定了HRegionServer进程将在哪些节点上运行\n```powershell\nmsater\nslave1\nslave2\n```\n　　如果有多个master，需要执行 vi conf/backup-masters，加入备份master节点，这里可以参考:http://blog.sina.com.cn/s/blog_474edf960101aetr.html\n向其他节点传递安装，使用下列命令\n```powershell\nscp /home/hbase root@slave1:/home/\nscp /home/hbase root@slave2:/home/\n```\n完成后使用vi /etc/profile 设置各自节点的环境变量\n\n4，启动hbase服务\n启动hbase前要确保，hadoop,zookeeper已经启动，进入$HBASE_HOME/bin目录下，输入命令start-hbase.sh\n\n执行jps查看系统进程\n\n其他节点\n\n启动日志会输出到/home/hbase/logs/hbase-root-master-master.log中，可以查看排除异常\n\n5，测试\n启动完成后，执行如下命令可以进入到hbase shell界面，使用命令status检查集群节点状态\n\n这里可以使用 hbase shell命令执行数据库操作，具体参考 http://www.cnblogs.com/nexiyi/p/hbase_shell.html \n\n另外也可以直接打开网址：http://192.168.137.122:16010/master-status，在web中查看集群状态，其中192.168.137.122是master所在节点的IP，16010为hbase默认端口（老版本中为60010）\n\n6，错误\n本次安装测试中主要出现了一下几个错误：\n\n各节点节点时间不一致\n```\norg.apache.hadoop.hbase.ClockOutOfSyncException: org.apache.hadoop.hbase.ClockOutOfSyncException: Server hadoopslave2,60020,1372320861420 has been rejected; Reported time is too far out of sync with master.  Time difference of 143732ms > max allowed of 30000ms\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:525)\n        at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:95)\n        at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:79)\n        at org.apache.hadoop.hbase.regionserver.HRegionServer.reportForDuty(HRegionServer.java:2093)\n        at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:744)\n        at java.lang.Thread.run(Thread.java:722)\nCaused by: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hbase.ClockOutOfSyncException: Server hadoopslave2,60020,1372320861420 has been rejected; Reported time is too far out of sync with master.  Time difference of 143732ms > max allowed of 30000ms\n在各节点的hbase-site.xml文件中加入下列代码\n\n   <property>\n     <name>hbase.master.maxclockskew</name>\n     <value>200000</value>\n   </property>\nDirectory is not empty\n\norg.apache.hadoop.ipc.RemoteException(org.apache.hadoop.fs.PathIsNotEmptyDirectoryException): `/hbase/WALs/slave1,16000,1446046595488-splitting is non empty': Directory is not empty\n    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:3524)\n    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInt(FSNamesystem.java:3479)\n    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:3463)\n    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:751)\n    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:562)\n    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\n    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\n    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)\n    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:415)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)\n    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)\n \n    at org.apache.hadoop.ipc.Client.call(Client.java:1411)\n    at org.apache.hadoop.ipc.Client.call(Client.java:1364)\n    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)\n    at com.sun.proxy.$Proxy15.delete(Unknown Source)\n    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.delete(ClientNamenodeProtocolTranslatorPB.java:490)\n    at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:606)\n    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n    at com.sun.proxy.$Proxy16.delete(Unknown Source)\n    at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:606)\n    at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)\n    at com.sun.proxy.$Proxy17.delete(Unknown Source)\n    at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:606)\n    at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)\n    at com.sun.proxy.$Proxy17.delete(Unknown Source)\n    at org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:1726)\n    at org.apache.hadoop.hdfs.DistributedFileSystem$11.doCall(DistributedFileSystem.java:588)\n    at org.apache.hadoop.hdfs.DistributedFileSystem$11.doCall(DistributedFileSystem.java:584)\n    at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n    at org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:584)\n    at org.apache.hadoop.hbase.master.SplitLogManager.splitLogDistributed(SplitLogManager.java:297)\n    at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:400)\n    at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:373)\n    at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:295)\n    at org.apache.hadoop.hbase.master.procedure.ServerCrashProcedure.splitLogs(ServerCrashProcedure.java:388)\n    at org.apache.hadoop.hbase.master.procedure.ServerCrashProcedure.executeFromState(ServerCrashProcedure.java:228)\n    at org.apache.hadoop.hbase.master.procedure.ServerCrashProcedure.executeFromState(ServerCrashProcedure.java:72)\n    at org.apache.hadoop.hbase.procedure2.StateMachineProcedure.execute(StateMachineProcedure.java:119)\n    at org.apache.hadoop.hbase.procedure2.Procedure.doExecute(Procedure.java:452)\n    at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.execProcedure(ProcedureExecutor.java:1050)\n    at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.execLoop(ProcedureExecutor.java:841)\n    at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.execLoop(ProcedureExecutor.java:794)\n    at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.access$400(ProcedureExecutor.java:75)\n    at org.apache.hadoop.hbase.procedure2.ProcedureExecutor$2.run(ProcedureExecutor.java:479)\n```\n参考https://issues.apache.org/jira/browse/HBASE-14729，进入hadoop文件系统，删除掉报错的目录或真个WALs\n\n\nTableExistsException: hbase:namespace\n```java\nzookeeper.MetaTableLocator: Failed verification of hbase:meta,,1 at address=slave1,16020,1428456823337, exception=org.apache.hadoop.hbase.NotServingRegionException: Region hbase:meta,,1 is not online on worker05,16020,1428461295266\n        at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionByEncodedName(HRegionServer.Java:2740)\n        at org.apache.hadoop.hbase.regionserver.RSRpcServices.getRegion(RSRpcServices.java:859)\n        at org.apache.hadoop.hbase.regionserver.RSRpcServices.getRegionInfo(RSRpcServices.java:1137)\n        at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$2.callBlockingMethod(AdminProtos.java:20862)\n        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2031)\n        at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:107)\n        at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:130)\n        at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:107)\n        at java.lang.Thread.run(Thread.java:745)\n```\nHMaster启动之后自动挂掉（或非正常重启），并且master的log里出现“TableExistsException: hbase:namespace”字样;\n很可能是更换了Hbase的版本过后zookeeper还保留着上一次的Hbase设置，所以造成了冲突.\n删除zookeeper信息，重启之后就没问题了\n\n```powershell\n# sh zkCli.sh -server slave1:2181\n[zk: slave1:2181(CONNECTED) 0] ls /\n[zk: slave1:2181(CONNECTED) 0] rmr /hbase\n[zk: slave1:2181(CONNECTED) 0] quit\n```","tags":["Bigdata","Hbase"]},{"title":"cnetos7安装hadoop2.6.4","url":"/2022/09/07/cyb-mds/bigdata/Hadoop/cnetos7安装hadoop2.6.4/","content":"\n==作者：YB-Chi==\n\n[TOC]\n\n### 准备软件\n\tjdk-7u80-linux-x64.tar.gz,hadoop-2.6.4.tar.gz\n### 安装过程\n#### 安装jdk\n配置jdk要先卸载centos自带的openjdk\n先查看 rpm -qa | grep java\n显示如下信息：\n\n\t[root@localhost /]# rpm -qa | grep java\n\tjava-1.8.0-openjdk-headless-1.8.0.65-3.b17.el7.x86_64\n\tjavapackages-tools-3.4.1-11.el7.noarch\n\tjava-1.8.0-openjdk-1.8.0.65-3.b17.el7.x86_64\n\ttzdata-java-2015g-1.el7.noarch\n\tjava-1.7.0-openjdk-1.7.0.91-2.6.2.3.el7.x86_64\n\tjava-1.7.0-openjdk-headless-1.7.0.91-2.6.2.3.el7.x86_64\n\tpython-javapackages-3.4.1-11.el7.noarch\n卸载\n\n\trpm -e --nodeps java-1.8.0-openjdk-headless-1.8.0.65-3.b17.el7.x86_64\n\trpm -e --nodeps java-1.8.0-openjdk-1.8.0.65-3.b17.el7.x86_64\n\trpm -e --nodeps java-1.7.0-openjdk-1.7.0.91-2.6.2.3.el7.x86_64\n\trpm -e --nodeps java-1.7.0-openjdk-headless-1.7.0.91-2.6.2.3.el7.x86_64\n解压jdk并配置环境变量\n\n\tvim /etc/profile\n\t在最后添加\n\tJAVA_HOME=/usr/local/jdk_1.7\n\tPATH=$PATH:$JAVA_HOME/bin:\n\tCLASSPATH=.:$JAVA_HOME/lib\n\texport JAVA_HOME  PATH CLASSPATH\n#### 安装SSH、配置SSH无密码登陆\n\n\t执行如下命令进行检验：\n\t[spark@localhost ~]$ rpm -qa | grep ssh\n\topenssh-6.6.1p1-22.el7.x86_64\n\tlibssh2-1.4.3-10.el7.x86_64\n\topenssh-server-6.6.1p1-22.el7.x86_64\n\topenssh-clients-6.6.1p1-22.el7.x86_64\n\t此时是已经安装了\n 若需要安装，则可以通过 yum 进行安装（安装过程中会让你输入 [y/N]，输入 y 即可）：\n\n    sudo yum install openssh-clients\n    sudo yum install openssh-server\n接着执行如下命令测试一下 SSH 是否可用：\n\n\t[spark@localhost ~]$ ssh localhost\n\tspark@localhost's password: \n\tLast login: Tue Feb  7 23:34:34 2017 from 192.168.15.1\n\t-bash: : No such file or directory\n但这样登陆是需要每次输入密码的，我们需要配置成SSH无密码登陆比较方便。\n\n首先输入 exit 退出刚才的 ssh，就回到了我们原先的终端窗口，然后利用 ssh-keygen 生成密钥，并将密钥加入到授权中：\n\n\t[spark@localhost ~]$ exit# 退出刚才的 ssh localhost\n\tlogout\n\tConnection to localhost closed.\n\t\n\t[spark@localhost ~]$ cd ~/.ssh/# 若没有该目录，请先执行一次ssh localhost\n\t[spark@localhost .ssh]$ ls\n\tknown_hosts\n\t\n\t[spark@localhost .ssh]$ ssh-keygen -t rsa# 会有提示，都按回车就可以\n\tGenerating public/private rsa key pair.\n\tEnter file in which to save the key (/home/spark/.ssh/id_rsa):    \n\tEnter passphrase (empty for no passphrase): \n\tEnter same passphrase again: \n\tYour identification has been saved in /home/spark/.ssh/id_rsa.\n\tYour public key has been saved in /home/spark/.ssh/id_rsa.pub.\n\tThe key fingerprint is:\n\t23:b2:30:77:8a:33:d8:93:84:c4:3a:57:4b:3e:bb:d4 spark@localhost.localdomain\n\tThe key's randomart image is:\n\t+--[ RSA 2048]----+\n\t|                 |\n\t|.                |\n\t| o  o            |\n\t|o. + .           |\n\t|+ = * o S        |\n\t| * * O . .       |\n\t|. B = E          |\n\t|   = .           |\n\t|    .            |\n\t+-----------------+\n\t\n\t[spark@localhost .ssh]$ cat id_rsa.pub >> authorized_keys# 加入授权\n\t[spark@localhost .ssh]$ chmod 600 ./authorized_keys# 修改文件权限\n\t在 Linux 系统中，~ 代表的是用户的主文件夹，即 “/home/用户名” 这个目录，如你的用户名为 hadoop，则 ~ 就代表 “/home/hadoop/”\n\t[spark@localhost .ssh]$ pwd\n\t/home/spark/.ssh\n此时再用 ssh localhost 命令，无需输入密码就可以直接登陆了\n\n\t[spark@localhost .ssh]$ ssh localhost\n\tLast login: Tue Feb  7 23:35:12 2017 from localhost\n\t-bash: : No such file or directory\n\t[spark@localhost ~]$ exit\n\tlogout\n\tConnection to localhost closed.\t\n#### 安装Hadoop\nHadoop 解压后即可使用。==如果没有解压权限，给压缩包的上一级文件夹变为用户的spark的==输入如下命令来检查 Hadoop 是否可用，成功则会显示 Hadoop 版本信息：\n​\t\n\t更改文件夹名方便配置文件\n\tmv hadoop-2.6.4 hadoop\n\t\n\tcd /usr/local/hadoop\n\t./bin/hadoop version\n设置 HADOOP 环境变量\n\n\tvim /etc/profile\n\t# Hadoop Environment Variables\n\texport HADOOP_HOME=/usr/local/hadoop\n\texport HADOOP_INSTALL=$HADOOP_HOME\n\texport HADOOP_MAPRED_HOME=$HADOOP_HOME\n\texport HADOOP_COMMON_HOME=$HADOOP_HOME\n\texport HADOOP_HDFS_HOME=$HADOOP_HOME\n\texport YARN_HOME=$HADOOP_HOME\n\texport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native\n\texport PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin\t\n执行如下命令使配置生效\n​\t\n\tcource /etc/profile\t\n修改配置文件 etc/hadoop\n​\t\n    core-site.xml\n\n <configuration>\n        <property>\n            <name>hadoop.tmp.dir</name>\n            <value>file:/usr/local/hadoop/tmp</value>\n            <description>Abase for other temporary directories.</description>\n        </property>\n        <property>\n            <name>fs.defaultFS</name>\n            <value>hdfs://localhost:9000</value>\n        </property>\n    </configuration>\n修改配置文件 hdfs-site.xml\n​\t\n\t    <configuration>\n\t    <property>\n\t        <name>dfs.replication</name>\n\t        <value>1</value>\n\t    </property>\n\t    <property>\n\t        <name>dfs.namenode.name.dir</name>\n\t        <value>file:/usr/local/hadoop/tmp/dfs/name</value>\n\t    </property>\n\t    <property>\n\t        <name>dfs.datanode.data.dir</name>\n\t        <value>file:/usr/local/hadoop/tmp/dfs/data</value>\n\t    </property>\n\t</configuration>\n\n配置完成后，执行 NameNode 的格式化:\n\n\t./bin/hdfs namenode -format\n\t 成功的话，会看到 “successfully formatted” 和 “Exitting with status 0” 的提示，若为 “Exitting with status 1” 则是出错。\n\n==接着开启 NaneNode 和 DataNode 守护进程：==\n\n    ./sbin/start-dfs.sh\n若出现如下 SSH 的提示 “Are you sure you want to continue connecting”，输入 yes 即可。\nlocalhost: Error: JAVA_HOME is not set and could not be found.\n​\t\n\t  修改/etc/hadoop/hadoop-env.sh中设JAVA_HOME\n\t  export JAVA_HOME=/usr/local/jdk_1.7\n启动完成后，可以通过命令 jps 来判断是否成功启动，若成功启动则会列出如下进程: “NameNode”、”DataNode”和SecondaryNameNode（如果 SecondaryNameNode 没有启动，请运行 sbin/stop-dfs.sh 关闭进程，然后再次尝试启动尝试）。如果没有 NameNode 或 DataNode ，那就是配置不成功，请仔细检查之前步骤，或通过查看启动日志排查原因。\n\n\n==通过查看启动日志分析启动失败原因==\n\n有时 Hadoop 无法正确启动，如 NameNode 进程没有顺利启动，这时可以查看启动日志来排查原因，注意几点：\n\n    启动时会提示形如 “dblab: starting namenode, logging to /usr/local/hadoop/logs/hadoop-hadoop-namenode-dblab.out”，其中 dblab 对应你的主机名，但启动的日志信息是记录在 /usr/local/hadoop/logs/hadoop-hadoop-namenode-dblab.log 中，所以应该查看这个后缀为 .log 的文件；\n    每一次的启动日志都是追加在日志文件之后，所以得拉到最后面看，看下记录的时间就知道了。\n    一般出错的提示在最后面，也就是写着 Fatal、Error 或者 Java Exception 的地方。\n    可以在网上搜索一下出错信息，看能否找到一些相关的解决方法。\n\n上面的单机模式，grep 例子读取的是本地数据，伪分布式读取的则是 HDFS 上的数据。要使用 HDFS，首先需要在 HDFS 中创建用户目录：\t\n​\t\n\t    ./bin/hdfs dfs -mkdir -p /user/spark\n接着将 ./etc/hadoop 中的 xml 文件作为输入文件复制到分布式文件系统中，即将 /usr/local/hadoop/etc/hadoop 复制到分布式文件系统中的 /user/hadoop/input 中。我们使用的是 spark 用户，并且已创建相应的用户目录 /user/spark ，因此在命令中就可以使用相对路径如 input，其对应的绝对路径就是 /user/spark/input:\n​\t\n\t./bin/hdfs dfs -mkdir input\n\t./bin/hdfs dfs -put ./etc/hadoop/*.xml input\n复制完成后，可以通过如下命令查看 HDFS 中的文件列表：\n\n\t ./bin/hdfs dfs -ls input\n[图片丢失]\n\n伪分布式运行 MapReduce 作业的方式跟单机模式相同，区别在于伪分布式读取的是HDFS中的文件（可以将单机步骤中创建的本地 input 文件夹，输出结果 output 文件夹都删掉来验证这一点）。\n\n\t    ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar grep input output 'dfs[a-z.]+'\n查看运行结果的命令（查看的是位于 HDFS 中的输出结果）：\n​\t    \n\t\t./bin/hdfs dfs -cat output/*\n结果如下，注意到刚才我们已经更改了配置文件，所以运行结果不同。\n\n[图片丢失]\n\n我们也可以将运行结果取回到本地：\n\n\t\trm -r ./output    # 先删除本地的 output 文件夹（如果存在）\n\t\t./bin/hdfs dfs -get output ./output     # 将 HDFS 上的 output 文件夹拷贝到本机\n\t\tcat ./output/*\nHadoop 运行程序时，输出目录不能存在，否则会提示错误 “org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://localhost:9000/user/hadoop/output already exists” ，因此若要再次执行，需要执行如下命令删除 output 文件夹:\t\t\n​\t\n\t    ./bin/hdfs dfs -rm -r output    # 删除 output 文件夹\n运行程序时，输出目录不能存在\n\n运行 Hadoop 程序时，为了防止覆盖结果，程序指定的输出目录（如 output）不能存在，否则会提示错误，因此运行前需要先删除输出目录。在实际开发应用程序时，可考虑在程序中加上如下代码，能在每次运行时自动删除输出目录，避免繁琐的命令行操作：\n\n    Configuration conf = new Configuration();\n    Job job = new Job(conf);\n     \n    /* 删除输出目录 */\n    Path outputPath = new Path(args[1]);\n    outputPath.getFileSystem(conf).delete(outputPath, true);\t\t\n==若要关闭 Hadoop，则运行==\n\n\t./sbin/stop-dfs.sh\n#### 启动YARN\t\n有的读者可能会疑惑，怎么启动 Hadoop 后，见不到书上所说的 JobTracker 和 TaskTracker，这是因为新版的 Hadoop 使用了新的 MapReduce 框架（MapReduce V2，也称为 YARN，Yet Another Resource Negotiator）。\n\nYARN 是从 MapReduce 中分离出来的，负责资源管理与任务调度。YARN 运行于 MapReduce 之上，提供了高可用性、高扩展性，YARN 的更多介绍在此不展开，有兴趣的可查阅相关资料。\n\n上述通过 ./sbin/start-dfs.sh 启动 Hadoop，仅仅是启动了 MapReduce 环境，我们可以启动 YARN ，让 YARN 来负责资源管理与任务调度。\n\n首先修改配置文件 mapred-site.xml，这边需要先进行重命名：\n\n    mv ./etc/hadoop/mapred-site.xml.template ./etc/hadoop/mapred-site.xml\n\nShell 命令\n\n然后再进行编辑，vim ./etc/hadoop/mapred-site.xml ：\n\n    <configuration>\n        <property>\n            <name>mapreduce.framework.name</name>\n            <value>yarn</value>\n        </property>\n    </configuration>\n\nXML\n\n接着修改配置文件 yarn-site.xml：\n\n\tvim ./etc/hadoop/yarn-site.xml \n\n    <configuration>\n        <property>\n            <name>yarn.nodemanager.aux-services</name>\n            <value>mapreduce_shuffle</value>\n         </property>\n    </configuration>\n ==然后就可以启动 YARN 了（需要先执行过 ./sbin/start-dfs.sh）：==\n\n    ./sbin/start-yarn.sh      $ 启动YARN\n    ./sbin/mr-jobhistory-daemon.sh start historyserver  # 开启历史服务器，才能在Web中查看任务运行情况\n\n\n启动 YARN 之后，运行实例的方法还是一样的，仅仅是资源管理方式、任务调度不同。观察日志信息可以发现，不启用 YARN 时，是 “mapred.LocalJobRunner” 在跑任务，启用 YARN 之后，是 “mapred.YARNRunner” 在跑任务。启动 YARN 有个好处是可以通过 Web 界面查看任务的运行情况：http://192.168.15.131:8088/cluster\n\n\n\n但 YARN 主要是为集群提供更好的资源管理与任务调度，然而这在单机上体现不出价值，反而会使程序跑得稍慢些。因此在单机上是否开启 YARN 就看实际情况了。\n== 不启动 YARN 需重命名 mapred-site.xml==\n\n\t如果不想启动 YARN，务必把配置文件 mapred-site.xml 重命名，改成 mapred-site.xml.template，需要用时改回来就行。否则在该配置文件存在，而未开启 YARN 的情况下，运行程序会提示 “Retrying connect to server: 0.0.0.0/0.0.0.0:8032” 的错误，这也是为何该配置文件初始文件名为 mapred-site.xml.template。\n==同样的，关闭 YARN 的脚本如下：==\n\n    ./sbin/stop-yarn.sh\n    ./sbin/mr-jobhistory-daemon.sh stop historyserver\n#### 配置变量\t\n在前面我们设置 HADOOP 环境变量时，我们已经顺便设置了 PATH 变量（即 “export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin”），那么以后我们在任意目录中都可以直接通过执行start-dfs.sh 来启动 Hadoop 或者执行 hdfs dfs -ls input 查看 HDFS 文件了，执行 hdfs dfs -ls input 试试看。","tags":["Bigdata","Hadoop"]},{"title":"Elasticsearch集群搭建","url":"/2022/09/07/cyb-mds/bigdata/ES/Elasticsearch集群搭建/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n\n### 搭建集群model\n\n准备一台虚机  \n    \n    centos6.7\n    免密码登陆\n    yum源\n    静态IP\n    /etc/hosts配置\n    /etc/sysconfig/network配置\n    jdk&环境变量\n\n解压elasticsearch到/usr/local下 \n\n配置elasticsearch.yml(注意要顶格写，冒号后面要加一个空格)\n    \n    a)  Cluster.name: elasticsearch_chi  (同一集群要一样)\n    b)  Node.name： node-1  (同一集群要不一样)\n    c)  Network.Host: 192.168.15.11\n    d)  discovery.zen.ping.multicast.enabled: false\n        discovery.zen.ping.timeout: 120s\n        client.transport.ping.timeout: 60s\n        discovery.zen.ping.unicast.hosts: [\"192.168.15.11\",\"192.168.15.12\", \"192.168.15.13\"]\n\n开始clone两台虚机  clone过后改ip方法见我笔记    \n### head插件\n    \n#### 插件安装方法1：\n    1.elasticsearch/bin/plugin -install mobz/elasticsearch-head\n    2.运行es\n    3.打开http://localhost:9200/_plugin/head/\n#### 插件安装方法2：\n    1.https://github.com/mobz/elasticsearch-head下载zip \n        解到压elasticsearch-1.0.0\\plugins\\  并重命名为head\n    2.运行es\n    3.打开http://localhost:9200/_plugin/head/","tags":["Bigdata","ES"]},{"title":"windows安装elasticsearch","url":"/2022/09/07/cyb-mds/bigdata/ES/windows安装elasticsearch/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n### 使用版本  \n公司版本\n\n    cupid-elasticsearch-1.7.2.zip\n### 更改配置文件  \n\n#### elasticsearch.bat\n    \n    #注释前两行错误配置\n    ::for /f %%a in (\"%~dp0\\..\") do set JAVA_HOME_=%%~dpajava\n    ::if exist \"%JAVA_HOME_%\\bin\\java.exe\" (set JAVA_HOME=%JAVA_HOME_%) else (echo %JAVA_HOME_%指向的路径不存在，使用本地的JAVA_HOME变量)\n    \n    set JAVA_HOME=C:\\Program Files\\Java\\jdk1.8.0_111\n    \n#### plugin.bat\n\n    同elasticsearch.bat\n    \n#### service.bat\n\n    elasticsearch.bat\n    \n#### elasticsearch.yml\n\n    只记录了需要更改的\n    cluster.name: cupid-es-chi\n    node.max_local_storage_nodes: 1\n    index.number_of_replicas: 1\n    gateway.recover_after_nodes: 1\n    gateway.expected_nodes: 1\n#### 完整版配置文件解释[不一定全配置]\n\n##### 配置文件详解1.0版\n\n    配置文件位于es根目录的config目录下面，有elasticsearch.yml和logging.yml两个配置，主配置文件是elasticsearch.yml，日志配置文件是logging.yml，elasticsearch调用log4j记录日志，所以日志的配置文件可以按照默认的设置，我来介绍下elasticsearch.yml里面的选项。\n    \n    cluster.name: elasticsearch\n    配置的集群名称，默认是elasticsearch，es服务会通过广播方式自动连接在同一网段下的es服务，通过多播方式进行通信，同一网段下可以有多个集群，通过集群名称这个属性来区分不同的集群。\n    \n     \n    \n    node.name: \"Franz Kafka\"\n    当前配置所在机器的节点名，你不设置就默认随机指定一个name列表中名字，该name列表在es的jar包中config文件夹里name.txt文件中，其中有很多作者添加的有趣名字。\n    \n     \n    \n    node.master: true\n    指定该节点是否有资格被选举成为node（注意这里只是设置成有资格， 不代表该node一定就是master），默认是true，es是默认集群中的第一台机器为master，如果这台机挂了就会重新选举master。\n    \n     \n    \n    node.data: true\n    指定该节点是否存储索引数据，默认为true。\n    \n     \n    \n    index.number_of_shards: 5\n    设置默认索引分片个数，默认为5片。\n    \n     \n    \n    index.number_of_replicas: 1\n    设置默认索引副本个数，默认为1个副本。如果采用默认设置，而你集群只配置了一台机器，那么集群的健康度为yellow，也就是所有的数据都是可用的，但是某些复制没有被分配（\n    \n    健康度可用 curl 'localhost:9200/_cat/health?v' 查看， 分为绿色、黄色或红色。绿色代表一切正常，集群功能齐全，黄色意味着所有的数据都是可用的，但是某些复制没有被分配，红色则代表因为某些原因，某些数据不可用）。\n    \n     \n    \n    path.conf: /path/to/conf\n    设置配置文件的存储路径，默认是es根目录下的config文件夹。\n    \n     \n    \n    path.data: /path/to/data\n    设置索引数据的存储路径，默认是es根目录下的data文件夹，可以设置多个存储路径，用逗号隔开，例：\n    \n    path.data: /path/to/data1,/path/to/data2\n    \n     \n    \n    path.work: /path/to/work\n    设置临时文件的存储路径，默认是es根目录下的work文件夹。\n    \n     \n    \n    path.logs: /path/to/logs\n    设置日志文件的存储路径，默认是es根目录下的logs文件夹\n    \n     \n    \n    path.plugins: /path/to/plugins\n    设置插件的存放路径，默认是es根目录下的plugins文件夹, 插件在es里面普遍使用，用来增强原系统核心功能。\n    \n     \n    \n    bootstrap.mlockall: true\n    设置为true来锁住内存不进行swapping。因为当jvm开始swapping时es的效率 会降低，所以要保证它不swap，可以把ES_MIN_MEM和ES_MAX_MEM两个环境变量设置成同一个值，并且保证机器有足够的内存分配给es。 同时也要允许elasticsearch的进程可以锁住内存，linux下启动es之前可以通过`ulimit -l unlimited`命令设置。\n    \n     \n    \n    network.bind_host: 192.168.0.1\n    设置绑定的ip地址，可以是ipv4或ipv6的，默认为0.0.0.0，绑定这台机器的任何一个ip。\n    \n    \n    network.publish_host: 192.168.0.1\n    设置其它节点和该节点交互的ip地址，如果不设置它会自动判断，值必须是个真实的ip地址。\n    \n     \n    \n    network.host: 192.168.0.1\n    这个参数是用来同时设置bind_host和publish_host上面两个参数。\n    \n     \n    \n    transport.tcp.port: 9300\n    设置节点之间交互的tcp端口，默认是9300。\n    \n     \n    \n    transport.tcp.compress: true\n    设置是否压缩tcp传输时的数据，默认为false，不压缩。\n    \n     \n    \n    http.port: 9200\n    设置对外服务的http端口，默认为9200。\n    \n     \n    \n    http.max_content_length: 100mb\n    设置内容的最大容量，默认100mb\n    \n     \n    \n    http.enabled: false\n    是否使用http协议对外提供服务，默认为true，开启。\n    \n     \n    \n    gateway.type: local\n    gateway的类型，默认为local即为本地文件系统，可以设置为本地文件系统，分布式文件系统，hadoop的HDFS，和amazon的s3服务器等。\n    \n     \n    \n    gateway.recover_after_nodes: 1\n    设置集群中N个节点启动时进行数据恢复，默认为1。\n    \n     \n    \n    gateway.recover_after_time: 5m\n    设置初始化数据恢复进程的超时时间，默认是5分钟。\n    \n     \n    \n    gateway.expected_nodes: 2\n    设置这个集群中节点的数量，默认为2，一旦这N个节点启动，就会立即进行数据恢复。\n    \n     \n    \n    cluster.routing.allocation.node_initial_primaries_recoveries: 4\n    初始化数据恢复时，并发恢复线程的个数，默认为4。\n    \n     \n    \n    cluster.routing.allocation.node_concurrent_recoveries: 2\n    添加删除节点或负载均衡时并发恢复线程的个数，默认为4。\n    \n     \n    \n    indices.recovery.max_size_per_sec: 0\n    设置数据恢复时限制的带宽，如入100mb，默认为0，即无限制。\n    \n     \n    \n    indices.recovery.concurrent_streams: 5\n    设置这个参数来限制从其它分片恢复数据时最大同时打开并发流的个数，默认为5。\n    \n     \n    \n    discovery.zen.minimum_master_nodes: 1\n    设置这个参数来保证集群中的节点可以知道其它N个有master资格的节点。默认为1，对于大的集群来说，可以设置大一点的值（2-4）\n    \n     \n    \n    discovery.zen.ping.timeout: 3s\n    设置集群中自动发现其它节点时ping连接超时时间，默认为3秒，对于比较差的网络环境可以高点的值来防止自动发现时出错。\n    \n     \n    \n    discovery.zen.ping.multicast.enabled: false\n    设置是否打开多播发现节点，默认是true。\n    \n     \n    \n    discovery.zen.ping.unicast.hosts: [\"host1\", \"host2:port\", \"host3[portX-portY]\"]\n    设置集群中master节点的初始列表，可以通过这些节点来自动发现新加入集群的节点。\n\n\n##### 配置文件详解2.0版\n\n    配置文件位于%ES_HOME%/config/elasticsearch.yml文件中，用Editplus打开它，你便可以进行配置。\n            所有的配置都可以使用环境变量，例如：\n    node.rack: ${RACK_ENV_VAR}\n            表示环境变量中有一个RACK_ENV_VAR变量。\n            下面列举一下elasticsearch的可配置项：\n            1. 集群名称，默认为elasticsearch：\n    cluster.name: elasticsearch\n            2. 节点名称，es启动时会自动创建节点名称，但你也可进行配置：\n    node.name: \"Franz Kafka\"\n            3. 是否作为主节点，每个节点都可以被配置成为主节点，默认值为true：\n    node.master: true\n            4. 是否存储数据，即存储索引片段，默认值为true：\n    node.data: true\n            master和data同时配置会产生一些奇异的效果：\n            1) 当master为false，而data为true时，会对该节点产生严重负荷；\n            2) 当master为true，而data为false时，该节点作为一个协调者；\n            3) 当master为false，data也为false时，该节点就变成了一个负载均衡器。\n            你可以通过连接http://localhost:9200/_cluster/health或者http://localhost:9200/_cluster/nodes，或者使用插件http://github.com/lukas-vlcek/bigdesk或http://mobz.github.com/elasticsearch-head来查看集群状态。\n            5. 每个节点都可以定义一些与之关联的通用属性，用于后期集群进行碎片分配时的过滤：\n    node.rack: rack314\n            6. 默认情况下，多个节点可以在同一个安装路径启动，如果你想让你的es只启动一个节点，可以进行如下设置：\n    node.max_local_storage_nodes: 1\n            7. 设置一个索引的碎片数量，默认值为5：\n    index.number_of_shards: 5\n            8. 设置一个索引可被复制的数量，默认值为1：\n    index.number_of_replicas: 1\n            当你想要禁用公布式时，你可以进行如下设置：\n    index.number_of_shards: 1\n    index.number_of_replicas: 0\n            这两个属性的设置直接影响集群中索引和搜索操作的执行。假设你有足够的机器来持有碎片和复制品，那么可以按如下规则设置这两个值：\n            1) 拥有更多的碎片可以提升索引执行能力，并允许通过机器分发一个大型的索引；\n            2) 拥有更多的复制器能够提升搜索执行能力以及集群能力。\n            对于一个索引来说，number_of_shards只能设置一次，而number_of_replicas可以使用索引更新设置API在任何时候被增加或者减少。\n            ElasticSearch关注加载均衡、迁移、从节点聚集结果等等。可以尝试多种设计来完成这些功能。\n            可以连接http://localhost:9200/A/_status来检测索引的状态。\n            9. 配置文件所在的位置，即elasticsearch.yml和logging.yml所在的位置：\n    path.conf: /path/to/conf\n            10. 分配给当前节点的索引数据所在的位置：\n    path.data: /path/to/data\n            可以可选择的包含一个以上的位置，使得数据在文件级别跨越位置，这样在创建时就有更多的自由路径，如：\n    path.data: /path/to/data1,/path/to/data2\n            11. 临时文件位置：\n    path.work: /path/to/work\n            12. 日志文件所在位置：\n    path.logs: /path/to/logs\n            13. 插件安装位置：\n    path.plugins: /path/to/plugins\n            14. 插件托管位置，若列表中的某一个插件未安装，则节点无法启动：\n    plugin.mandatory: mapper-attachments,lang-groovy\n            15. JVM开始交换时，ElasticSearch表现并不好：你需要保障JVM不进行交换，可以将bootstrap.mlockall设置为true禁止交换：\n    bootstrap.mlockall: true\n            请确保ES_MIN_MEM和ES_MAX_MEM的值是一样的，并且能够为ElasticSearch分配足够的内在，并为系统操作保留足够的内存。\n            16. 默认情况下，ElasticSearch使用0.0.0.0地址，并为http传输开启9200-9300端口，为节点到节点的通信开启9300-9400端口，也可以自行设置IP地址：\n    network.bind_host: 192.168.0.1\n            17. publish_host设置其他节点连接此节点的地址，如果不设置的话，则自动获取，publish_host的地址必须为真实地址：\n    network.publish_host: 192.168.0.1\n            18. bind_host和publish_host可以一起设置：\n    network.host: 192.168.0.1\n            19. 可以定制该节点与其他节点交互的端口：\n    transport.tcp.port: 9300\n            20. 节点间交互时，可以设置是否压缩，转为为不压缩：\n    transport.tcp.compress: true\n            21. 可以为Http传输监听定制端口：\n    http.port: 9200\n            22. 设置内容的最大长度：\n    http.max_content_length: 100mb\n            23. 禁止HTTP\n    http.enabled: false\n            24. 网关允许在所有集群重启后持有集群状态，集群状态的变更都会被保存下来，当第一次启用集群时，可以从网关中读取到状态，默认网关类型（也是推荐的）是local：\n    gateway.type: local\n            25. 允许在N个节点启动后恢复过程：\n    gateway.recover_after_nodes: 1\n            26. 设置初始化恢复过程的超时时间：\n    gateway.recover_after_time: 5m\n            27. 设置该集群中可存在的节点上限：\n    gateway.expected_nodes: 2\n            28. 设置一个节点的并发数量，有两种情况，一种是在初始复苏过程中：\n    cluster.routing.allocation.node_initial_primaries_recoveries: 4\n            另一种是在添加、删除节点及调整时：\n    cluster.routing.allocation.node_concurrent_recoveries: 2\n            29. 设置复苏时的吞吐量，默认情况下是无限的：\n    indices.recovery.max_size_per_sec: 0\n            30. 设置从对等节点恢复片段时打开的流的数量上限：\n    indices.recovery.concurrent_streams: 5\n            31. 设置一个集群中主节点的数量，当多于三个节点时，该值可在2-4之间：\n    discovery.zen.minimum_master_nodes: 1\n            32. 设置ping其他节点时的超时时间，网络比较慢时可将该值设大：\n    discovery.zen.ping.timeout: 3s\n    http://elasticsearch.org/guide/reference/modules/discovery/zen.html上有更多关于discovery的设置。\n            33. 禁止当前节点发现多个集群节点，默认值为true：\n    discovery.zen.ping.multicast.enabled: false\n            34. 设置新节点被启动时能够发现的主节点列表（主要用于不同网段机器连接）：\n    \n    discovery.zen.ping.unicast.hosts: [\"host1\", \"host2:port\", \"host3[portX-portY]\"]\n    \n           35.设置是否可以通过正则或者_all删除或者关闭索引\n    \n    action.destructive_requires_name 默认false 允许 可设置true不允许     ","tags":["Bigdata","ES"]},{"title":"flume安装","url":"/2022/09/07/cyb-mds/bigdata/Flume-NG/flume安装/","content":"\n==作者：YB-Chi==\n\n[toc]\n\n添加agent节点 \n\n在BDS部署\n\n配置环境变量\n\n###### 修改配置文件\n\n`cp flume-env.sh.template flume-env.sh`\n\n加入jdk和hadoop的环境变量\n\n`flume-ng version`验证\n\n创建agent.conf\n\n```powershell\n#source\na1.sources = sysSrc\na1.sources.sysSrc.type=syslogudp\na1.sources.sysSrc.bind=0.0.0.0\na1.sources.sysSrc.port=514\na1.sources.sysSrc.channels=fileChannel\n\n\n#channel\na1.channels = fileChannel\na1.channels.fileChannel.type =memory\na1.channels.fileChannel.capacity=100000\n\n\n#sink\na1.sinks = kafkaSink\na1.sinks.kafkaSink.channel=fileChannel\na1.sinks.kafkaSink.type=org.apache.flume.sink.kafka.KafkaSink\na1.sinks.kafkaSink.brokerList=BDS:9092,BDS-1:9092,BDS-2:9092\na1.sinks.kafkaSink.custom.partition.key=kafkaPartition\na1.sinks.kafkaSink.topic=flumetest\na1.sinks.kafkaSink.serializer.class=kafka.serializer.StringEncoder\n\n```\n\n启动\n\n`bin/flume-ng agent --conf conf --conf-file conf/agent.conf --name a1 -Dflume.root.logger=INFO,console`\n\n启动消费者\n\n```powershell\ncd /opt/cloudera/parcels/KAFKA-2.1.1-1.2.1.1.p0.18/lib/kafka\n\nbin/kafka-console-consumer.sh --zookeeper BDS:2181,BDS-1:2181,BDS-2:2181 --topic flumetest --from-beginning\n```\n\n使用测试工具发送信息进行测试 查看消费者是否收到\n\n\n\n\n\n\n\n","tags":["Bigdata","Flume-NG"]},{"title":"CDH6.2集群安装简略","url":"/2022/09/07/cyb-mds/bigdata/Hadoop/CDH6.2集群安装简略/","content":"\n==作者：YB-Chi==\n\n# CDH6.2集群安装简略\n\n[toc]\n\n\n\n文档只记录CDH安装，前期准备工作参考文档：https://www.shuzhiduo.com/A/LPdoGaWyd3/\n\n#### 全步骤：\n\n```shell\n所有操作使用root用户\n\n时间同步ntpd或者chrony\n\n防火墙关闭\n\nroot和主用用户的多机信任（免密ssh）\n\njdk1.8\n\nmysql以及建表以及connect jar包\n\n系统禁用透明页 修改swappiness参数\n\n安装cm等rpm包\n\n登录cdh界面安装组件\n```\n\n#### 补充\n\n```\nyum install perl\n\n所有节点更改jdk软链（这是cdh6.2默认读取路径）\nmkdir -p /usr/java\nln -s /home/module/jdk1.8.0_212 /usr/java/default\n\n更改py版本为2，改软链就行（102被设置了python3软链到python，这种情况才需要改回来）\n\n\nmysql修改大小写为不敏感,这个可以不做，这个是出于systemctl status cloudera-scm-server.service有个报错\n[main] ERROR org.hibernate.engine.jdbc.spi.SqlExceptionHelper - Table 'cmf.cm_version' doesn't exist\n但是修改后仍旧有报错，所以这个不做了\n```\n\n```shell\n安装包路径：\n/home/module/cloudera-repos\n\n安装后程序路径：\n程序默认安装在/opt/下\n\n数据路径\n界面安装组件，注意所有软件的路径要符合服务器的数据盘路径\n比如公司几台机器只有home磁盘较大，所以在home下建立了data文件夹，用于做cdh数据存储，路径需要根据系统挂载磁盘路径做更改\nmkdir -p /home/module/cdh_data\n后续会有个权限类的报错，直接给个777权限\nchmod 777 /home/module/cdh_data\n然后根据cdh界面安装组件时，所有出现数据路径的地方，在/home/module/cdh_data创建同名文件夹，并更改界面上的路径为home下的\n例如：mkdir -p /home/module/cdh_data/dfs/nn\n\n日志路径\n默认都在/var/log下\n```\n\n\n\n#### 常见问题：\n\n```sql\nhive中文乱码\nalter改mysql几个表的编码为utf-8\n\nmysql –u root –p\nuse hive;\n--修改字段注释字符集\nalter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8;\n--修改表注释字符集\nalter table TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;\n--修改分区参数，支持分区建用中文表示\nalter table PARTITION_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;\nalter table PARTITION_KEYS modify column PKEY_COMMENT varchar(4000) character set utf8;\n--修改表名注释，支持中文表示\nalter table INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;\n--修改视图，支持视图中文\nALTER TABLE TBLS modify COLUMN VIEW_EXPANDED_TEXT mediumtext CHARACTER SET utf8;\nALTER TABLE TBLS modify COLUMN VIEW_ORIGINAL_TEXT mediumtext CHARACTER SET utf8;\n```\n\n```shell\n特殊bug的修复，有个报错里面有提示到cm_guid 忘了啥bug了\n删除3台的cm——guid\n[root@namenode1 ~]$ sudo find / -name cm_guid\n/var/lib/cloudera-scm-agent/cm_guid\n[root@namenode1 ~]$ sudo rm /var/lib/cloudera-scm-agent/cm_guid\nrm: remove regular file ‘/var/lib/cloudera-scm-agent/cm_guid’? y\n```\n\n```shell\nspark没有hdfs权限\n添加超级用户组 并把root和主用户加进去\ngroupadd supergroup\nusermod -a -G supergroup root\nusermod -a -G supergroup xbsafe\nsu - hdfs -s /bin/bash -c \"hdfs dfsadmin -refreshUserToGroupsMappings\"\n```\n\n```shell\nspark使用内置hive2.7  CDH是hive2.1.1 启动spark会报错  尝试了网上的一些方案改配置等没生效，暂时是更改客户端参数的方式来解决\n改为spark-submit/sql/shell时指定参数\nspark.sql.hive.metastore.version=2.1.1\nspark.sql.hive.metastore.jars=/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/hive/lib/*\n示例：\nspark-sql --master yarn --conf spark.sql.hive.metastore.version=2.1.1 --conf spark.sql.hive.metastore.jars=/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/hive/lib/*\nspark-submit --master yarn  --conf spark.sql.hive.metastore.version=2.1.1 --conf spark.sql.hive.metastore.jars=/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/hive/lib/*  --executor-memory 2g --total-executor-cores 2 --driver-memory 4G --class com.xbsafe.run.MarkRunningTime /home/module/testdata/sparkSQL.jar\n\nbeeline -u jdbc:hive2://xbsafe102:10000 -n xbsafe\n```\n\n\n\n\n\n\n\n","tags":["Bigdata","linux"]}]