<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="HDFS"><meta name="keywords" content="Bigdata,Hadoop,HDFS"><meta name="author" content="YB-CHI"><meta name="copyright" content="YB-CHI"><title>HDFS | YB-CHI</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.1"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.2.0'
} </script><meta name="generator" content="Hexo 6.2.0"></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="toc-number">1.</span> <span class="toc-text">HDFS的工作机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS-%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B"><span class="toc-number">2.</span> <span class="toc-text">HDFS 写数据流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS-%E8%AF%BB%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B"><span class="toc-number">3.</span> <span class="toc-text">HDFS 读数据流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#NAMENODE%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="toc-number">4.</span> <span class="toc-text">NAMENODE工作机制</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#NAMENODE%E8%81%8C%E8%B4%A3"><span class="toc-number">4.1.</span> <span class="toc-text">NAMENODE职责</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%83%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86"><span class="toc-number">4.2.</span> <span class="toc-text">元数据管理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%83%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E6%9C%BA%E5%88%B6"><span class="toc-number">4.3.</span> <span class="toc-text">元数据存储机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%83%E6%95%B0%E6%8D%AE%E6%89%8B%E5%8A%A8%E6%9F%A5%E7%9C%8B"><span class="toc-number">4.4.</span> <span class="toc-text">元数据手动查看</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%83%E6%95%B0%E6%8D%AE%E7%9A%84checkpoint"><span class="toc-number">4.5.</span> <span class="toc-text">元数据的checkpoint</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#checkpoint%E7%9A%84%E8%AF%A6%E7%BB%86%E8%BF%87%E7%A8%8B"><span class="toc-number">4.6.</span> <span class="toc-text">checkpoint的详细过程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#checkpoint%E6%93%8D%E4%BD%9C%E7%9A%84%E8%A7%A6%E5%8F%91%E6%9D%A1%E4%BB%B6%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0"><span class="toc-number">4.7.</span> <span class="toc-text">checkpoint操作的触发条件配置参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#checkpoint%E7%9A%84%E9%99%84%E5%B8%A6%E4%BD%9C%E7%94%A8"><span class="toc-number">4.8.</span> <span class="toc-text">checkpoint的附带作用</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DATANODE%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="toc-number">5.</span> <span class="toc-text">DATANODE的工作机制</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Datanode%E5%B7%A5%E4%BD%9C%E8%81%8C%E8%B4%A3"><span class="toc-number">5.1.</span> <span class="toc-text">Datanode工作职责</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Datanode%E6%8E%89%E7%BA%BF%E5%88%A4%E6%96%AD%E6%97%B6%E9%99%90%E5%8F%82%E6%95%B0"><span class="toc-number">5.2.</span> <span class="toc-text">Datanode掉线判断时限参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%82%E5%AF%9F%E9%AA%8C%E8%AF%81DATANODE%E5%8A%9F%E8%83%BD"><span class="toc-number">5.3.</span> <span class="toc-text">观察验证DATANODE功能</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://raw.githubusercontent.com/chiyuanbo/pic/master/25052916.jpg"></div><div class="author-info__name text-center">YB-CHI</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">119</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">53</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://raw.githubusercontent.com/chiyuanbo/pic/master/1299-Panigale-Final-Edition-MY18-12-Slider-Gallery-1920X1080.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">YB-CHI</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">HDFS</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2022-10-27</time><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">1.7k</span><span class="post-meta__separator">|</span><span>阅读时长: 5 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>&#x3D;&#x3D;作者：YB-Chi&#x3D;&#x3D;</p>
<p>[toc]</p>
<h3 id="HDFS的工作机制"><a href="#HDFS的工作机制" class="headerlink" title="HDFS的工作机制"></a>HDFS的工作机制</h3><p>工作机制的学习主要是为加深对分布式系统的理解，以及增强遇到各种问题时的分析解决能力，形成一定的集群运维能力。</p>
<p>很多不是真正理解hadoop技术体系的人会常常觉得HDFS可用于网盘类应用，但实际并非如此。要想将技术准确用在恰当的地方，必须对技术有深刻的理解。</p>
<ol>
<li><p>HDFS集群分为两大角色：NameNode、DataNode</p>
</li>
<li><p>NameNode负责管理整个文件系统的元数据（元数据就是文件数据块放置在DataNode位置和数量等信息）</p>
</li>
<li><p>DataNode 负责管理用户的文件数据块</p>
</li>
<li><p>文件会按照固定的大小（blocksize）切成若干块后分布式存储在若干台datanode上</p>
</li>
<li><p>每一个文件块可以有多个副本，并存放在不同的datanode上</p>
</li>
<li><p>Datanode会定期向Namenode汇报自身所保存的文件block信息，而namenode则会负责保持文件的副本数量</p>
</li>
<li><p>HDFS的内部工作机制对客户端保持透明，客户端请求访问HDFS都是通过向namenode申请来进行</p>
</li>
</ol>
<h3 id="HDFS-写数据流程"><a href="#HDFS-写数据流程" class="headerlink" title="HDFS 写数据流程"></a>HDFS 写数据流程</h3><p>客户端要向HDFS写数据，首先要跟Namenode通信以确认可以写文件并获得接收文件block的Datanode，然后，客户端按顺序将文件逐个block传递给相应Datanode，并由接收到block的Datanode负责向其他Datanode复制block的副本</p>
<p><img src="https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fyl50mqcw9j20ov0fiwgu.jpg" alt="image"></p>
<p>写数据步骤详解</p>
<ol>
<li><p>Client向Namenode通信请求上传文件，Namenode检查目标文件是否已存在，父目录是否存在</p>
</li>
<li><p>Namenode返回是否可以上传</p>
</li>
<li><p>Client请求第一个 block该传输到哪些Datanode服务器上</p>
</li>
<li><p>Namenode返回3个(主副本数量)Datanode服务器ABC</p>
</li>
<li><p>Client请求3台DataNode中的一台A上传数据（本质上是一个RPC调用，建立pipeline），A收到请求会继续调用B，然后B调用C，将整个pipeline建立完成，逐级返回客户端</p>
</li>
<li><p>Client开始往A上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，A收到一个packet就会传给B，B传给C；A每传一个packet会放入一个应答队列等待应答</p>
</li>
<li><p>当一个block传输完成之后，Client再次请求Namenode上传第二个block的服务器。</p>
</li>
</ol>
<h3 id="HDFS-读数据流程"><a href="#HDFS-读数据流程" class="headerlink" title="HDFS 读数据流程"></a>HDFS 读数据流程</h3><p>客户端将要读取的文件路径发送给namenode，namenode获取文件的元信息（主要是block的存放位置信息）返回给客户端，客户端根据返回的信息找到相应datanode逐个获取文件的block并在客户端本地进行数据追加合并从而获得整个文件</p>
<p><img src="https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fyl53k503lj20pv0f9tb6.jpg" alt="image"></p>
<p>读数据步骤详解</p>
<ol>
<li><p>跟namenode通信查询元数据，找到文件块所在的datanode服务器</p>
</li>
<li><p>挑选一台datanode（就近原则，然后随机）服务器，请求建立socket流</p>
</li>
<li><p>datanode开始发送数据（从磁盘里面读取数据放入流，以packet为单位来做校验）</p>
</li>
<li><p>客户端以packet为单位接收，现在本地缓存，然后写入目标文件</p>
</li>
</ol>
<h3 id="NAMENODE工作机制"><a href="#NAMENODE工作机制" class="headerlink" title="NAMENODE工作机制"></a>NAMENODE工作机制</h3><h4 id="NAMENODE职责"><a href="#NAMENODE职责" class="headerlink" title="NAMENODE职责"></a>NAMENODE职责</h4><ul>
<li><p>负责客户端请求的响应</p>
</li>
<li><p>元数据的管理（查询，修改）</p>
</li>
</ul>
<h4 id="元数据管理"><a href="#元数据管理" class="headerlink" title="元数据管理"></a>元数据管理</h4><p>namenode对数据的管理采用了三种存储形式：</p>
<ul>
<li><p>内存元数据(NameSystem)</p>
</li>
<li><p>磁盘元数据镜像文件</p>
</li>
<li><p>数据操作日志文件（可通过日志运算出元数据）</p>
</li>
</ul>
<h4 id="元数据存储机制"><a href="#元数据存储机制" class="headerlink" title="元数据存储机制"></a>元数据存储机制</h4><ol>
<li><p>内存中有一份完整的元数据(内存meta data)</p>
</li>
<li><p>磁盘有一个“准完整”的元数据镜像（fsimage）文件(在namenode的工作目录中)</p>
</li>
<li><p>用于衔接内存metadata和持久化元数据镜像fsimage之间的操作日志（edits文件）</p>
</li>
</ol>
<p>注：当客户端对hdfs中的文件进行新增或者修改操作，操作记录首先被记入edits日志文件中，当客户端操作成功后，相应的元数据会更新到内存meta.data中</p>
<h4 id="元数据手动查看"><a href="#元数据手动查看" class="headerlink" title="元数据手动查看"></a>元数据手动查看</h4><p>可以通过hdfs的一个工具来查看edits中的信息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs oev -i edits -o edits.xml </span><br><span class="line">bin/hdfs oiv -i fsimage_0000000000000000087 -p XML -o fsimage.xml</span><br></pre></td></tr></table></figure>

<h4 id="元数据的checkpoint"><a href="#元数据的checkpoint" class="headerlink" title="元数据的checkpoint"></a>元数据的checkpoint</h4><p>每隔一段时间，会由secondary namenode将namenode上积累的所有edits和一个最新的fsimage下载到本地，并加载到内存进行merge（这个过程称为checkpoint）</p>
<h4 id="checkpoint的详细过程"><a href="#checkpoint的详细过程" class="headerlink" title="checkpoint的详细过程"></a>checkpoint的详细过程</h4><p><img src="https://raw.githubusercontent.com/chiyuanbo/pic/master/be1594fagy1fyl54ejcsrj20y70erjuz.jpg" alt="image"></p>
<h4 id="checkpoint操作的触发条件配置参数"><a href="#checkpoint操作的触发条件配置参数" class="headerlink" title="checkpoint操作的触发条件配置参数"></a>checkpoint操作的触发条件配置参数</h4><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dfs.namenode.checkpoint.check.period=<span class="number">60</span> <span class="comment">#检查触发条件是否满足的频率，60秒</span></span><br><span class="line">dfs.namenode.checkpoint.dir=file://<span class="variable">$</span>&#123;hadoop.tmp.dir&#125;/dfs/namesecondary</span><br><span class="line">dfs.namenode.checkpoint.edits.dir=<span class="variable">$</span>&#123;dfs.namenode.checkpoint.dir&#125; <span class="comment">#以上两个参数做checkpoint操作时，secondary namenode的本地工作目录</span></span><br><span class="line">dfs.namenode.checkpoint.max<span class="literal">-retries</span>=<span class="number">3</span> <span class="comment">#最大重试次数</span></span><br><span class="line">dfs.namenode.checkpoint.period=<span class="number">3600</span> <span class="comment">#两次checkpoint之间的时间间隔3600秒</span></span><br><span class="line">dfs.namenode.checkpoint.txns=<span class="number">1000000</span> <span class="comment">#两次checkpoint之间最大的操作记录</span></span><br></pre></td></tr></table></figure>
<h4 id="checkpoint的附带作用"><a href="#checkpoint的附带作用" class="headerlink" title="checkpoint的附带作用"></a>checkpoint的附带作用</h4><p>namenode和secondary namenode的工作目录存储结构完全相同，所以，当namenode故障退出需要重新恢复时，可以从secondary namenode的工作目录中将fsimage拷贝到namenode的工作目录，以恢复namenode的元数据</p>
<h3 id="DATANODE的工作机制"><a href="#DATANODE的工作机制" class="headerlink" title="DATANODE的工作机制"></a>DATANODE的工作机制</h3><h4 id="Datanode工作职责"><a href="#Datanode工作职责" class="headerlink" title="Datanode工作职责"></a>Datanode工作职责</h4><ol>
<li>存储管理用户的文件块数据</li>
<li>定期向namenode汇报自身所持有的block信息（通过心跳信息上报）（这点很重要，因为，当集群中发生某些block副本失效时，集群如何恢复block初始副本数量的问题）</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>  </span><br><span class="line">	 <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.blockreport.intervalMsec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	 <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600000<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line">	 <span class="tag">&lt;<span class="name">description</span>&gt;</span>Determines block reporting interval in milliseconds. <span class="tag">&lt;/<span class="name">description</span>&gt;</span>   </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="Datanode掉线判断时限参数"><a href="#Datanode掉线判断时限参数" class="headerlink" title="Datanode掉线判断时限参数"></a>Datanode掉线判断时限参数</h4><p>datanode进程死亡或者网络故障造成datanode无法与namenode通信，namenode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为:</p>
<p><strong><code>timeout = 2 * heartbeat.recheck.interval + 10 * dfs.heartbeat.interval</code></strong></p>
<p>而默认的heartbeat.recheck.interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。<br>需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。所以，举个例子，如果heartbeat.recheck.interval设置为5000（毫秒），dfs.heartbeat.interval设置为3（秒，默认），则总的超时时间为40秒。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>heartbeat.recheck.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>2000<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.heartbeat.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="观察验证DATANODE功能"><a href="#观察验证DATANODE功能" class="headerlink" title="观察验证DATANODE功能"></a>观察验证DATANODE功能</h4><p>上传一个文件，观察文件的block具体的物理存放情况：<br>在每一台datanode机器上的这个目录中能找到文件的切块：<br>&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;hadoop-2.7.3&#x2F;tmp&#x2F;dfs&#x2F;data&#x2F;current&#x2F;BP-193442119-192.168.88.3-1432458743457&#x2F;current&#x2F;finalized</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">YB-CHI</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://yb-chi.top/2022/10/27/cyb-mds/bigdata/Hadoop/HDFS/">https://yb-chi.top/2022/10/27/cyb-mds/bigdata/Hadoop/HDFS/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://yb-chi.top">YB-CHI</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Bigdata/">Bigdata</a><a class="post-meta__tags" href="/tags/Hadoop/">Hadoop</a><a class="post-meta__tags" href="/tags/HDFS/">HDFS</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2022/10/27/cyb-mds/bigdata/Hbase/Hbase%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB/"><i class="fa fa-chevron-left">  </i><span>Hbase数据迁移</span></a></div><div class="next-post pull-right"><a href="/2022/10/27/cyb-mds/bigdata/Hadoop/Hadoop%E5%8D%95%E6%9C%BA%E5%AE%89%E8%A3%85/"><span>Hadoop单机安装</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://raw.githubusercontent.com/chiyuanbo/pic/master/1299-Panigale-Final-Edition-MY18-12-Slider-Gallery-1920X1080.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2016 - 2024 By YB-CHI</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/lib/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.1"></script><script src="/js/fancybox.js?version=1.9.1"></script><script src="/js/sidebar.js?version=1.9.1"></script><script src="/js/copy.js?version=1.9.1"></script><script src="/js/fireworks.js?version=1.9.1"></script><script src="/js/transition.js?version=1.9.1"></script><script src="/js/scroll.js?version=1.9.1"></script><script src="/js/head.js?version=1.9.1"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>